[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Wstep",
    "section": "",
    "text": "Cel i charakter kursu\nKsiążka jest przygotowana dla studentów kierunków matematyka oraz inżynieria i analiza danych.\nCelem niniejszego kursu jest stworzenie studentom możliwości zapoznania się z metodami eksploracji danych oraz klasycznego uczenia maszynowego, ze szczególnym naciskiem na zrozumienie ich założeń, mechanizmów działania oraz miejsca w procesie analizy danych. Kurs ma umożliwić nie tylko poznanie konkretnych algorytmów, lecz przede wszystkim wykształcenie świadomości metodologicznej, która pozwala na świadomy dobór narzędzi analitycznych do charakteru problemu i struktury danych. Eksploracja danych i uczenie maszynowe są w ramach kursu traktowane jako spójny zbiór technik służących do wydobywania wiedzy z danych empirycznych oraz budowy modeli predykcyjnych i decyzyjnych.\nKurs jest osadzony w kontekście analizy danych tablicowych oraz klasycznych problemów klasyfikacji i regresji. Studenci będą mogli zapoznać się z metodami, które odgrywają fundamentalną rolę w praktyce analitycznej i stanowią podstawę wielu współczesnych rozwiązań stosowanych w analizie danych. Omawiane techniki, takie jak regresja logistyczna, analiza dyskryminacyjna, drzewa decyzyjne, metody zespołowe, klasyfikatory bayesowskie, algorytm k najbliższych sąsiadów, modele addytywne oraz maszyny wektorów nośnych, są prezentowane jako elementy spójnej tradycji metod eksploracyjnych i predykcyjnych. Kurs celowo pomija zagadnienia wdrażania modeli oraz głębokich sieci neuronowych, które są realizowane w ramach innych przedmiotów, aby zachować koncentrację na klasycznych fundamentach uczenia maszynowego.\nW trakcie kursu studenci będą mogli nauczyć się, że eksploracja danych nie ogranicza się wyłącznie do wizualizacji czy statystyk opisowych, lecz obejmuje również budowę modeli umożliwiających identyfikację wzorców, segmentację obserwacji oraz podejmowanie decyzji na podstawie danych. Jednocześnie będą mogli zrozumieć, że uczenie maszynowe nie sprowadza się jedynie do stosowania algorytmów optymalizacyjnych, lecz stanowi formalny aparat pozwalający na automatyczne uczenie się zależności oraz ocenę jakości predykcji. Takie ujęcie ma umożliwić lepsze zrozumienie, dlaczego te same metody znajdują zastosowanie zarówno w analizie eksploracyjnej, jak i w zadaniach stricte predykcyjnych.\nIstotnym elementem kursu jest umożliwienie studentom zapoznania się z etapem przygotowania danych, który w praktyce analizy danych ma kluczowe znaczenie dla jakości modeli. Studenci będą mogli nauczyć się zasad czyszczenia danych, obsługi braków, kodowania zmiennych, standaryzacji oraz wstępnej analizy rozkładów i zależności. Etap ten jest przedstawiany nie jako czynność czysto techniczna, lecz jako integralna część procesu eksploracji danych, wymagająca podejmowania świadomych decyzji analitycznych i uwzględniania kontekstu problemu. Takie podejście ma umożliwić lepsze przygotowanie do pracy z rzeczywistymi danymi empirycznymi.\nW dalszej części kursu studenci będą mogli zapoznać się z metodami uczenia nadzorowanego w sposób systematyczny, z naciskiem na ich interpretację, założenia statystyczne oraz różnice pomiędzy podejściami generatywnymi i dyskryminacyjnymi. Kurs ma umożliwić zrozumienie, w jakich sytuacjach zasadne jest stosowanie prostych modeli liniowych, a kiedy warto sięgnąć po metody nieliniowe lub zespołowe. Szczególna uwaga poświęcona jest zagadnieniu interpretowalności modeli oraz kompromisowi pomiędzy złożonością modelu a jego zdolnością do generalizacji, co stanowi jeden z kluczowych problemów w analizie danych stosowanej.\nChoć kurs koncentruje się na klasycznych metodach, jego celem jest również umożliwienie studentom zrozumienia szerszego kontekstu uczenia maszynowego. Krótkie wprowadzenia do reinforcement learning, modeli koszykowych oraz wykrywania anomalii mają pozwolić na zapoznanie się z podstawowymi zasadami działania innych paradygmatów uczenia. Zagadnienia te są omawiane na poziomie koncepcyjnym, tak aby studenci mogli rozpoznać, jakie typy problemów wymagają odmiennych podejść oraz jakie są granice stosowalności klasycznych metod eksploracji danych.\nOstatecznie kurs ma umożliwić studentom wykształcenie umiejętności krytycznego myślenia o danych i modelach. Po jego ukończeniu studenci będą mogli nie tylko znać podstawowe algorytmy eksploracji danych i uczenia maszynowego, lecz także rozumieć ich genezę, założenia oraz konsekwencje ich zastosowania. Kurs stanowi fundament dla dalszych zajęć z zakresu zaawansowanego uczenia maszynowego, analizy dużych zbiorów danych oraz systemów opartych na sztucznej inteligencji, dostarczając solidnych podstaw teoretycznych i metodologicznych.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Wstep</span>"
    ]
  },
  {
    "objectID": "index.html#struktura-ksiazki",
    "href": "index.html#struktura-ksiazki",
    "title": "Wstep",
    "section": "Struktura ksiazki",
    "text": "Struktura ksiazki\n\nWprowadzenie i historia\nPrzygotowanie i czyszczenie danych\nModele liniowe i dyskryminacyjne\nDrzewa decyzyjne i zespoly modeli\nKlasyfikatory bayesowskie\nk-NN\nModele addytywne\nSVM\nInne metody\nPodsumowanie",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Wstep</span>"
    ]
  },
  {
    "objectID": "chapters/01-wprowadzenie.html",
    "href": "chapters/01-wprowadzenie.html",
    "title": "2  Wprowadzenie i historia",
    "section": "",
    "text": "2.1 Historia eksploracji danych i uczenia maszynowego\nHistoria eksploracji danych oraz uczenia maszynowego jest ściśle związana z rozwojem statystyki, informatyki i sztucznej inteligencji, jednak obie dziedziny wywodzą się z odmiennych tradycji badawczych. Eksploracja danych (data mining) rozwijała się przede wszystkim na gruncie statystyki matematycznej, analizy danych oraz teorii baz danych i była odpowiedzią na rosnącą dostępność dużych zbiorów danych empirycznych. Jej głównym celem było odkrywanie wzorców, struktur i zależności, które nie były bezpośrednio widoczne przy użyciu klasycznych narzędzi analizy. Uczenie maszynowe (machine learning) natomiast wyrosło z badań nad sztuczną inteligencją i algorytmami uczącymi się, kładąc nacisk na formalne modele predykcyjne, automatyczne dostosowywanie się do danych oraz własności generalizacji. W kontekście niniejszego wykładu oba podejścia spotykają się w obszarze metod klasyfikacyjnych i regresyjnych, które jednocześnie służą eksploracji danych i budowie modeli uczących się.\nPierwsze istotne fundamenty eksploracji danych pojawiły się już na początku XX wieku wraz z rozwojem statystyki wielowymiarowej. Szczególne znaczenie miały badania nad klasyfikacją i redukcją wymiaru, prowadzone w ramach wnioskowania statystycznego. Przełomową rolę odegrała praca Ronalda A. Fishera z 1936 roku, w której zaproponowano liniową analizę dyskryminacyjną jako metodę rozróżniania klas na podstawie kombinacji liniowych zmiennych. Choć Fisher nie posługiwał się pojęciem uczenia maszynowego, jego metoda stanowi bezpośredni pierwowzór współczesnych modeli dyskryminacyjnych, takich jak LDA, QDA oraz ich późniejsze uogólnienia. W tym okresie analiza danych była traktowana przede wszystkim jako narzędzie inferencyjne, służące do testowania hipotez i opisu struktury populacji.\nRównolegle do rozwoju statystyki, w połowie XX wieku zaczęły kształtować się idee sztucznej inteligencji. To właśnie na tym gruncie narodziło się uczenie maszynowe jako odrębna dziedzina. Jednym z najczęściej cytowanych momentów symbolicznych jest publikacja Arthura Samuela z 1959 roku, w której autor opisał algorytm uczący się gry w warcaby poprzez doświadczenie. Samuel zaproponował definicję uczenia maszynowego jako procesu, w którym system poprawia swoje działanie na podstawie obserwacji danych, co do dziś pozostaje centralnym elementem tej dziedziny. W tym samym czasie rozwijano probabilistyczne metody klasyfikacji, oparte na twierdzeniu Bayesa, które umożliwiały formalne łączenie danych empirycznych z wiedzą a priori. Klasyfikatory ML, MAP oraz naiwny klasyfikator Bayesa znalazły szerokie zastosowanie zarówno w eksploracji danych, jak i w uczeniu maszynowym.\nLata siedemdziesiąte i osiemdziesiąte XX wieku przyniosły intensywny rozwój metod klasyfikacyjnych, które do dziś stanowią podstawę analizy danych. Szczególnie istotne okazały się drzewa decyzyjne, ugruntowane w monografii Breimana, Friedmana, Olshena i Stone’a z 1984 roku. Metody CART wprowadziły ideę rekurencyjnego podziału przestrzeni cech oraz kryteria optymalizacji oparte na nieczystości węzłów, co pozwoliło na budowę modeli jednocześnie skutecznych i interpretowalnych. Drzewa decyzyjne szybko stały się jednym z podstawowych narzędzi eksploracji danych, zwłaszcza w analizie zbiorów o złożonej strukturze i mieszanych typach zmiennych.\nW tym samym okresie rozwijano metody oparte na podobieństwie obserwacji, w szczególności algorytm k najbliższych sąsiadów. Praca Covera i Harta z 1967 roku formalnie opisała własności klasyfikatora k-NN, który nie wymaga jawnej estymacji parametrów modelu, a decyzje podejmuje na podstawie lokalnej struktury danych. Metody te odegrały ważną rolę w eksploracji danych, ponieważ umożliwiały analizę bez silnych założeń rozkładowych i stanowiły punkt odniesienia dla późniejszych, bardziej złożonych algorytmów.\nIstotnym krokiem w stronę formalizacji uczenia maszynowego było wprowadzenie maszyn wektorów nośnych w latach dziewięćdziesiątych. Praca Cortes i Vapnika z 1995 roku zaproponowała model klasyfikacyjny oparty na maksymalizacji marginesu separacji klas, osadzony w aparacie optymalizacji wypukłej. Dzięki zastosowaniu funkcji jądrowych SVM umożliwiły efektywną klasyfikację danych nieliniowo separowalnych w przestrzeniach o bardzo wysokim wymiarze. Metoda ta stała się jednym z najlepiej ugruntowanych teoretycznie algorytmów uczenia maszynowego i do dziś pełni istotną rolę w analizie danych.\nKolejnym przełomem w eksploracji danych było pojawienie się metod zespołowych. Leo Breiman wprowadził ideę baggingu w 1996 roku, pokazując, że agregacja wielu niestabilnych modeli może znacząco poprawić jakość predykcji. Rozwinięciem tej koncepcji były lasy losowe, które połączyły bagging z losowym wyborem cech, tworząc jeden z najbardziej uniwersalnych algorytmów analizy danych. Równolegle rozwijał się nurt boostingu, zapoczątkowany przez Freund i Schapire, który polegał na sekwencyjnym uczeniu słabych klasyfikatorów i ich adaptacyjnym ważeniu. Współczesne algorytmy, takie jak Gradient Boosting, XGBoost czy CatBoost, stanowią bezpośrednie rozwinięcie tych idei i są dziś standardem w analizie danych tablicowych.\nNa styku statystyki i uczenia maszynowego rozwijały się także modele addytywne. Prace Hastiego i Tibshiraniego z lat osiemdziesiątych wprowadziły uogólnione modele addytywne, które umożliwiają elastyczne modelowanie zależności nieliniowych przy zachowaniu interpretowalnej struktury. Modele te stanowią naturalne rozszerzenie klasycznej regresji i dobrze ilustrują ewolucję metod eksploracji danych w kierunku większej elastyczności bez rezygnacji z kontroli nad złożonością modelu.\nZ perspektywy dydaktycznej historia eksploracji danych i uczenia maszynowego pokazuje, że większość współczesnych algorytmów opiera się na ideach rozwijanych od dziesięcioleci. Metody omawiane w dalszej części wykładu nie są oderwanymi narzędziami, lecz elementami spójnej ewolucji pojęć takich jak klasyfikacja, estymacja, generalizacja i kompromis między złożonością a interpretowalnością. Zrozumienie tego kontekstu historycznego pozwala lepiej interpretować zachowanie modeli, ich założenia oraz ograniczenia w praktycznej analizie danych.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Wprowadzenie i historia</span>"
    ]
  },
  {
    "objectID": "chapters/02-przygotowanie-danych.html",
    "href": "chapters/02-przygotowanie-danych.html",
    "title": "3  Przygotowanie i czyszczenie danych",
    "section": "",
    "text": "3.1 Import i czyszczenie danych\nPrzygotowanie danych stanowi jeden z kluczowych etapów procesu eksploracji danych i uczenia maszynowego. Jakość danych wejściowych w dużej mierze determinuje jakość modeli, niezależnie od stopnia ich złożoności. W praktyce analizy danych etap ten obejmuje zarówno wstępną eksplorację danych, jak i ich transformację, czyszczenie oraz konstrukcję cech, które będą następnie wykorzystywane przez algorytmy uczenia nadzorowanego. W niniejszym rozdziale przygotowanie danych jest traktowane jako proces analityczny, a nie jedynie techniczny etap przetwarzania.\nImport danych jest pierwszym momentem, w którym można świadomie zminimalizować późniejsze problemy jakościowe. W praktyce ważne jest nie tylko „wczytanie pliku”, ale kontrola takich elementów jak kodowanie znaków, separator, typy danych, format dat, niestandardowe znaczniki braków oraz interpretacja wartości logicznych. pandas daje w tym zakresie bardzo duże możliwości, a poprawna konfiguracja importu bywa istotniejsza niż późniejsze „naprawy” wykonywane ad hoc.\nNajczęściej spotykanym formatem jest CSV, który bywa myląco prosty: różne pliki mogą używać separatora , lub ;, różnych separatorów dziesiętnych (kropka/przecinek), mogą zawierać znaki narodowe (tu: polskie nazwy miast), a braki mogą być kodowane jako puste pole, NA, N/A, null, -999 itd. Dlatego przy imporcie CSV bardzo często warto jawnie ustawić: sep, encoding, na_values, parse_dates, a w razie potrzeby także dtype. Przykładowo1:\nKod\nimport pandas as pd\n\ndf = pd.read_csv(\n    \"data.csv\",\n    encoding=\"utf-8\",\n    na_values=[\"\", \"NA\", \"N/A\", \"null\", \"None\"],\n    parse_dates=[\"signup_date\", \"last_purchase_date\"]\n)\ndf.head()\n\n\n\n\n\n\n\n\n\ncustomer_id\nsignup_date\nlast_purchase_date\nage\nincome_eur\ncity\nsegment\ndevice_type\nvisits_last30\navg_basket_eur\nreturns_last90\nhas_promo\nsatisfaction_1_5\nchurned\nnotes\n\n\n\n\n0\n100000\n2022-04-08\n2022-10-19\nNaN\n72955.0\nWarszawa\nB2B\ndesktop\n8.0\n39.88\n1.0\nFalse\n4.0\n0\nklient powracający\n\n\n1\n100001\n2024-04-27\n2024-05-13\n52.0\n24863.0\nKraków\nB2C\nmobile\n12.0\n67.80\n1.0\nFalse\n3.0\n0\nbrak uwag\n\n\n2\n100002\n2023-12-18\n2024-04-05\nNaN\n25333.0\nWrocław\nB2C\ndesktop\n6.0\n27.04\n1.0\nTrue\n4.0\n0\nreklamacja\n\n\n3\n100003\n2023-04-26\n2023-10-01\n16.0\nNaN\nŁódź\nB2B\nmobile\n6.0\n149.03\n1.0\nFalse\n3.0\n1\nbrak uwag\n\n\n4\n100004\n2023-04-20\n2024-04-07\n42.0\n27954.0\nWarszawa\nB2C\ndesktop\n7.0\n49.73\n1.0\nTrue\n3.0\n0\nNaN\nW tym przykładzie na_values powoduje, że zarówno puste pola, jak i tekstowe znaczniki braków zostaną zamienione na NaN, a parse_dates zadba o automatyczną konwersję wskazanych kolumn do typu daty. To jest szczególnie ważne, bo daty wczytane jako tekst utrudniają analizę sezonowości, czasu od rejestracji, czy prostych agregacji po miesiącach.\nW przypadku danych w Excelu (.xlsx) import odbywa się przez read_excel. W praktyce warto pamiętać, że Excel często zawiera dodatkowe arkusze, nagłówki „opisowe” nad tabelą, albo mieszane typy w kolumnach. Gdy dane są w konkretnym arkuszu i zaczynają się od konkretnego wiersza, przydają się parametry sheet_name oraz skiprows. Dla dołączonego pliku:\nKod\ndf = pd.read_excel(\n    \"data.xlsx\",\n    sheet_name=0,\n    na_values=[\"\", \"NA\", \"N/A\", \"null\", \"None\"],\n    parse_dates=[\"signup_date\", \"last_purchase_date\"]\n)\n\ndf.head()\n\n\n\n\n\n\n\n\n\ncustomer_id\nsignup_date\nlast_purchase_date\nage\nincome_eur\ncity\nsegment\ndevice_type\nvisits_last30\navg_basket_eur\nreturns_last90\nhas_promo\nsatisfaction_1_5\nchurned\nnotes\n\n\n\n\n0\n100000\n2022-04-08\n2022-10-19\nNaN\n72955.0\nWarszawa\nB2B\ndesktop\n8\n39.88\n1\nFalse\n4.0\n0\nklient powracający\n\n\n1\n100001\n2024-04-27\n2024-05-13\n52.0\n24863.0\nKraków\nB2C\nmobile\n12\n67.80\n1\nFalse\n3.0\n0\nbrak uwag\n\n\n2\n100002\n2023-12-18\n2024-04-05\nNaN\n25333.0\nWrocław\nB2C\ndesktop\n6\n27.04\n1\nTrue\n4.0\n0\nreklamacja\n\n\n3\n100003\n2023-04-26\n2023-10-01\n16.0\nNaN\nŁódź\nB2B\nmobile\n6\n149.03\n1\nFalse\n3.0\n1\nbrak uwag\n\n\n4\n100004\n2023-04-20\n2024-04-07\n42.0\n27954.0\nWarszawa\nB2C\ndesktop\n7\n49.73\n1\nTrue\n3.0\n0\nNaN\nKlasyczny format JSON (.json) przechowuje dane jako jedną spójną strukturę, najczęściej listę obiektów (rekordów). Taki zapis jest szczególnie często spotykany w interfejsach API, plikach konfiguracyjnych oraz w wymianie danych pomiędzy systemami informatycznymi. Z punktu widzenia analizy danych format ten jest bardziej „opisowy” i czytelny dla człowieka, ale jednocześnie wymaga załadowania całej struktury do pamięci.\nW przygotowanym pliku data.json dane zapisane są jako lista rekordów, gdzie każdy rekord odpowiada jednej obserwacji, a klucze obiektów odpowiadają nazwom zmiennych. Taki układ bardzo naturalnie mapuje się na strukturę ramki danych w pandas. Podstawowy import danych JSON do pandas odbywa się za pomocą funkcji read_json.\nKod\ndf = pd.read_json(\"data.json\")\ndf.head()\n\n\n\n\n\n\n\n\n\ncustomer_id\nsignup_date\nlast_purchase_date\nage\nincome_eur\ncity\nsegment\ndevice_type\nvisits_last30\navg_basket_eur\nreturns_last90\nhas_promo\nsatisfaction_1_5\nchurned\nnotes\n\n\n\n\n0\n100000\n1649376000000\n1666137600000\nNaN\n72955.0\nWarszawa\nB2B\ndesktop\n8\n39.88\n1\nFalse\n4.0\n0\nklient powracający\n\n\n1\n100001\n1714176000000\n1715558400000\n52.0\n24863.0\nKraków\nB2C\nmobile\n12\n67.80\n1\nFalse\n3.0\n0\nbrak uwag\n\n\n2\n100002\n1702857600000\n1712275200000\nNaN\n25333.0\nWrocław\nB2C\ndesktop\n6\n27.04\n1\nTrue\n4.0\n0\nreklamacja\n\n\n3\n100003\n1682467200000\n1696118400000\n16.0\nNaN\nŁódź\nB2B\nmobile\n6\n149.03\n1\nFalse\n3.0\n1\nbrak uwag\n\n\n4\n100004\n1681948800000\n1712448000000\n42.0\n27954.0\nWarszawa\nB2C\ndesktop\n7\n49.73\n1\nTrue\n3.0\n0\nNA\nPo wczytaniu danych pandas automatycznie spróbuje rozpoznać typy zmiennych, jednak – podobnie jak w przypadku CSV – nie zawsze zrobi to zgodnie z oczekiwaniami analityka. W szczególności kolumny datowe są często wczytywane jako typ object, dlatego dobrą praktyką jest ich jawna konwersja:\nKod\ndf[\"signup_date\"] = pd.to_datetime(df[\"signup_date\"], errors=\"coerce\")\ndf[\"last_purchase_date\"] = pd.to_datetime(df[\"last_purchase_date\"], errors=\"coerce\")\ndf.head()\n\n\n\n\n\n\n\n\n\ncustomer_id\nsignup_date\nlast_purchase_date\nage\nincome_eur\ncity\nsegment\ndevice_type\nvisits_last30\navg_basket_eur\nreturns_last90\nhas_promo\nsatisfaction_1_5\nchurned\nnotes\n\n\n\n\n0\n100000\n1970-01-01 00:27:29.376000\n1970-01-01 00:27:46.137600\nNaN\n72955.0\nWarszawa\nB2B\ndesktop\n8\n39.88\n1\nFalse\n4.0\n0\nklient powracający\n\n\n1\n100001\n1970-01-01 00:28:34.176000\n1970-01-01 00:28:35.558400\n52.0\n24863.0\nKraków\nB2C\nmobile\n12\n67.80\n1\nFalse\n3.0\n0\nbrak uwag\n\n\n2\n100002\n1970-01-01 00:28:22.857600\n1970-01-01 00:28:32.275200\nNaN\n25333.0\nWrocław\nB2C\ndesktop\n6\n27.04\n1\nTrue\n4.0\n0\nreklamacja\n\n\n3\n100003\n1970-01-01 00:28:02.467200\n1970-01-01 00:28:16.118400\n16.0\nNaN\nŁódź\nB2B\nmobile\n6\n149.03\n1\nFalse\n3.0\n1\nbrak uwag\n\n\n4\n100004\n1970-01-01 00:28:01.948800\n1970-01-01 00:28:32.448000\n42.0\n27954.0\nWarszawa\nB2C\ndesktop\n7\n49.73\n1\nTrue\n3.0\n0\nNA\nParametr errors=\"coerce\" powoduje, że ewentualne niepoprawne formaty dat zostaną zamienione na NaT, co jest bezpieczniejsze niż przerwanie importu błędem. W kontekście eksploracji danych takie zachowanie pozwala szybko zidentyfikować problemy jakościowe bez utraty całego zbioru. Warto podkreślić, że format JSON nie posiada natywnego pojęcia braków danych w sensie znanym z analizy statystycznej. Braki mogą być reprezentowane jako null, brak klucza lub wartość tekstowa (np. NA). pandas zamienia null na NaN, ale nie rozpoznaje automatycznie tekstowych znaczników braków. Dlatego po imporcie zalecane jest jawne czyszczenie takich wartości:\nKod\ndf.replace([\"NA\", \"N/A\", \"\"], pd.NA, inplace=True)\ndf.head()\n\n\n\n\n\n\n\n\n\ncustomer_id\nsignup_date\nlast_purchase_date\nage\nincome_eur\ncity\nsegment\ndevice_type\nvisits_last30\navg_basket_eur\nreturns_last90\nhas_promo\nsatisfaction_1_5\nchurned\nnotes\n\n\n\n\n0\n100000\n1970-01-01 00:27:29.376000\n1970-01-01 00:27:46.137600\nNaN\n72955.0\nWarszawa\nB2B\ndesktop\n8\n39.88\n1\nFalse\n4.0\n0\nklient powracający\n\n\n1\n100001\n1970-01-01 00:28:34.176000\n1970-01-01 00:28:35.558400\n52.0\n24863.0\nKraków\nB2C\nmobile\n12\n67.80\n1\nFalse\n3.0\n0\nbrak uwag\n\n\n2\n100002\n1970-01-01 00:28:22.857600\n1970-01-01 00:28:32.275200\nNaN\n25333.0\nWrocław\nB2C\ndesktop\n6\n27.04\n1\nTrue\n4.0\n0\nreklamacja\n\n\n3\n100003\n1970-01-01 00:28:02.467200\n1970-01-01 00:28:16.118400\n16.0\nNaN\nŁódź\nB2B\nmobile\n6\n149.03\n1\nFalse\n3.0\n1\nbrak uwag\n\n\n4\n100004\n1970-01-01 00:28:01.948800\n1970-01-01 00:28:32.448000\n42.0\n27954.0\nWarszawa\nB2C\ndesktop\n7\n49.73\n1\nTrue\n3.0\n0\n&lt;NA&gt;\nW porównaniu do CSV, format JSON lepiej zachowuje strukturę danych (np. brak problemów z separatorami czy kodowaniem znaków), ale gorzej skaluje się dla bardzo dużych zbiorów. Z tego względu klasyczny JSON jest szczególnie przydatny w dydaktyce oraz w pracy z danymi średniej wielkości, gdzie czytelność i jednoznaczność struktury są ważniejsze niż wydajność. Z perspektywy dalszych etapów kursu istotne jest, aby rozumieć, że import danych nie jest neutralnym technicznie krokiem, lecz pierwszym momentem, w którym podejmowane są decyzje wpływające na całą analizę. Różnice pomiędzy CSV, JSONL i JSON przekładają się nie tylko na sposób wczytania danych, ale także na późniejsze możliwości ich walidacji, przetwarzania i skalowania.￼\nW praktyce import często wymaga kontroli typów. Jeśli np. identyfikator klienta ma wyglądać jak liczba, ale nie wolno dopuścić do utraty wiodących zer (częsty przypadek dla kodów), należy wymusić typ tekstowy przez dtype={\"customer_id\": \"string\"}. W tym konkretnym zbiorze customer_id jest liczbowy, ale w realnych danych biznesowych to częsty problem. Analogicznie, gdy w jednej kolumnie występują liczby i tekst (np. „brak”), pandas może ustawić typ object, a to utrudni obliczenia – lepiej wczytać z na_values i później rzutować typy jawnie.\nIstotne są też parametry wpływające na wydajność i kontrolę pamięci. Dla dużych plików CSV warto rozważyć usecols (czytać tylko potrzebne kolumny), chunksize (czytanie porcjami) oraz low_memory=False (mniej błędnych inferencji typów kosztem RAM). Przykład importu porcjami:\nKod\nchunks = pd.read_csv(\"data.csv\", chunksize=50_000)\nfor chunk in chunks:\n    # walidacje / czyszczenie / zapis częściowy\n    pass\nNa końcu, dobrą praktyką po imporcie jest natychmiastowa „kontrola jakości importu”: df.info(), df.isna().sum(), sprawdzenie liczby unikatów w kategoriach oraz szybkie oględziny podejrzanych wartości (np. wiek 120). To pozwala wcześnie odróżnić problemy wynikające z danych od problemów wynikających z błędnego importu.\nW praktyce analizy danych bardzo często spotyka się zbiory danych, których nazwy kolumn są niewygodne lub wręcz problematyczne z punktu widzenia języka Python oraz bibliotek analitycznych. Dotyczy to w szczególności nazw zawierających spacje, znaki specjalne, polskie znaki diakrytyczne, rozpoczynających się od cyfr, a także nazw bardzo długich lub opisowych. Choć pandas technicznie dopuszcza niemal dowolne nazwy kolumn, ich nieprzemyślane użycie prowadzi do błędów, nieczytelnego kodu oraz problemów w dalszych etapach analizy i modelowania. Problem ten ujawnia się szczególnie wyraźnie wtedy, gdy użytkownik próbuje korzystać z notacji kropkowej (df.column_name), budować formuły modelowe, pisać potoki przetwarzania lub eksportować dane do innych narzędzi analitycznych. Nazwy kolumn zawierające spacje, znaki -, %, (), #, rozpoczynające się od cyfr lub zawierające znaki typowe dla języków innych niż angielski (np. ś, ć, ń) nie mogą być używane jako poprawne identyfikatory w Pythonie. W efekcie kod staje się mniej czytelny i bardziej podatny na błędy.\nRozważmy przykładowy zbiór danych, w którym nazwy kolumn zostały nadane w sposób typowy dla arkuszy Excel lub raportów biznesowych:\nKod\ndf2 = pd.DataFrame({\n    \"Customer ID\": [1, 2, 3],\n    \"2023 Revenue (€)\": [12000, 34000, 18000],\n    \"Avg Basket Value\": [45.5, 78.2, 33.1],\n    \"% Return\": [0.02, 0.01, 0.05],\n    \"Very long column name describing customer behaviour in detail\": [1, 0, 1]\n})\ndf2\n\n\n\n\n\n\n\n\n\nCustomer ID\n2023 Revenue (€)\nAvg Basket Value\n% Return\nVery long column name describing customer behaviour in detail\n\n\n\n\n0\n1\n12000\n45.5\n0.02\n1\n\n\n1\n2\n34000\n78.2\n0.01\n0\n\n\n2\n3\n18000\n33.1\n0.05\n1\nZ punktu widzenia pandas taki zbiór danych jest poprawny, jednak już próba użycia notacji kropkowej zakończy się błędem:\nKod\ndf2.2023 Revenue (€) # błąd składni\n\n\n\n  Cell In[7], line 1\n    df2.2023 Revenue (€) # błąd składni\n                      ^\nSyntaxError: invalid character '€' (U+20AC)\nKażdorazowo konieczne byłoby odwoływanie się do kolumn przez nawiasy i łańcuchy znaków, co znacząco obniża czytelność kodu:\nKod\ndf2[\"2023 Revenue (€)\"].mean()\n\n\nnp.float64(21333.333333333332)\nDlatego dobrą praktyką w analizie danych jest normalizacja nazw kolumn bezpośrednio po imporcie danych. Najczęściej stosowana konwencja obejmuje użycie wyłącznie małych liter, znaków ASCII, podkreśleń zamiast spacji oraz nazw rozpoczynających się literą. Taki styl jest zgodny z konwencją snake_case powszechnie stosowaną w Pythonie. Podstawowa korekta nazw kolumn może zostać wykonana w kilku krokach. Najpierw usuwa się nadmiarowe spacje, zamienia litery na małe, a spacje na podkreślenia:\nKod\ndf2.columns = (\n    df2.columns\n    .str.strip()\n    .str.lower()\n    .str.replace(r\"\\s+\", \"_\", regex=True)\n)\nJednak w praktyce to zwykle nie wystarcza, ponieważ w nazwach mogą występować znaki specjalne, symbole walut, procenty czy nawiasy. W takim przypadku warto usunąć wszystkie znaki niedozwolone i pozostawić jedynie litery, cyfry i podkreślenia:\nKod\ndf2.columns = df2.columns.str.replace(r\"[^a-zA-Z0-9_]\", \"\", regex=True)\n\n# opcjonalnie: porządkujemy podkreślenia (np. po usunięciu znaków specjalnych)\ndf2.columns = (\n    df2.columns\n    .str.replace(r\"_+\", \"_\", regex=True)\n    .str.strip(\"_\")\n)\ndf2.columns\n\n\nIndex(['customer_id', '2023_revenue', 'avg_basket_value', 'return',\n       'very_long_column_name_describing_customer_behaviour_in_detail'],\n      dtype='object')\nWarto zauważyć, że po czyszczeniu może powstać nazwa będąca słowem kluczowym Pythona (np. return). Jeśli zależy nam na możliwości użycia notacji kropkowej, warto takie przypadki automatycznie zmienić, np. przez dodanie sufiksu:\nKod\nimport keyword\n\ndf2.columns = [\n    f\"{c}_var\" if keyword.iskeyword(c) else c\n    for c in df2.columns\n]\nWciąż jednak pozostaje problem nazw rozpoczynających się od cyfr. W Pythonie identyfikator nie może zaczynać się od liczby, dlatego zaleca się automatyczne dodanie prefiksu, na przykład x_:\nKod\ndf2.columns = [\n    f\"x_{c}\" if (c != \"\" and c[0].isdigit()) else c\n    for c in df2.columns\n]\ndf2.columns\n\n\nIndex(['customer_id', 'x_2023_revenue', 'avg_basket_value', 'return_var',\n       'very_long_column_name_describing_customer_behaviour_in_detail'],\n      dtype='object')\nKolejnym, często niedocenianym problemem są zbyt długie nazwy kolumn. Choć technicznie są poprawne, znacząco utrudniają pracę z kodem, zwłaszcza w dalszych etapach, takich jak budowa modeli, interpretacja współczynników czy wizualizacja wyników. W takich sytuacjach dobrą praktyką jest ręczne lub półautomatyczne skracanie nazw, przy zachowaniu ich semantyki:\nKod\ndf2 = df2.rename(columns={\n    \"very_long_column_name_describing_customer_behaviour_in_detail\": \"customer_behavior_flag\"\n})\ndf2.columns\n\n\nIndex(['customer_id', 'x_2023_revenue', 'avg_basket_value', 'return_var',\n       'customer_behavior_flag'],\n      dtype='object')\nW projektach analitycznych o większej skali często stosuje się funkcję pomocniczą, która automatycznie „czyści” nazwy kolumn według ustalonej reguły:\nKod\ndf2 = pd.DataFrame({\n    \"Customer ID\": [1, 2, 3],\n    \"2023 Revenue (€)\": [12000, 34000, 18000],\n    \"Avg Basket Value\": [45.5, 78.2, 33.1],\n    \"% Return\": [0.02, 0.01, 0.05],\n    \"Very long column name describing customer behaviour in detail\": [1, 0, 1]\n})\n\ndef clean_column_names(df, max_len=40):\n    cleaned = (\n        pd.Index(df.columns).map(str)\n        .str.strip()\n        .str.lower()\n        .str.replace(r\"\\s+\", \"_\", regex=True)\n        .str.replace(r\"[^a-z0-9_]\", \"\", regex=True)\n        .str.replace(r\"_+\", \"_\", regex=True)\n        .str.strip(\"_\")\n    )\n\n    seen = {}\n    final_cols = []\n    for c in cleaned:\n        # pusta nazwa po czyszczeniu\n        if c == \"\":\n            c = \"col\"\n\n        # nazwa zaczyna się od cyfry\n        if c[0].isdigit():\n            c = f\"x_{c}\"\n\n        # słowo kluczowe Pythona\n        if keyword.iskeyword(c):\n            c = f\"{c}_var\"\n\n        # opcjonalnie: skracamy bardzo długie nazwy (np. do pracy z modelami/tabelami)\n        if max_len is not None and len(c) &gt; max_len:\n            c = c[:max_len].rstrip(\"_\")\n            if c == \"\":\n                c = \"col\"\n\n        # unikamy kolizji nazw (np. dwie różne kolumny mogą się „zlać” po czyszczeniu)\n        count = seen.get(c, 0)\n        seen[c] = count + 1\n        if count &gt; 0:\n            suffix = f\"_{count+1}\"\n            base = c\n            if max_len is not None and len(base) + len(suffix) &gt; max_len:\n                base = base[: max_len - len(suffix)].rstrip(\"_\")\n                if base == \"\":\n                    base = \"col\"\n            c = f\"{base}{suffix}\"\n\n        final_cols.append(c)\n\n    df.columns = final_cols\n    return df\n\ndf2 = clean_column_names(df2, max_len = 22)\ndf2\n\n\n\n\n\n\n\n\n\ncustomer_id\nx_2023_revenue\navg_basket_value\nreturn_var\nvery_long_column_name\n\n\n\n\n0\n1\n12000\n45.5\n0.02\n1\n\n\n1\n2\n34000\n78.2\n0.01\n0\n\n\n2\n3\n18000\n33.1\n0.05\n1\nTakie podejście sprawia, że już na wczesnym etapie analizy dane zostają dostosowane do dalszej pracy z modelami, formułami i potokami przetwarzania. Co istotne, normalizacja nazw kolumn nie jest kosmetyką, lecz elementem przygotowania danych, który wpływa na czytelność, reprodukowalność oraz odporność kodu na błędy.\nW kontekście tego kursu warto podkreślić, że praca z „brudnymi” nazwami kolumn jest codziennością analityka danych. Umiejętność ich systematycznego korygowania powinna być traktowana na równi z obsługą braków danych czy standaryzacją zmiennych. Jest to jeden z pierwszych kroków w kierunku tworzenia stabilnych i profesjonalnych analiz.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Przygotowanie i czyszczenie danych</span>"
    ]
  },
  {
    "objectID": "chapters/02-przygotowanie-danych.html#import-i-czyszczenie-danych",
    "href": "chapters/02-przygotowanie-danych.html#import-i-czyszczenie-danych",
    "title": "3  Przygotowanie i czyszczenie danych",
    "section": "",
    "text": "1 Dane przypominają prosty wycinek danych klient–zakupy. Zawierają zmienne liczbowe (np. income_eur, avg_basket_eur, visits_last30), jakościowe (city, segment, device_type, notes), logiczne (has_promo), daty (signup_date, last_purchase_date) oraz zmienną docelową (churned). Celowo wprowadzono braki danych (w tym puste pola i znaczniki NA), wartości odstające (np. nienaturalnie wysokie dochody, koszyki zakupowe i liczby wizyt) oraz błędy jakościowe (np. wiek 5 i 120, ujemne zwroty, rozbieżności w kodowaniu kategorii: spacje i różne wielkości liter).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Przygotowanie i czyszczenie danych</span>"
    ]
  },
  {
    "objectID": "chapters/02-przygotowanie-danych.html#obserwacje-odstające-i-braki-danych",
    "href": "chapters/02-przygotowanie-danych.html#obserwacje-odstające-i-braki-danych",
    "title": "3  Przygotowanie i czyszczenie danych",
    "section": "3.2 Obserwacje odstające i braki danych",
    "text": "3.2 Obserwacje odstające i braki danych\nW praktyce analizy danych obserwacje odstające oraz braki danych są jednymi z najczęstszych źródeł błędnych wniosków i niestabilnych modeli. W EDA traktujemy je jako sygnał diagnostyczny: mogą oznaczać błędy pomiaru lub wprowadzania danych, ale mogą też być prawdziwymi rzadkimi zdarzeniami (np. klienci o bardzo wysokich dochodach). W modelowaniu ML odstające wartości i braki wpływają na estymację parametrów, stabilność uczenia i jakość generalizacji; dlatego kluczowe jest, aby postępowanie z nimi było konsekwentne, udokumentowane oraz wykonywane bez wycieku informacji (wszystkie „uczące się” transformacje dopasowujemy tylko na zbiorze treningowym).\n\n3.2.1 Obserwacje odstające\nObserwacje odstające (outliers) to wartości, które są „nietypowe” względem reszty danych. „Nietypowość” trzeba rozumieć w kontekście: może wynikać z rozkładu, jednostki miary, specyfiki biznesowej oraz tego, jakiego modelu używamy. W EDA zwykle rozróżnia się: (1) odstające wartości w pojedynczej zmiennej (univariate), (2) obserwacje odstające w relacji między zmiennymi (bivariate), oraz (3) odstające obserwacje wielowymiarowo (multivariate).\n\n3.2.1.1 Reguły matematyczne identyfikacji odstających (univariate)\nNajczęściej stosuje się regułę IQR (Tukeya) oraz regułę opartą o standaryzację. W regule IQR wyznaczamy kwartyle \\(Q_1\\) i \\(Q_3\\) oraz rozstęp międzykwartylowy \\(IQR = Q_3 - Q_1\\). Obserwację \\(x\\) uznaje się za odstającą, gdy:\n\\[\nx &lt; Q_1 - 1.5\\cdot IQR \\quad \\text{lub} \\quad x &gt; Q_3 + 1.5\\cdot IQR.\n\\]\nReguła standaryzacyjna używa z-score:\n\\[\nz = \\frac{x-\\mu}{\\sigma}.\n\\]\nTypowo za odstające przyjmuje się obserwacje o \\(|z| &gt; 3\\), ale jest to reguła wrażliwa na odstające wartości, ponieważ \\(\\mu\\) i \\(\\sigma\\) same ulegają zniekształceniu. W danych skośnych (np. dochody) często lepsze są metody odporne.\nOdporna standaryzacja opiera się na medianie i medianowym odchyleniu bezwzględnym MAD. Dla \\(MAD = \\mathrm{median}(|x - \\mathrm{median}(x)|)\\) stosuje się tzw. odporny z-score:\n\\[\nz_{\\mathrm{rob}} = \\frac{x-\\mathrm{median}(x)}{1.4826\\cdot MAD}.\n\\]\nWspółczynnik (1.4826 2) skaluje MAD do odpowiednika odchylenia standardowego przy rozkładzie normalnym.\n2 Liczba 1.4826 pochodzi ze „skalowania” miary MAD tak, aby była porównywalna z odchyleniem standardowym przy rozkładzie normalnym. Dla rozkładu normalnego \\(X \\sim \\mathcal N(\\mu,\\sigma)\\) zachodzi zależność \\(MAD = \\Phi^{-1}(0.75)\\,\\sigma \\approx 0.6745,\\sigma\\) bo dla \\(Z\\sim\\mathcal N(0,1)\\) wartość \\(\\mathrm{median}(|Z|)\\) to dokładnie 75. percentyl rozkładu normalnego: \\(\\Phi^{-1}(0.75)\\). Żeby z MAD zrobić estymator „w skali \\(\\sigma\\)”, mnoży się przez odwrotność tej stałej \\(\\sigma \\approx \\frac{MAD}{0.6745} \\approx 1.4826 \\cdot MAD\\) i stąd bierze się 1.4826 (dokładniej \\(1 / \\Phi^{-1}(0.75)\\)).W naszych danych sensownymi kandydatami do analizy odstających są: income_eur, avg_basket_eur, visits_last30, a także „błędy logiczne” w age i returns_last90.\n\n\nKod\nimport numpy as np\n\nnum_cols = [\"age\", \"income_eur\", \"visits_last30\", \"avg_basket_eur\", \"returns_last90\", \"satisfaction_1_5\"]\n\n# Szybka diagnostyka: podstawowe statystyki + percentyle\ndf[num_cols].describe(percentiles=[0.01, 0.05, 0.95, 0.99]).T\n\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n1%\n5%\n50%\n95%\n99%\nmax\n\n\n\n\nage\n333.0\n38.207207\n14.428125\n5.00\n16.0000\n16.6000\n38.000\n58.4000\n69.0000\n120.0\n\n\nincome_eur\n317.0\n40776.328076\n44393.786000\n4302.00\n8218.6800\n11883.6000\n32632.000\n83047.6000\n177796.6000\n490032.0\n\n\nvisits_last30\n350.0\n6.045714\n2.714320\n0.00\n1.0000\n2.0000\n6.000\n11.0000\n13.0000\n24.0\n\n\navg_basket_eur\n332.0\n52.835060\n48.248423\n16.05\n18.0016\n20.9775\n43.835\n99.6435\n150.4721\n668.8\n\n\nreturns_last90\n350.0\n0.777143\n0.880572\n-1.00\n0.0000\n0.0000\n1.000\n3.0000\n3.0000\n4.0\n\n\nsatisfaction_1_5\n327.0\n3.645260\n0.887628\n1.00\n1.0000\n2.0000\n4.000\n5.0000\n5.0000\n5.0\n\n\n\n\n\n\n\n\n\n3.2.1.2 Reguła IQR dla jednej zmiennej\n\n\nKod\ndef iqr_outliers(s: pd.Series, k: float = 1.5):\n    s = s.dropna()\n    q1, q3 = s.quantile(0.25), s.quantile(0.75)\n    iqr = q3 - q1\n    lower, upper = q1 - k * iqr, q3 + k * iqr\n    return lower, upper\n\nlower_inc, upper_inc = iqr_outliers(df[\"income_eur\"])\nout_income = df[(df[\"income_eur\"] &lt; lower_inc) | (df[\"income_eur\"] &gt; upper_inc)]\nout_income[[\"customer_id\", \"income_eur\", \"city\", \"segment\"]].head(10)\n\n\n\n\n\n\n\n\n\ncustomer_id\nincome_eur\ncity\nsegment\n\n\n\n\n39\n100039\n485953.0\nŁódź\nVIP\n\n\n55\n100055\n146032.0\nŁódź\nB2C\n\n\n59\n100059\n86006.0\nWrocław\nVIP\n\n\n66\n100066\n98746.0\nWrocław\nStudent\n\n\n67\n100067\n85420.0\nWrocław\nB2C\n\n\n82\n100082\n80700.0\nLublin\nB2C\n\n\n83\n100083\n82821.0\nNone\nB2C\n\n\n94\n100094\n490032.0\nLublin\nB2C\n\n\n95\n100095\n86004.0\nPoznań\nB2B\n\n\n151\n100151\n87909.0\nWrocław\nB2C\n\n\n\n\n\n\n\n\n\n3.2.1.3 Odporny z-score (MAD)\n\n\nKod\ndef robust_zscore(s: pd.Series):\n    s = s.astype(float)\n    med = np.nanmedian(s)\n    mad = np.nanmedian(np.abs(s - med))\n    return (s - med) / (1.4826 * mad)\n\ndf[\"income_robust_z\"] = robust_zscore(df[\"income_eur\"])\ndf.loc[df[\"income_robust_z\"].abs() &gt; 3, [\"customer_id\", \"income_eur\", \"income_robust_z\"]].head(10)\n\n\n\n\n\n\n\n\n\ncustomer_id\nincome_eur\nincome_robust_z\n\n\n\n\n39\n100039\n485953.0\n26.659763\n\n\n55\n100055\n146032.0\n6.669043\n\n\n59\n100059\n86006.0\n3.138920\n\n\n66\n100066\n98746.0\n3.888158\n\n\n67\n100067\n85420.0\n3.104457\n\n\n94\n100094\n490032.0\n26.899649\n\n\n95\n100095\n86004.0\n3.138802\n\n\n151\n100151\n87909.0\n3.250835\n\n\n153\n100153\n91242.0\n3.446848\n\n\n155\n100155\n83954.0\n3.018242\n\n\n\n\n\n\n\n\n\n3.2.1.4 „Odstające” jako błąd jakości danych\nCzęść wartości odstających to nie „rzadkie przypadki”, tylko po prostu błędne dane. W data.csv celowo pojawiają się np. wiek 5 i 120 oraz ujemna liczba zwrotów (returns_last90 == -1). To nie są outliery w sensie rozkładu, tylko naruszenia dziedziny wartości.\nW EDA warto wprost zdefiniować reguły walidacji, np.: \\(16 \\leq \\text{age} \\leq 80,\\) \\(\\text{returns\\_last90} \\geq 0.\\)\nW Pythonie:\n\n\nKod\nbad_age = df[(df[\"age\"] &lt; 16) | (df[\"age\"] &gt; 80)]\nbad_returns = df[df[\"returns_last90\"] &lt; 0]\n\nbad_age[[\"customer_id\",\"age\"]].head(), bad_returns[[\"customer_id\",\"returns_last90\"]].head()\n\n\n(     customer_id    age\n 43        100043  120.0\n 105       100105  120.0\n 144       100144    5.0\n 250       100250    5.0\n 340       100340  120.0,\n      customer_id  returns_last90\n 5         100005              -1\n 93        100093              -1\n 203       100203              -1)\n\n\nTakie przypadki najczęściej traktuje się jako: (1) poprawa na podstawie źródła (jeśli możliwa), albo (2) ustawienie na brak (NaN) i późniejsza imputacja, albo (3) usunięcie obserwacji (jeżeli jest ich mało i są ewidentnie błędne).\n\n\n3.2.1.5 Co robimy z obserwacjami odstającymi w EDA i ML\nW EDA celem jest zrozumienie przyczyny i konsekwencji odstających wartości. Zwykle robimy trzy rzeczy: po pierwsze, oceniamy skalę zjawiska (ile obserwacji, w których zmiennych), po drugie, sprawdzamy czy są to błędy (np. wartości niemożliwe), a po trzecie, analizujemy wpływ na wnioski (np. średnie, korelacje, wykresy). Warto pamiętać, że średnia i odchylenie standardowe są wrażliwe, dlatego do EDA często stosuje się medianę i IQR.\nW ML decyzja zależy od modelu i celu. Dla modeli liniowych i dyskryminacyjnych (np. regresja logistyczna, LDA/QDA) odstające wartości mogą znacząco zaburzyć dopasowanie, dlatego typowe strategie to: transformacje (np. \\(\\log\\) dla income_eur), winsoryzacja/obcięcie ogonów, zastosowanie skalowania odpornego (np. RobustScaler), albo świadome usunięcie obserwacji błędnych. Dla drzew i ich zespołów (Random Forest, boosting) pojedyncze ekstremalne wartości często są mniej groźne, ale nadal mogą wpływać na podziały lub na stabilność w małych próbkach. Dla metod odległościowych i SVM skalowanie jest kluczowe: outliery w skali potrafią zdominować metrykę lub margines.\nNajczęściej spotykane „operacyjne” podejścia to:\n\ntransformacja rozkładu (np. \\(\\log\\), Box–Cox/Yeo–Johnson),\nwinsoryzacja: obcięcie do percentyli (np. 1%–99%),\nflaga odstających jako dodatkowa cecha (model uczy się, że to przypadek nietypowy),\nusunięcie tylko wtedy, gdy jest to błąd lub obserwacja spoza domeny problemu.\n\nPrzykład winsoryzacji (percentyle) dla income_eur:\n\n\nKod\np01, p99 = df[\"income_eur\"].quantile([0.01, 0.99])\ndf[\"income_eur_capped\"] = df[\"income_eur\"].clip(lower=p01, upper=p99)\n\n\n\n\n\n3.2.2 Braki danych i imputacja\nBraki danych (missing values) mogą wynikać z problemów pomiaru, integracji źródeł, błędów ETL, ale także z „logiki procesu” (np. klient nie ma ocen satysfakcji, bo nie wypełnił ankiety). W EDA kluczowe jest rozpoznanie: (1) gdzie braki występują, (2) ile ich jest, (3) czy są zależne od innych zmiennych (braki „systematyczne”). W ML brak danych wymaga decyzji, ponieważ większość klasycznych modeli nie obsługuje NaN wprost.\nNa początek warto policzyć braki:\n\n\nKod\nna_counts = df.isna().sum().sort_values(ascending=False)\nna_share = (df.isna().mean().sort_values(ascending=False) * 100).round(2)\npd.DataFrame({\"missing_count\": na_counts, \"missing_%\": na_share})\n\n\n\n\n\n\n\n\n\nmissing_count\nmissing_%\n\n\n\n\nincome_eur_capped\n33\n9.43\n\n\nincome_robust_z\n33\n9.43\n\n\nincome_eur\n33\n9.43\n\n\nsatisfaction_1_5\n23\n6.57\n\n\ndevice_type\n21\n6.00\n\n\navg_basket_eur\n18\n5.14\n\n\nage\n17\n4.86\n\n\ncity\n17\n4.86\n\n\nnotes\n16\n4.57\n\n\nhas_promo\n0\n0.00\n\n\nchurned\n0\n0.00\n\n\ncustomer_id\n0\n0.00\n\n\nreturns_last90\n0\n0.00\n\n\nsignup_date\n0\n0.00\n\n\nsegment\n0\n0.00\n\n\nlast_purchase_date\n0\n0.00\n\n\nvisits_last30\n0\n0.00\n\n\n\n\n\n\n\n\n3.2.2.1 Co robimy z brakami w EDA i ML\nW EDA braki traktujemy jako informację o jakości danych i procesie ich powstawania. Sprawdzamy, czy braki są losowe, czy dotyczą specyficznych segmentów (np. brak income_eur tylko dla jednego segmentu), oraz czy ich usunięcie nie zmieni populacji (bias). W ML trzeba z kolei zadbać o to, aby imputacja była wykonywana bez wycieku informacji: parametry imputacji wyznaczamy na zbiorze treningowym i stosujemy do walidacyjnego/testowego.\nDecyzje praktyczne obejmują: usuwanie cech/wierszy (gdy braków jest bardzo dużo), imputację prostą (średnia/mediana/moda), imputację warunkową (np. mediana w grupach), metody oparte na podobieństwie (k-NN), metody iteracyjne (MICE), a także dodawanie wskaźników braków (missing indicators), które pozwalają modelowi uczyć się samego faktu braku jako sygnału.\n\n\n3.2.2.2 Metody imputacji\n\nImputacja stałą lub modą (kategorie i zmienne dyskretne)\n\nDla zmiennych kategorycznych często stosuje się modę lub specjalną kategorię ‘missing’. Dlaczego to bywa dobre? Ponieważ w wielu danych fakt, że czegoś brakuje, jest informacją samą w sobie. Na przykład: brak miasta może oznaczać, że klient nie podał danych adresowych, co może korelować z innymi cechami lub nawet z churned. Jeśli zamienimy brak na osobną kategorię “missing”, model może się tego „nauczyć”.\n\n\nKod\ndf[\"city\"] = df[\"city\"].fillna(\"missing\")\ndf[\"device_type\"] = df[\"device_type\"].fillna(\"missing\")\n\n\n\nImputacja medianą/średnią (zmienne liczbowe)\n\nJest to metoda szybka, ale ignoruje zależności między cechami:\n\n\nKod\ndf[\"income_eur\"] = df[\"income_eur\"].fillna(df[\"income_eur\"].median())\n\n\n\nImputacja grupowa (warunkowa)\n\nBardzo użyteczna, gdy rozkłady różnią się w segmentach, np. dochody w segmentach VIP vs Student:\n\n\nKod\ndf[\"income_eur\"] = df[\"income_eur\"].fillna(\n    df.groupby(\"segment\")[\"income_eur\"].transform(\"median\")\n)\n\n\n\nk-NN Imputer (podobieństwo obserwacji)\n\nWykorzystuje sąsiadów w przestrzeni cech do uzupełniania braków. Wymaga skalowania i pracy na liczbach.\n\n\nKod\nfrom sklearn.impute import KNNImputer\n\nnum_features = [\"age\", \"income_eur\", \"visits_last30\", \"avg_basket_eur\", \"returns_last90\", \"satisfaction_1_5\"]\nimputer = KNNImputer(n_neighbors=5, weights=\"distance\") # obserwacje imputujące będą ważone odległością\n\ndf[num_features] = imputer.fit_transform(df[num_features])\n\n\n\nIterative Imputer (MICE – imputacja modelowa)\n\nUzupełnia jedną zmienną na podstawie pozostałych (iteracyjnie), co lepiej zachowuje relacje w danych. Domyślną opcją IterativeImputer jest regresja liniowa z regularyzacją typu ridge w ujęciu bayesowskim (sklearn.linear_model.BayesianRidge). Można jednak wybrać inne techniki jako model uzupelniający\n\n\nKod\nfrom sklearn.ensemble import RandomForestRegressor\nimp = IterativeImputer(estimator=RandomForestRegressor(n_estimators=200, random_state=42))\n\n\n\n\nKod\nfrom sklearn.experimental import enable_iterative_imputer  # noqa: F401\nfrom sklearn.impute import IterativeImputer\n\nimp = IterativeImputer(random_state=42, max_iter=20)\ndf[num_features] = imp.fit_transform(df[num_features])\n\n\n\nWskaźniki braków jako cechy\n\nCzasem sam fakt braku jest informacyjny (np. brak satysfakcji może korelować z churn). Można dodać flagi:\n\n\nKod\n# ponieważ wcześniej zmienna income_eur była zastępowana medianami to poniższy kod da same 0 dla income_missing. Podobnie dla zmiennej satisfaction_1_5\ndf[\"income_missing\"] = df[\"income_eur\"].isna().astype(int)\ndf[\"satisfaction_missing\"] = df[\"satisfaction_1_5\"].isna().astype(int)\n\n\nW praktyce (szczególnie w ML) flagi braków często działają dobrze w połączeniu z imputacją, bo pozwalają modelowi rozróżnić „wartość prawdziwą” od „wartości wstawionej”.\n\n\n\n\n\n\nOstrzeżenie\n\n\n\nW modelowaniu nie imputujemy „ręcznie na całym df”, tylko budujemy potok, który dopasowuje imputery na treningu, a potem stosuje na walidacji/teście:\n\n\nKod\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.linear_model import LogisticRegression\n\nX = df.drop(columns=[\"churned\"])\ny = df[\"churned\"]\n\n# Uwaga: pandas może trzymać braki jako `pd.NA` (np. typy StringDtype/boolean).\n# scikit-learn oczekuje braków jako `np.nan` i potrafi się wysypać na `pd.NA`.\nX = X.replace({pd.NA: np.nan})\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n\nnum_cols = [\"age\", \"income_eur\", \"visits_last30\", \"avg_basket_eur\", \"returns_last90\", \"satisfaction_1_5\"]\ncat_cols = [\"city\", \"segment\", \"device_type\", \"has_promo\", \"notes\"]\n\n# (opcjonalnie) upewniamy się, że kategorie są zwykłym `object`, a nie np. `string[python]` z `pd.NA`\nX_train[cat_cols] = X_train[cat_cols].astype(\"object\")\nX_test[cat_cols] = X_test[cat_cols].astype(\"object\")\n\npreprocess = ColumnTransformer(\n    transformers=[\n        (\"num\", Pipeline(steps=[\n            (\"imputer\", SimpleImputer(strategy=\"median\"))\n        ]), num_cols),\n        (\"cat\", Pipeline(steps=[\n            (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n            (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n        ]), cat_cols),\n    ],\n    remainder=\"drop\"\n)\n\nmodel = Pipeline(steps=[\n    (\"prep\", preprocess),\n    (\"clf\", LogisticRegression(max_iter=200))\n])\n\nmodel.fit(X_train, y_train)\n\n\nPipeline(steps=[('prep',\n                 ColumnTransformer(transformers=[('num',\n                                                  Pipeline(steps=[('imputer',\n                                                                   SimpleImputer(strategy='median'))]),\n                                                  ['age', 'income_eur',\n                                                   'visits_last30',\n                                                   'avg_basket_eur',\n                                                   'returns_last90',\n                                                   'satisfaction_1_5']),\n                                                 ('cat',\n                                                  Pipeline(steps=[('imputer',\n                                                                   SimpleImputer(strategy='most_frequent')),\n                                                                  ('onehot',\n                                                                   OneHotEncoder(handle_unknown='ignore'))]),\n                                                  ['city', 'segment',\n                                                   'device_type', 'has_promo',\n                                                   'notes'])])),\n                ('clf', LogisticRegression(max_iter=200))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  Pipeline?Documentation for PipelineiFittedPipeline(steps=[('prep',\n                 ColumnTransformer(transformers=[('num',\n                                                  Pipeline(steps=[('imputer',\n                                                                   SimpleImputer(strategy='median'))]),\n                                                  ['age', 'income_eur',\n                                                   'visits_last30',\n                                                   'avg_basket_eur',\n                                                   'returns_last90',\n                                                   'satisfaction_1_5']),\n                                                 ('cat',\n                                                  Pipeline(steps=[('imputer',\n                                                                   SimpleImputer(strategy='most_frequent')),\n                                                                  ('onehot',\n                                                                   OneHotEncoder(handle_unknown='ignore'))]),\n                                                  ['city', 'segment',\n                                                   'device_type', 'has_promo',\n                                                   'notes'])])),\n                ('clf', LogisticRegression(max_iter=200))])  prep: ColumnTransformer?Documentation for prep: ColumnTransformerColumnTransformer(transformers=[('num',\n                                 Pipeline(steps=[('imputer',\n                                                  SimpleImputer(strategy='median'))]),\n                                 ['age', 'income_eur', 'visits_last30',\n                                  'avg_basket_eur', 'returns_last90',\n                                  'satisfaction_1_5']),\n                                ('cat',\n                                 Pipeline(steps=[('imputer',\n                                                  SimpleImputer(strategy='most_frequent')),\n                                                 ('onehot',\n                                                  OneHotEncoder(handle_unknown='ignore'))]),\n                                 ['city', 'segment', 'device_type', 'has_promo',\n                                  'notes'])]) num['age', 'income_eur', 'visits_last30', 'avg_basket_eur', 'returns_last90', 'satisfaction_1_5']  SimpleImputer?Documentation for SimpleImputerSimpleImputer(strategy='median') cat['city', 'segment', 'device_type', 'has_promo', 'notes']  SimpleImputer?Documentation for SimpleImputerSimpleImputer(strategy='most_frequent')  OneHotEncoder?Documentation for OneHotEncoderOneHotEncoder(handle_unknown='ignore')  LogisticRegression?Documentation for LogisticRegressionLogisticRegression(max_iter=200) \n\n\nTo podejście jest spójne z „dobrą praktyką” w ML: wszystkie transformacje uczące się parametrów (imputacja, skalowanie, kodowanie) są częścią pipeline i nie „podglądają” testu.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Przygotowanie i czyszczenie danych</span>"
    ]
  },
  {
    "objectID": "chapters/02-przygotowanie-danych.html#eda-i-preprocessing-danych",
    "href": "chapters/02-przygotowanie-danych.html#eda-i-preprocessing-danych",
    "title": "3  Przygotowanie i czyszczenie danych",
    "section": "3.3 EDA i preprocessing danych",
    "text": "3.3 EDA i preprocessing danych\nTen rozdział porządkuje praktyczny przebieg pracy „od surowego pliku do macierzy cech gotowej dla modelu”. Najpierw wykonujemy EDA (ang. exploratory data analysis), aby zrozumieć dane (struktura, rozkłady, zależności, potencjalne problemy jakościowe), a dopiero potem przechodzimy do preprocessingu, który ma zapewnić poprawne działanie modeli i powtarzalność całego procesu. Kwestie braków danych i obserwacji odstających są tu jedynie sygnalizowane (szczegółowe strategie były w osobnych podrozdziałach).\n\n3.3.1 Szybki „sanity check” po imporcie\nPo wczytaniu danych pierwszym krokiem jest szybka kontrola struktury: rozmiar zbioru, typy kolumn, podstawowe podsumowania i liczba braków. W tym momencie warto również ujednolicić nazwy kolumn (np. snake_case), aby dalszy kod był czytelny i odporny na błędy.\n\n\nKod\ndf = pd.read_csv(\n    \"data.csv\",\n    na_values=[\"\", \"NA\", \"N/A\", \"null\", \"None\"],\n    parse_dates=[\"signup_date\", \"last_purchase_date\"],\n    encoding=\"utf-8\"\n)\n\ndf.shape\ndf.info()\ndf.head()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 350 entries, 0 to 349\nData columns (total 15 columns):\n #   Column              Non-Null Count  Dtype         \n---  ------              --------------  -----         \n 0   customer_id         350 non-null    int64         \n 1   signup_date         350 non-null    datetime64[ns]\n 2   last_purchase_date  350 non-null    datetime64[ns]\n 3   age                 333 non-null    float64       \n 4   income_eur          317 non-null    float64       \n 5   city                333 non-null    object        \n 6   segment             350 non-null    object        \n 7   device_type         329 non-null    object        \n 8   visits_last30       350 non-null    float64       \n 9   avg_basket_eur      332 non-null    float64       \n 10  returns_last90      350 non-null    float64       \n 11  has_promo           350 non-null    bool          \n 12  satisfaction_1_5    327 non-null    float64       \n 13  churned             350 non-null    int64         \n 14  notes               334 non-null    object        \ndtypes: bool(1), datetime64[ns](2), float64(6), int64(2), object(4)\nmemory usage: 38.8+ KB\n\n\n\n\n\n\n\n\n\ncustomer_id\nsignup_date\nlast_purchase_date\nage\nincome_eur\ncity\nsegment\ndevice_type\nvisits_last30\navg_basket_eur\nreturns_last90\nhas_promo\nsatisfaction_1_5\nchurned\nnotes\n\n\n\n\n0\n100000\n2022-04-08\n2022-10-19\nNaN\n72955.0\nWarszawa\nB2B\ndesktop\n8.0\n39.88\n1.0\nFalse\n4.0\n0\nklient powracający\n\n\n1\n100001\n2024-04-27\n2024-05-13\n52.0\n24863.0\nKraków\nB2C\nmobile\n12.0\n67.80\n1.0\nFalse\n3.0\n0\nbrak uwag\n\n\n2\n100002\n2023-12-18\n2024-04-05\nNaN\n25333.0\nWrocław\nB2C\ndesktop\n6.0\n27.04\n1.0\nTrue\n4.0\n0\nreklamacja\n\n\n3\n100003\n2023-04-26\n2023-10-01\n16.0\nNaN\nŁódź\nB2B\nmobile\n6.0\n149.03\n1.0\nFalse\n3.0\n1\nbrak uwag\n\n\n4\n100004\n2023-04-20\n2024-04-07\n42.0\n27954.0\nWarszawa\nB2C\ndesktop\n7.0\n49.73\n1.0\nTrue\n3.0\n0\nNaN\n\n\n\n\n\n\n\n\n\nKod\n# szybki przegląd braków\n(df.isna().mean().sort_values(ascending=False) * 100).round(2)\n\n\nincome_eur            9.43\nsatisfaction_1_5      6.57\ndevice_type           6.00\navg_basket_eur        5.14\nage                   4.86\ncity                  4.86\nnotes                 4.57\ncustomer_id           0.00\nsignup_date           0.00\nlast_purchase_date    0.00\nsegment               0.00\nvisits_last30         0.00\nreturns_last90        0.00\nhas_promo             0.00\nchurned               0.00\ndtype: float64\n\n\nNa tym etapie sprawdzamy też duplikaty oraz ewentualne naruszenia prostych reguł domenowych (np. ujemne wartości tam, gdzie nie powinny wystąpić). Jeśli widzimy ewidentne błędy, zazwyczaj oznaczamy je do późniejszej korekty, zamiast natychmiast usuwać dane „w ciemno”.\n\n\nKod\ndf.duplicated().sum()\n\n\nnp.int64(0)\n\n\n\n\n3.3.2 EDA - rozkłady i struktura zmiennych\nEDA zaczynamy od uporządkowania typów zmiennych: rozdzielenia zmiennych liczbowych i kategorycznych oraz rozpoznania, które kolumny są zmiennymi docelowymi, identyfikatorami lub metadanymi (np. daty). W naszym data.csv naturalnie wyróżniają się liczby (np. income_eur, avg_basket_eur), kategorie (np. segment, city) i daty.\n\n\nKod\nnum_cols = df.select_dtypes(include=\"number\").columns.tolist()\ncat_cols = df.select_dtypes(include=[\"object\", \"string\", \"bool\"]).columns.tolist()\n\nnum_cols, cat_cols\n\n\n(['customer_id',\n  'age',\n  'income_eur',\n  'visits_last30',\n  'avg_basket_eur',\n  'returns_last90',\n  'satisfaction_1_5',\n  'churned'],\n ['city', 'segment', 'device_type', 'has_promo', 'notes'])\n\n\nW analizie rozkładów dla zmiennych liczbowych bardzo dobre są histogramy z nakładką gęstości oraz wykresy pudełkowe. Warto pamiętać, że część zmiennych może być skośna (np. dochody), więc czasem sensowne jest pokazanie wykresu w skali logarytmicznej.\n\n\nKod\nimport matplotlib.pyplot as plt\n\ndef univariate_view(series, title):\n    fig = plt.figure(figsize=(10, 4))\n    gs = fig.add_gridspec(1, 2, width_ratios=[3, 1])\n\n    ax1 = fig.add_subplot(gs[0, 0])\n    ax1.hist(series.dropna(), bins=30)\n    ax1.set_title(f\"Histogram: {title}\")\n    ax1.set_xlabel(title)\n    ax1.set_ylabel(\"Liczność\")\n\n    ax2 = fig.add_subplot(gs[0, 1])\n    ax2.boxplot(series.dropna(), vert=True)\n    ax2.set_title(\"Boxplot\")\n    ax2.set_xticks([])\n\n    plt.tight_layout()\n    plt.show()\n\nunivariate_view(df[\"income_eur\"], \"income_eur\")\nunivariate_view(df[\"avg_basket_eur\"], \"avg_basket_eur\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDla zmiennych kategorycznych kluczowe jest porównanie częstości. Prosty wykres słupkowy umożliwia identyfikację rzadkich kategorii oraz niespójności kodowania (np. różne wielkości liter, dodatkowe spacje).\n\n\nKod\ndef top_categories(series, title, top=10):\n    counts = series.value_counts(dropna=False).head(top)\n    plt.figure(figsize=(10, 4))\n    plt.bar(counts.index.astype(str), counts.values)\n    plt.title(f\"Top {top} kategorii: {title}\")\n    plt.xticks(rotation=30, ha=\"right\")\n    plt.ylabel(\"Liczność\")\n    plt.tight_layout()\n    plt.show()\n\ntop_categories(df[\"segment\"], \"segment\")\ntop_categories(df[\"device_type\"], \"device_type\")\ntop_categories(df[\"city\"], \"city\", top=12)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.3.3 EDA - zależności między zmiennymi\nW celu odkrycia zależności pomiędzy cechami wykreślamy macierz korelacji.\n\n\nKod\n# macierz korelacji (numeryczne)\ncorr = df[num_cols].corr(numeric_only=True)\n\nfig, ax = plt.subplots(figsize=(8, 6))\nim = ax.imshow(corr, aspect=\"auto\")\nfig.colorbar(im, ax=ax)\n\nax.set_xticks(range(len(corr.columns)))\nax.set_xticklabels(corr.columns, rotation=30, ha=\"right\")\nax.set_yticks(range(len(corr.index)))\nax.set_yticklabels(corr.index)\nax.set_title(\"Macierz korelacji (numeryczne)\")\n\n# wartości korelacji w komórkach\nfor i in range(corr.shape[0]):\n    for j in range(corr.shape[1]):\n        val = corr.iloc[i, j]\n        color = \"black\" if abs(val) &gt;= 0.5 else \"white\"\n        ax.text(j, i, f\"{val:.2f}\", ha=\"center\", va=\"center\", color=color, fontsize=8)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nW kontekście klasyfikacji warto też zobaczyć, jak rozkłady cech różnią się między klasami (tu: churned). Bardzo czytelny jest wykres pudełkowy/wiolinowy „cecha vs klasa” albo porównanie histogramów. Poniżej przykład boxplotu cechy w podziale na churned.\n\n\nKod\ndef box_by_target(df, feature, target=\"churned\"):\n    groups = [df.loc[df[target] == t, feature].dropna() for t in sorted(df[target].dropna().unique())]\n    plt.figure(figsize=(7, 4))\n    plt.boxplot(groups, labels=[f\"{target}={t}\" for t in sorted(df[target].dropna().unique())])\n    plt.title(f\"{feature} względem {target}\")\n    plt.ylabel(feature)\n    plt.tight_layout()\n    plt.show()\n\nbox_by_target(df, \"avg_basket_eur\")\nbox_by_target(df, \"visits_last30\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDla zmiennych kategorycznych przydatne są wykresy udziałów klas w kategoriach (np. wskaźnik churn w segmentach). W prostym wariancie liczymy średnią churned w grupach (to jest odsetek „1” w danej kategorii) i wykreślamy słupki.\n\n\nKod\nrate = df.groupby(\"segment\")[\"churned\"].mean().sort_values(ascending=False)\n\nplt.figure(figsize=(7, 4))\nplt.bar(rate.index.astype(str), rate.values)\nplt.title(\"Średni churn w segmentach\")\nplt.ylabel(\"P(churned=1)\")\nplt.xticks(rotation=20, ha=\"right\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nJeżeli w danych występują daty, dobrze jest wykonać podstawowe przekroje czasowe: liczba rejestracji w czasie, średni koszyk w czasie, itp. Pozwala to zidentyfikować sezonowość, zmiany procesu zbierania danych, kampanie marketingowe.\n\n\nKod\ntmp = df.copy()\ntmp[\"signup_month\"] = tmp[\"signup_date\"].dt.to_period(\"M\").dt.to_timestamp()\n\ncounts = tmp.groupby(\"signup_month\")[\"customer_id\"].count()\n\nplt.figure(figsize=(10, 4))\nplt.plot(counts.index, counts.values)\nplt.title(\"Liczba rejestracji w czasie\")\nplt.ylabel(\"Liczność\")\nplt.xlabel(\"Miesiąc\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nNa tym etapie jedynie notujemy, czy widać: braki danych (np. całe grupy z brakami), wartości podejrzanie ekstremalne lub naruszenia domeny. Szczegółowe postępowanie (imputacja, outliery) realizujemy według strategii opisanych w dedykowanych podrozdziałach.\n\n3.3.3.1 Narzędzia i biblioteki\nW praktycznej EDA najczęściej bazuje się na zestawie narzędzi „rdzeniowych”: pandas służy do przeglądu struktury danych, typów, braków, agregacji i podstawowych statystyk, natomiast matplotlib i seaborn odpowiadają za wizualizacje. matplotlib daje pełną kontrolę nad wykresem i jest dobry do precyzyjnych, publikacyjnych rysunków, a seaborn pozwala szybciej tworzyć estetyczne wykresy statystyczne typowe dla EDA (rozkłady, zależności, porównania grup). Jeżeli zależy Ci na interaktywności (zoom, podgląd wartości, selekcja), warto sięgnąć po plotly albo altair, które dobrze sprawdzają się w notebookach i podczas zajęć, bo ułatwiają „badanie danych w locie”.\nDo szybkiej diagnostyki całego zbioru danych, bez ręcznego pisania wielu wykresów i tabel, przydatne są biblioteki raportujące: ydata-profiling (dawniej pandas-profiling) oraz sweetviz. Generują one raporty HTML z podsumowaniami rozkładów, braków, korelacji i potencjalnych problemów jakościowych, a sweetviz dodatkowo dobrze nadaje się do porównywania zbiorów (np. train vs test) lub analiz w odniesieniu do zmiennej docelowej. Z kolei dtale daje interfejs „jak arkusz kalkulacyjny”, umożliwiając interaktywne filtrowanie i szybkie sprawdzanie danych w przeglądarce, co bywa bardzo wygodne w dydaktyce i debugowaniu.\nJeżeli chcemy rozszerzyć EDA o wątki jakości danych i zależności specyficznych dla danych mieszanych, pomocne są narzędzia wyspecjalizowane: missingno służy do wizualizacji wzorców braków (gdzie i jak współwystępują), great_expectations pozwala formalizować reguły jakości danych i testować je w sposób powtarzalny, a phik bywa użyteczne do badania zależności również wtedy, gdy zmienne są kategoryczne lub relacje są nieliniowe. W praktyce dobry „stos” do Twojego kursu to: pandas + (matplotlib/seaborn) jako fundament, missingno do braków oraz ydata-profiling lub sweetviz do szybkiego raportu całości.\n\n\n\n3.3.4 Preprocessing - przygotowanie macierzy cech do modeli\nPo EDA przechodzimy do preprocessingu, którego cel jest stricte „modelowy”: wytworzyć wejście w formacie akceptowalnym przez algorytmy oraz zapewnić powtarzalność transformacji. Kluczowa zasada brzmi: transformacje, które uczą się parametrów (imputer, scaler, encoder), dopasowujemy na zbiorze treningowym i stosujemy do walidacji/testu – najlepiej w pipeline.\n\n3.3.4.1 Podział na zbiory treningowy i testowy\nPodział na train/test wykonujemy przed dopasowaniem transformacji. Dla klasyfikacji zwykle stosujemy stratyfikację.\n\n\nKod\nX = df.drop(columns=[\"churned\"])\ny = df[\"churned\"]\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\n\n\nJeżeli dodatkowo używamy walidacji, możemy wydzielić X_val z X_train analogicznie albo korzystać z cross-validation.\n\n\n3.3.4.2 Kodowanie kategorii\nWiększość klasycznych modeli (regresja logistyczna, SVM, LDA/QDA) wymaga liczb, więc zmienne kategoryczne kodujemy. Standardem jest one-hot encoding z obsługą nieznanych kategorii w teście.\n\n\n3.3.4.3 Skalowanie zmiennych liczbowych\nSkalowanie jest szczególnie ważne dla metod opartych na odległości i marginesie (k-NN, SVM) oraz dla modeli liniowych przy regularyzacji. Dla drzew i metod zespołowych zwykle nie jest konieczne, ale utrzymywanie jednolitego pipeline’u ułatwia porównania.\n\n\n3.3.4.4 Pipeline preprocessingu\nPoniżej wzorcowa konstrukcja preprocessingu: dla liczb imputacja (tu jako placeholder) + skalowanie; dla kategorii imputacja (placeholder) + one-hot encoding. Strategię imputacji i obsługę outlierów możesz później łatwo podmienić.\n\n\nKod\nfrom sklearn.preprocessing import StandardScaler\n\nnum_cols = [\"age\", \"income_eur\", \"visits_last30\", \"avg_basket_eur\", \"returns_last90\", \"satisfaction_1_5\"]\ncat_cols = [\"city\", \"segment\", \"device_type\", \"has_promo\", \"notes\"]\n\nnumeric_pipe = Pipeline(steps=[\n    (\"imputer\", SimpleImputer(strategy=\"median\")),   # szczegóły w rozdziale o imputacji\n    (\"scaler\", StandardScaler())\n])\n\ncategorical_pipe = Pipeline(steps=[\n    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n])\n\npreprocess = ColumnTransformer(\n    transformers=[\n        (\"num\", numeric_pipe, num_cols),\n        (\"cat\", categorical_pipe, cat_cols),\n    ],\n    remainder=\"drop\"\n)\n\n\nNastępnie łączymy preprocessing z modelem w jeden pipeline. Dzięki temu cały proces jest powtarzalny i bezpieczny względem testu:\n\n\nKod\nclf = Pipeline(steps=[\n    (\"prep\", preprocess),\n    (\"model\", LogisticRegression(max_iter=300))\n])\n\nclf.fit(X_train, y_train)\n\n\nPipeline(steps=[('prep',\n                 ColumnTransformer(transformers=[('num',\n                                                  Pipeline(steps=[('imputer',\n                                                                   SimpleImputer(strategy='median')),\n                                                                  ('scaler',\n                                                                   StandardScaler())]),\n                                                  ['age', 'income_eur',\n                                                   'visits_last30',\n                                                   'avg_basket_eur',\n                                                   'returns_last90',\n                                                   'satisfaction_1_5']),\n                                                 ('cat',\n                                                  Pipeline(steps=[('imputer',\n                                                                   SimpleImputer(strategy='most_frequent')),\n                                                                  ('onehot',\n                                                                   OneHotEncoder(handle_unknown='ignore'))]),\n                                                  ['city', 'segment',\n                                                   'device_type', 'has_promo',\n                                                   'notes'])])),\n                ('model', LogisticRegression(max_iter=300))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  Pipeline?Documentation for PipelineiFittedPipeline(steps=[('prep',\n                 ColumnTransformer(transformers=[('num',\n                                                  Pipeline(steps=[('imputer',\n                                                                   SimpleImputer(strategy='median')),\n                                                                  ('scaler',\n                                                                   StandardScaler())]),\n                                                  ['age', 'income_eur',\n                                                   'visits_last30',\n                                                   'avg_basket_eur',\n                                                   'returns_last90',\n                                                   'satisfaction_1_5']),\n                                                 ('cat',\n                                                  Pipeline(steps=[('imputer',\n                                                                   SimpleImputer(strategy='most_frequent')),\n                                                                  ('onehot',\n                                                                   OneHotEncoder(handle_unknown='ignore'))]),\n                                                  ['city', 'segment',\n                                                   'device_type', 'has_promo',\n                                                   'notes'])])),\n                ('model', LogisticRegression(max_iter=300))])  prep: ColumnTransformer?Documentation for prep: ColumnTransformerColumnTransformer(transformers=[('num',\n                                 Pipeline(steps=[('imputer',\n                                                  SimpleImputer(strategy='median')),\n                                                 ('scaler', StandardScaler())]),\n                                 ['age', 'income_eur', 'visits_last30',\n                                  'avg_basket_eur', 'returns_last90',\n                                  'satisfaction_1_5']),\n                                ('cat',\n                                 Pipeline(steps=[('imputer',\n                                                  SimpleImputer(strategy='most_frequent')),\n                                                 ('onehot',\n                                                  OneHotEncoder(handle_unknown='ignore'))]),\n                                 ['city', 'segment', 'device_type', 'has_promo',\n                                  'notes'])]) num['age', 'income_eur', 'visits_last30', 'avg_basket_eur', 'returns_last90', 'satisfaction_1_5']  SimpleImputer?Documentation for SimpleImputerSimpleImputer(strategy='median')  StandardScaler?Documentation for StandardScalerStandardScaler() cat['city', 'segment', 'device_type', 'has_promo', 'notes']  SimpleImputer?Documentation for SimpleImputerSimpleImputer(strategy='most_frequent')  OneHotEncoder?Documentation for OneHotEncoderOneHotEncoder(handle_unknown='ignore')  LogisticRegression?Documentation for LogisticRegressionLogisticRegression(max_iter=300)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Przygotowanie i czyszczenie danych</span>"
    ]
  },
  {
    "objectID": "chapters/02-przygotowanie-danych.html#inżynieria-cech",
    "href": "chapters/02-przygotowanie-danych.html#inżynieria-cech",
    "title": "3  Przygotowanie i czyszczenie danych",
    "section": "3.4 Inżynieria cech",
    "text": "3.4 Inżynieria cech\nInżynieria cech (feature engineering) to etap, w którym surowe dane przekształcamy w reprezentację bardziej użyteczną dla modeli uczenia maszynowego. Jej celem nie jest „upiększanie” danych, lecz zwiększenie ilości informacji, jaką model może efektywnie wykorzystać, oraz dopasowanie formatu cech do wymagań algorytmów. W praktyce inżynieria cech obejmuje zarówno proste transformacje (np. logarytmowanie zmiennej skośnej), jak i budowę cech pochodnych, interakcji, agregacji czy cech czasowych. W tym kursie inżynieria cech jest szczególnie istotna, ponieważ omawiane modele klasyczne (regresja logistyczna, LDA/QDA, SVM, k-NN) są wrażliwe na sposób reprezentacji danych i często korzystają bardziej z dobrze zaprojektowanych cech niż z samej „mocy” algorytmu.\n\n3.4.1 Ujednolicanie i czyszczenie kategorii jako element inżynierii cech\nZanim zakodujemy kategorie, warto je znormalizować, ponieważ w danych rzeczywistych często występują spacje, różne wielkości liter oraz warianty zapisu tej samej kategorii. To nie jest „czyszczenie kosmetyczne” – bez tego model będzie traktował np. Mobile i mobile jako różne wartości, co rozbije informację na wiele sztucznych kategorii.\n\n\nKod\n# ujednolicenie prostych kolumn kategorycznych\nfor col in [\"city\", \"segment\", \"device_type\", \"notes\"]:\n    df[col] = (df[col]\n               .astype(\"string\")\n               .str.strip()\n               .str.lower())\n\n# przykładowo: ujednolicenie nazw urządzeń\ndf[\"device_type\"] = df[\"device_type\"].replace({\"mobile\": \"mobile\", \"desktop\": \"desktop\", \"tablet\": \"tablet\"})\n\n\nW wielu projektach opłaca się również „skleić” rzadkie kategorie do wspólnej kategorii other, aby ograniczyć wymiar po kodowaniu one-hot.\n\n\nKod\ntop_cities = df[\"city\"].value_counts().head(6).index\ndf[\"city_reduced\"] = df[\"city\"].where(df[\"city\"].isin(top_cities), other=\"other\")\n\n\n\n\n3.4.2 Cechy czasowe z dat\nDaty rzadko trafiają do modelu w postaci surowej. Zwykle tworzy się z nich cechy o sensie behawioralnym: czas od zdarzenia, miesiąc, dzień tygodnia, czy wskaźniki sezonowe. Dla danych klient–zakupy naturalne są cechy typu tenure oraz recency.\nNiech „datą odniesienia” będzie np. maksymalna obserwowana data zakupu (w realnym projekcie byłby to moment as-of).\n\n\nKod\nimport pandas as pd\n\nas_of = df[\"last_purchase_date\"].max()\n\ndf[\"tenure_days\"] = (as_of - df[\"signup_date\"]).dt.days\ndf[\"recency_days\"] = (as_of - df[\"last_purchase_date\"]).dt.days\n\n# sezonowość i kalendarz\ndf[\"signup_month\"] = df[\"signup_date\"].dt.month\ndf[\"signup_dow\"] = df[\"signup_date\"].dt.dayofweek  # 0=pon, 6=niedz\n\n\nTakie cechy są zwykle dużo bardziej informacyjne dla klasyfikacji churn niż same daty, ponieważ mają bezpośredni związek z aktywnością klienta.\n\n\n3.4.3 Transformacje rozkładów\nW danych ekonomicznych i behawioralnych (np. income_eur, avg_basket_eur) rozkłady są często prawoskośne. Dla modeli liniowych oraz metod odległościowych zyskujemy, gdy przekształcimy skalę tak, aby relacje były bardziej „liniowe” i mniej zdominowane przez ogon rozkładu.\nStandardowa transformacja logarytmiczna (z zabezpieczeniem na zera) to:\n\\[\nx' = \\log(1 + x).\n\\]\n\n\nKod\nimport numpy as np\n\ndf[\"log_income\"] = np.log1p(df[\"income_eur\"])\ndf[\"log_avg_basket\"] = np.log1p(df[\"avg_basket_eur\"])\ndf[\"log_visits\"] = np.log1p(df[\"visits_last30\"])\n\n\nW praktyce często lepsze jest utrzymywanie zarówno cechy pierwotnej, jak i przekształconej – modele drzewiaste zwykle skorzystają z obu, a modele liniowe częściej z wersji log.\n\n\n3.4.4 Cechy intensywności i „normalizacja przez czas” (rate features)\nTypowym zabiegiem jest tworzenie cech typu „na jednostkę czasu”. Jeśli klient jest w systemie krótko, jego liczby bezwzględne mogą być nieporównywalne do klienta z długim stażem. Stąd tworzy się wskaźniki intensywności:\n\n\nKod\n# zabezpieczenie przed dzieleniem przez 0\ndf[\"tenure_days_safe\"] = df[\"tenure_days\"].clip(lower=1)\n\ndf[\"visits_per_day\"] = df[\"visits_last30\"] / 30.0\ndf[\"returns_rate_90\"] = df[\"returns_last90\"] / 90.0\ndf[\"visits_per_tenure\"] = df[\"visits_last30\"] / df[\"tenure_days_safe\"]\n\n\nTe cechy są szczególnie wartościowe w klasycznych modelach liniowych, bo stabilizują skalę i często lepiej korelują z decyzją niż wartości surowe.\n\n\n3.4.5 Interakcje i cechy „biznesowe” z sensowną interpretacją\nInterakcje to bardzo ważna klasa cech w klasycznych modelach. Jeżeli nie używasz modeli, które „same” łatwo uczą się złożonych interakcji (np. boosting), to możesz jawnie dodać kilka logicznych interakcji. Przykładowo: wartość koszyka może działać inaczej dla segmentu VIP, a liczba wizyt może inaczej wpływać na churn, gdy satysfakcja jest niska.\n\n\nKod\n# przykładowe interakcje liczbowe\ndf[\"basket_x_visits\"] = df[\"avg_basket_eur\"] * df[\"visits_last30\"]\ndf[\"income_x_visits\"] = df[\"income_eur\"] * df[\"visits_last30\"]\n\n\nMożemy również budować interakcje „warunkowe”, np. flaga „niska satysfakcja” i interakcja z wizytami:\n\n\nKod\ndf[\"low_satisfaction\"] = (df[\"satisfaction_1_5\"] &lt;= 2).astype(int)\ndf[\"visits_if_low_satisfaction\"] = df[\"visits_last30\"] * df[\"low_satisfaction\"]\n\n\nTo jest szczególnie dydaktyczne, bo pokazuje, jak złożone hipotezy można przenieść do prostego modelu.\n\n\n3.4.6 Grupowanie zmiennych ciągłych do przedziałów (binning)\nCzasem warto przekształcić zmienną ciągłą do kategorii, zwłaszcza gdy zależność jest progowa. Przykładowo: recency_days może mieć silny efekt dopiero po przekroczeniu pewnego progu. Binning bywa użyteczny też w analizie dyskryminacyjnej i prostych modelach interpretowalnych.\n\n\nKod\ndf[\"recency_bin\"] = pd.cut(\n    df[\"recency_days\"],\n    bins=[-1, 7, 30, 90, 180, 365, 10_000],\n    labels=[\"&lt;=7\", \"8-30\", \"31-90\", \"91-180\", \"181-365\", \"&gt;365\"]\n)\n\n\n\n\n3.4.7 Kodowanie kategorii pod modele\nNajbardziej standardowym kodowaniem jest one-hot encoding. Możemy je zrobić w pandas, ale w modelowaniu lepszy jest pipeline w sklearn. Dla celów EDA i demonstracji:\n\n\nKod\nX_cat = pd.get_dummies(df[[\"segment\", \"device_type\", \"city_reduced\"]], dummy_na=True)\nX_cat.head()\n\n\n\n\n\n\n\n\n\nsegment_b2b\nsegment_b2c\nsegment_student\nsegment_vip\nsegment_&lt;NA&gt;\ndevice_type_desktop\ndevice_type_mobile\ndevice_type_tablet\ndevice_type_&lt;NA&gt;\ncity_reduced_katowice\ncity_reduced_kraków\ncity_reduced_lublin\ncity_reduced_other\ncity_reduced_poznań\ncity_reduced_wrocław\ncity_reduced_łódź\ncity_reduced_&lt;NA&gt;\n\n\n\n\n0\nTrue\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\n\n\n1\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n2\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\n\n\n3\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n4\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\n\n\n\n\n\n\n\n\n\n3.4.8 Składanie cech do macierzy modelowej\nPo inżynierii cech zwykle wybieramy zestaw kolumn do modelu. Na potrzeby naszych klasycznych algorytmów rozsądne jest przygotowanie „bazowego” zestawu cech liczbowych oraz zakodowanych kategorii.\n\n\nKod\nfeature_cols_num = [\n    \"tenure_days\", \"recency_days\",\n    \"income_eur\", \"avg_basket_eur\", \"visits_last30\", \"returns_last90\",\n    \"log_income\", \"log_avg_basket\", \"log_visits\",\n    \"visits_per_day\", \"returns_rate_90\",\n    \"low_satisfaction\", \"basket_x_visits\"\n]\n\nfeature_cols_cat = [\"segment\", \"device_type\", \"city_reduced\", \"recency_bin\"]\n\nX_num = df[feature_cols_num]\nX_cat = pd.get_dummies(df[feature_cols_cat], dummy_na=True)\n\nX = pd.concat([X_num, X_cat], axis=1)\ny = df[\"churned\"]\n\n\n\n\n\n\n\n\nAdnotacja\n\n\n\nPrzedstawione powyżej przykłady inżynierii cech mają charakter ilustracyjny i służą pokazaniu typowych mechanizmów oraz kierunków pracy z danymi. W kontekście konkretnego problemu analitycznego część tych kroków może zostać zmodyfikowana, uproszczona lub całkowicie pominięta, a inne – niewystępujące w przykładach – mogą okazać się kluczowe. Inżynieria cech nie jest zestawem sztywnych reguł, lecz procesem zależnym od charakteru danych, celu analizy oraz stosowanego modelu.\nW praktyce najlepszym podejściem jest realizowanie inżynierii cech w sposób systematyczny i powtarzalny, z wykorzystaniem pipeline’ów. Pozwala to połączyć imputację, skalowanie, kodowanie oraz konstrukcję cech w jeden spójny proces, który jest dopasowywany wyłącznie na zbiorze treningowym i następnie stosowany do walidacji oraz testu. Takie podejście minimalizuje ryzyko wycieku informacji, ułatwia porównywanie modeli oraz sprawia, że eksperymenty analityczne są w pełni reprodukowalne.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Przygotowanie i czyszczenie danych</span>"
    ]
  },
  {
    "objectID": "chapters/03-klasyfikacja-liniowa.html",
    "href": "chapters/03-klasyfikacja-liniowa.html",
    "title": "4  Klasyfikacja liniowa",
    "section": "",
    "text": "4.1 Modele liniowe w uczeniu nadzorowanym\nModele liniowe stanowią fundament klasycznego uczenia maszynowego dla danych tablicowych. Ich atrakcyjność wynika z połączenia trzech cech: prostoty konstrukcji, relatywnie łatwej interpretacji oraz stabilnych własności matematycznych. W ujęciu uczenia nadzorowanego model liniowy opisuje zależność pomiędzy wektorem cech \\(x \\in \\mathbb{R}^p\\) a zmienną docelową \\(y\\), ucząc się parametrów na podstawie par \\((x_i, y_i)\\). W praktyce modele liniowe służą zarówno do regresji (gdy \\(y\\) jest zmienną ciągłą), jak i do klasyfikacji (gdy \\(y\\) jest zmienną dyskretną). W tym kursie modele liniowe są szczególnie ważne, ponieważ stanowią punkt odniesienia dla późniejszych metod (drzew, zespołów, SVM), a także umożliwiają wprowadzenie pojęć takich jak funkcja straty, estymacja parametrów, kompromis bias–variance oraz interpretacja współczynników.\nW najprostszym ujęciu model liniowy zakłada, że predykcja jest liniową kombinacją cech. Dla obserwacji \\(i\\) zapisujemy:\n\\[\n\\eta_i = \\beta_0 + \\beta_1 x_{i1} + \\cdots + \\beta_p x_{ip}\n= \\beta_0 + x_i^\\top \\beta,\n\\]\ngdzie \\(\\beta_0\\) to wyraz wolny, a \\(\\beta \\in \\mathbb{R}^p\\) to wektor parametrów. Różne modele liniowe różnią się tym, jak \\(\\eta_i\\) jest powiązane z \\(y_i\\) (funkcja łącząca) oraz jak definiowana jest funkcja straty, którą minimalizujemy w procesie uczenia.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Klasyfikacja liniowa</span>"
    ]
  },
  {
    "objectID": "chapters/03-klasyfikacja-liniowa.html#regresja-wieloraka",
    "href": "chapters/03-klasyfikacja-liniowa.html#regresja-wieloraka",
    "title": "4  Klasyfikacja liniowa",
    "section": "4.2 Regresja wieloraka",
    "text": "4.2 Regresja wieloraka\n\n4.2.1 Definicja modelu\nRegresja wieloraka (regresja liniowa) jest modelem nadzorowanym, w którym zmienna docelowa \\(y\\) jest ciągła. Zakładamy:\n\\[\ny_i = \\beta_0 + x_i^\\top \\beta + \\varepsilon_i,\n\\]\ngdzie \\(\\varepsilon_i\\) to składnik losowy (szum). W ML nie musimy eksponować interpretacji probabilistycznej, natomiast kluczowe jest to, że parametry \\(\\beta_0, \\beta\\) uczymy tak, aby predykcje \\(\\hat{y}_i\\) były możliwie bliskie \\(y_i\\).\n\n\n4.2.2 Estymacja jako problem uczenia nadzorowanego\nNajczęściej stosuje się minimalizację sumy kwadratów błędów (least squares):\n\\[\n\\min_{\\beta_0, \\beta} \\sum_{i=1}^n \\left(y_i - (\\beta_0 + x_i^\\top \\beta)\\right)^2.\n\\]\nJest to klasyczny przykład uczenia nadzorowanego: posiadamy etykiety \\(y_i\\), a algorytm uczy parametry minimalizujące zadaną funkcję straty.\n\nPrzykład 4.1 Poniżej jest przykład regresji na rzeczywistych danych z Kaggle (dataset, nie konkurs): Boston House Prices. Plik danych to boston.csv, a kolumna zmiennej wyjściowej to MEDV (wartość domu).\nUwaga: Kaggle zwykle wymaga konta i tokenu API (~/.kaggle/kaggle.json).\nJeśli używasz Kaggle CLI, pobierz i rozpakuj dane:\n#| eval: false\nkaggle datasets download -d fedesoriano/the-boston-houseprice-data -p chapters/kaggle_boston\n\nls -la chapters/kaggle_boston\nunzip -o chapters/kaggle_boston/*.zip -d chapters/kaggle_boston || true\nNastępnie przejdziemy do budowy modelu\n\n\nKod\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom pathlib import Path\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\n\ndata_path = Path(\"kaggle_boston/boston.csv\")\n\ndf = pd.read_csv(data_path)\n\n# Target jest znany z opisu danych\ntarget = \"MEDV\"\nif target not in df.columns:\n    raise ValueError(\n        f\"Brakuje kolumny `{target}` w danych. Dostępne kolumny: \" + \", \".join(df.columns)\n    )\n\n# Bierzemy wszystkie cechy liczbowe\ndf_num = df.select_dtypes(include=[\"number\"]).copy()\n\ntmp = df_num.dropna(subset=[target]).copy()\nX = tmp.drop(columns=[target]).dropna()\ny = tmp.loc[X.index, target]\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\nreg = LinearRegression()\nreg.fit(X_train, y_train)\n\n# predykcje\ny_pred_train = reg.predict(X_train)\ny_pred_test = reg.predict(X_test)\n\n# R^2\nr2_train = reg.score(X_train, y_train)\nr2_test = reg.score(X_test, y_test)\n\n# „klasyczne” podsumowanie regresji (liczone na zbiorze treningowym)\nn = X_train.shape[0]\np = X_train.shape[1]\ndof = n - p - 1\n\nresid = y_train.values - y_pred_train\nsse = np.sum(resid**2)  # sum of squared errors\ntss = np.sum((y_train.values - y_train.mean())**2)\nssr = tss - sse         # sum of squares regression\n\nsigma = np.sqrt(sse / dof)          # residual std. error (RSE)\nF = (ssr / p) / (sse / dof)         # F-statistic dla H0: beta_1=...=beta_p=0\n\nsummary = pd.DataFrame({\n    \"metric\": [\"R2_train\", \"R2_test\", \"sigma(RSE)_train\", \"F_train\", \"n_train\", \"p\", \"dof\"],\n    \"value\":  [r2_train, r2_test, sigma, F, n, p, dof],\n})\nprint(summary.to_string(index=False))\n\n# wykres: y_true vs y_pred (zbiór testowy)\nplt.figure(figsize=(6, 6))\nplt.scatter(y_test, y_pred_test, alpha=0.7)\nmn = min(y_test.min(), y_pred_test.min())\nmx = max(y_test.max(), y_pred_test.max())\nplt.plot([mn, mx], [mn, mx], linestyle=\"--\")\nplt.xlabel(f\"y_true ({target})\")\nplt.ylabel(\"y_pred\")\nplt.title(\"Boston Housing (Kaggle): y_pred vs y_true (test)\")\nplt.tight_layout()\nplt.show()\n\n\n          metric      value\n        R2_train   0.750886\n         R2_test   0.668759\nsigma(RSE)_train   4.734795\n         F_train  90.426617\n         n_train 404.000000\n               p  13.000000\n             dof 390.000000\n\n\n\n\n\n\n\n\n\n\nR2_train = 0.751, R2_test = 0.669 - model liniowy wyjaśnia ok. 75% wariancji zmiennej docelowej na treningu i ok. 67% na teście. Spadek jest znaczny, co może sugerować przeuczenie modelu.\nsigma (RSE)_train = 4.735 - to oszacowanie odchylenia standardowego reszt (typowy błąd predykcji) na treningu. Interpretacja jednostek zależy od tego, czym jest target: jeśli target to klasyczny MEDV w tysiącach USD, to przeciętny błąd rzędu ~4.7 tys. USD na treningu. To jest „typowa” skala odchyłki predykcji od wartości rzeczywistej w modelu liniowym.\nF_train = 90.43 przy p = 13 i dof = 390 - statystyka \\(F\\) testuje hipotezę zerową: wszystkie współczynniki nachyleń są równe 0 (model nie wnosi nic ponad średnią). Tak wysoka wartość \\(F\\) oznacza, że model jako całość jest istotny statystycznie (praktycznie na pewno p-value \\(\\ll 0.001\\)) — czyli przynajmniej część cech ma realny związek z targetem.\n\nPonieważ model wykazuje delikatne znamiona przeuczenia zastosujemy do niego regularyzacje.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Klasyfikacja liniowa</span>"
    ]
  },
  {
    "objectID": "chapters/03-klasyfikacja-liniowa.html#regularyzacja-modeli-liniowych",
    "href": "chapters/03-klasyfikacja-liniowa.html#regularyzacja-modeli-liniowych",
    "title": "4  Klasyfikacja liniowa",
    "section": "4.3 Regularyzacja modeli liniowych",
    "text": "4.3 Regularyzacja modeli liniowych\n\n4.3.1 Cel stosowania\nW praktyce modele liniowe mogą cierpieć na przeuczenie, niestabilność estymacji (zwłaszcza przy silnej współliniowości cech) oraz zbyt dużą wariancję współczynników. Regularyzacja dodaje do funkcji straty karę za zbyt duże wartości parametrów, co stabilizuje estymację i poprawia uogólnianie.\nOgólny zapis problemu z regularyzacją:\n\\[\n\\min_{\\beta_0, \\beta} \\mathcal{L}(y, X; \\beta_0, \\beta) + \\lambda \\, \\mathcal{P}(\\beta),\n\\]\ngdzie \\(\\mathcal{L}\\) to funkcja straty (np. MSE w regresji lub log-loss w regresji logistycznej), \\(\\mathcal{P}(\\beta)\\) to kara, a \\(\\lambda \\ge 0\\) kontroluje siłę regularyzacji. Zwykle nie karzemy wyrazu wolnego \\(\\beta_0\\).\n\n\n4.3.2 Ridge regression (kara \\(L_2\\))\nRidge regression (Tikhonov) stosuje karę \\(L_2\\):\n\\[\n\\min_{\\beta_0, \\beta} \\mathcal{L}(y, X; \\beta_0, \\beta) + \\lambda \\|\\beta\\|_2^2.\n\\]\nWspółczynniki są „kurczone” w stronę zera, ale zwykle nie stają się dokładnie równe zeru. Ridge jest szczególnie użyteczny przy współliniowości cech, bo stabilizuje estymacje i obniża wariancję.\n\n\n4.3.3 LASSO (kara \\(L_1\\))\nLASSO (ang. Least Absolute Shrinkage and Selection Operator) używa kary \\(L_1\\):\n\\[\n\\min_{\\beta_0, \\beta} \\mathcal{L}(y, X; \\beta_0, \\beta) + \\lambda \\|\\beta\\|_1.\n\\]\nKara L1 sprzyja rozwiązaniom rzadkim — część współczynników staje się dokładnie zerowa. Dzięki temu LASSO pełni jednocześnie rolę selekcji cech.\n\n\n4.3.4 Elastic Net (kara mieszana)\nElastic Net łączy zalety Ridge i LASSO:\n\\[\n\\min_{\\beta_0, \\beta}  \\mathcal{L}(y, X; \\beta_0, \\beta) + \\lambda \\left(\\alpha \\|\\beta\\|_1 + (1-\\alpha)\\|\\beta\\|_2^2\\right),\n\\]\ngdzie \\(\\alpha \\in [0,1]\\) steruje proporcją kary \\(L_1\\) do \\(L_2\\). Elastic Net jest szczególnie przydatny, gdy mamy wiele skorelowanych cech: LASSO wybiera pojedyncze zmienne z grupy, a Elastic Net częściej zachowuje całe grupy w postaci „współdzielonego” kurczenia.\n\n\n4.3.5 Interpretacja i praktyka\nRegularyzacja polega na tym, że do klasycznej funkcji straty dodajemy karę za „zbyt duże” współczynniki. W modelach liniowych i logistycznych najczęściej spotkasz kary typu (\\(L_2\\)) (ridge) albo (\\(L_1\\)) (lasso). W ujęciu matematycznym dla regresji liniowej jest to odpowiednio:\n\\[\n\\min_{\\beta_0,\\beta} \\sum_{i=1}^n (y_i - \\beta_0 - x_i^\\top \\beta)^2 + \\lambda |\\beta|_2^2\n\\quad\\text{(ridge)}\n\\]\nalbo\n\\[\n\\min_{\\beta_0,\\beta} \\sum_{i=1}^n (y_i - \\beta_0 - x_i^\\top \\beta)^2 + \\lambda |\\beta|_1\n\\quad\\text{(lasso)}\n\\]\nAnalogicznie w regresji logistycznej zamiast sumy kwadratów masz stratę logarytmiczną, ale idea kary jest identyczna. Parametr \\(\\lambda \\ge 0\\) steruje „siłą” regularyzacji: im większy, tym mocniej model preferuje małe współczynniki.\nW modelu bez regularyzacji współczynniki \\(\\hat\\beta\\) są dobierane tak, aby możliwie najlepiej dopasować dane uczące (minimalizować samą stratę). W przypadku danych z szumem, współliniowością cech lub dużą liczbą predyktorów, takie dopasowanie może prowadzić do tego, że współczynniki stają się „niestabilne”: niewielka zmiana danych (inna próba, inny podział train/test) daje wyraźnie inne \\(\\hat\\beta\\). Regularyzacja celowo ogranicza swobodę modelu, „ściągając” współczynniki w stronę zera. To powoduje, że estymator staje się obciążony (biased) – współczynniki nie są już czystym odzwierciedleniem relacji w danych, bo zostały sztucznie zmniejszone przez karę. Jednocześnie znacząco spada wariancja estymatora: współczynniki są bardziej stabilne, a predykcje częściej lepiej generalizują na dane testowe. To klasyczny kompromis bias–variance: akceptujemy pewną stronniczość w zamian za mniejszą wrażliwość na szum.\nW praktyce interpretacja jest taka: w modelu regularyzowanym nie traktujesz wartości \\(\\beta_j\\) jako „czystego” oszacowania wpływu cechy \\(x_j\\) (w sensie klasycznej regresji), tylko jako wynik kompromisu pomiędzy dopasowaniem i prostotą modelu. Szczególnie przy silnej regularyzacji współczynniki należy interpretować ostrożnie: „to jest kierunek i względna siła sygnału po uwzględnieniu kary”, a nie „dokładna zmiana oczekiwanej wartości \\(y\\) na jednostkę \\(x_j\\)”.\nGdy \\(\\lambda \\to 0\\), kara zanika i wracamy do klasycznego uczenia bez regularyzacji: w regresji liniowej do OLS, w logistycznej do MLE bez kary. Wtedy współczynniki są „najmniej ściągnięte”, ale mogą być niestabilne (szczególnie przy współliniowości i dużym \\(p\\)). Gdy \\(\\lambda\\) rośnie, kara zaczyna dominować i wymusza zmniejszanie współczynników. Dla ridge (\\(L_2\\)) współczynniki są płynnie „kurczone” w kierunku zera, ale zwykle nie są równe zero. Dla lasso (\\(L_1\\)) część współczynników może stać się dokładnie równa zero, co prowadzi do selekcji zmiennych. Przy bardzo dużym \\(\\lambda\\) model może w praktyce „zrezygnować” z większości sygnału i zbliżyć się do modelu prawie stałego (predykcja głównie przez \\(\\beta_0\\)).\nRegularyzacja karze współczynniki, a nie bezpośrednio cechy. Ponieważ współczynnik \\(\\beta_j\\) jest ściśle powiązany ze skalą \\(x_j\\), brak standaryzacji powoduje, że kara działa niesprawiedliwie względem zmiennych o różnych jednostkach.\nZałóżmy dwie cechy niosące podobną informację, ale w różnych skalach:\n\n\\(x_1\\) - „dochód” rzędu dziesiątek tysięcy,\n\\(x_2\\) - „udział zwrotów” w zakresie 0–1.\n\nAby obie cechy miały podobny wpływ na \\(\\eta\\), model bez regularyzacji może dopasować:\n\nmały współczynnik przy dochodzie (bo sama cecha ma duże liczby),\nduży współczynnik przy zmiennej 0–1 (bo cecha jest mała).\n\nJeżeli dodamy karę typu ridge \\(\\lambda \\sum_j \\beta_j^2\\), to duży współczynnik przy \\(x_2\\) zostanie ukarany dużo silniej niż mały współczynnik przy \\(x_1\\), mimo że obie cechy mogą być równie istotne. W efekcie model może preferować cechy o dużej skali nie dlatego, że są lepsze, tylko dlatego, że wymagają mniejszych \\(|\\beta_j|\\), a więc „taniej” przechodzą przez karę. To jest artefakt skali, a nie właściwość danych.\nStandardyzacja usuwa ten problem, sprowadzając każdą cechę do porównywalnej skali, najczęściej:\n\\[\nz_{ij} = \\frac{x_{ij}-\\mu_j}{\\sigma_j}.\n\\]\nPo standaryzacji „jednostka” każdej cechy jest porównywalna (1 odchylenie standardowe), więc kara na współczynnikach działa symetrycznie. Dlatego w praktyce dla regresji logistycznej, ridge/lasso oraz większości modeli liniowych z regularyzacją standaryzacja jest traktowana jako element obowiązkowy.\nW ujęciu uczenia maszynowego regularyzacja jest narzędziem kontrolowania złożoności modelu. \\(\\lambda\\) staje się hiperparametrem, który dobiera się na podstawie jakości generalizacji (np. przez walidację). Wraz ze wzrostem \\(\\lambda\\) model staje się prostszy i stabilniejszy, ale może gorzej dopasowywać dane treningowe. Standaryzacja jest integralną częścią tego procesu, bo zapewnia, że dobór \\(\\lambda\\) i sama kara mają sens niezależnie od jednostek i skali cech.\n\n\nKod\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom pathlib import Path\n\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.metrics import r2_score, mean_squared_error\n\n# ----------------------------\n# 1) Pipeline: standaryzacja + ElasticNet\n# ----------------------------\npipe = Pipeline(steps=[\n    (\"scaler\", StandardScaler()),\n    (\"model\", ElasticNet(max_iter=50_000, random_state=42))\n])\n\n# ----------------------------\n# 2) Kalibracja (GridSearch): lambda i alpha\n#    - lambda -&gt; model__alpha\n#    - alpha  -&gt; model__l1_ratio\n# ----------------------------\n# Sensowna siatka: logarytmiczna dla lambda\nlambda_grid = np.logspace(-4, 2, 25)      # 1e-4 ... 1e2\nalpha_grid = np.linspace(0.0, 1.0, 11)    # 0, 0.1, ..., 1.0\n\nparam_grid = {\n    \"model__alpha\": lambda_grid,          # lambda (siła kary)\n    \"model__l1_ratio\": alpha_grid         # alpha (mieszanka L1/L2)\n}\n\n# CV: w regresji klasycznie KFold; domyślnie GridSearchCV użyje KFold\n# Skoring: R^2, bo tak raportujesz w przykładzie\nsearch = GridSearchCV(\n    estimator=pipe,\n    param_grid=param_grid,\n    scoring=\"r2\",\n    cv=5,\n    n_jobs=-1\n)\n\nsearch.fit(X_train, y_train)\n\nbest_model = search.best_estimator_\nbest_params = search.best_params_\nbest_cv_r2 = search.best_score_\n\nprint(\"Najlepsze hiperparametry (CV):\")\nprint(f\"  lambda (model__alpha)   = {best_params['model__alpha']:.6g}\")\nprint(f\"  alpha  (model__l1_ratio)= {best_params['model__l1_ratio']:.3f}\")\nprint(f\"  CV R^2 (mean)           = {best_cv_r2:.4f}\")\n\n# ----------------------------\n# 3) Ocena na train/test + metryki jak w Twoim stylu\n# ----------------------------\ny_pred_train = best_model.predict(X_train)\ny_pred_test = best_model.predict(X_test)\n\nr2_train = r2_score(y_train, y_pred_train)\nr2_test = r2_score(y_test, y_pred_test)\n\n# Dodatkowo RMSE (często użyteczne w regresji)\nrmse_train = np.sqrt(mean_squared_error(y_train, y_pred_train))\nrmse_test = np.sqrt(mean_squared_error(y_test, y_pred_test))\n\nsummary = pd.DataFrame({\n    \"metric\": [\"R2_train\", \"R2_test\", \"RMSE_train\", \"RMSE_test\", \"best_lambda\", \"best_alpha(l1_ratio)\"],\n    \"value\":  [r2_train, r2_test, rmse_train, rmse_test, best_params[\"model__alpha\"], best_params[\"model__l1_ratio\"]],\n})\nprint(\"\\nPodsumowanie ElasticNet:\")\nprint(summary.to_string(index=False))\n\n# ----------------------------\n# 4) Współczynniki po regularyzacji (już po standaryzacji)\n# ----------------------------\n# Uwaga: współczynniki dotyczą cech po standaryzacji (porównywalne między sobą).\ncoefs = best_model.named_steps[\"model\"].coef_\ncoef_table = pd.DataFrame({\"feature\": X.columns, \"coef\": coefs}).sort_values(\"coef\", key=np.abs, ascending=False)\n\nprint(\"\\nNajwiększe (bezwzględnie) współczynniki ElasticNet:\")\nprint(coef_table.head(10).to_string(index=False))\n\n# ----------------------------\n# 5) Wykres: y_true vs y_pred (test)\n# ----------------------------\nplt.figure(figsize=(6, 6))\nplt.scatter(y_test, y_pred_test, alpha=0.7)\nmn = min(y_test.min(), y_pred_test.min())\nmx = max(y_test.max(), y_pred_test.max())\nplt.plot([mn, mx], [mn, mx], linestyle=\"--\")\nplt.xlabel(f\"y_true ({target})\")\nplt.ylabel(\"y_pred\")\nplt.title(\"Boston Housing (Kaggle): ElasticNet y_pred vs y_true (test)\")\nplt.tight_layout()\nplt.show()\n\n# ----------------------------\n# 6) (Opcjonalnie) mapa wyników: R^2 w funkcji (lambda, alpha)\n# ----------------------------\n# To jest przydatne dydaktycznie: pokazuje krajobraz hiperparametrów.\nresults = pd.DataFrame(search.cv_results_)\npivot = results.pivot_table(\n    index=\"param_model__l1_ratio\",\n    columns=\"param_model__alpha\",\n    values=\"mean_test_score\"\n).sort_index()\n\nplt.figure(figsize=(10, 4))\nplt.imshow(pivot.values, aspect=\"auto\")\nplt.colorbar(label=\"mean CV R^2\")\nplt.yticks(range(pivot.shape[0]), [f\"{v:.1f}\" for v in pivot.index])\nplt.xticks(range(pivot.shape[1]), [f\"{v:.0e}\" for v in pivot.columns], rotation=45, ha=\"right\")\nplt.ylabel(\"alpha (l1_ratio)\")\nplt.xlabel(\"lambda (model__alpha)\")\nplt.title(\"ElasticNet: średnie CV R^2 dla (lambda, alpha)\")\nplt.tight_layout()\nplt.show()\n\n\nNajlepsze hiperparametry (CV):\n  lambda (model__alpha)   = 0.01\n  alpha  (model__l1_ratio)= 0.000\n  CV R^2 (mean)           = 0.7245\n\nPodsumowanie ElasticNet:\n              metric    value\n            R2_train 0.750670\n             R2_test 0.667571\n          RMSE_train 4.654048\n           RMSE_test 4.937440\n         best_lambda 0.010000\nbest_alpha(l1_ratio) 0.000000\n\nNajwiększe (bezwzględnie) współczynniki ElasticNet:\nfeature      coef\n  LSTAT -3.562197\n     RM  3.167423\n    DIS -2.939338\nPTRATIO -1.999596\n    RAD  1.966321\n    NOX -1.901429\n    TAX -1.510458\n      B  1.120095\n   CRIM -0.964915\n   CHAS  0.732337\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWidać wyraźnie, że kalibaracja parametrów \\(\\lambda\\) i \\(\\alpha\\) sprowadziła praktycznie model do regresji grzbietowej (ridge) z parametrem \\(\\lambda=0.01\\). Wyniki porównania \\(R^2\\) pomiędzy zbiorem treningowym i testowym po regularyzacji się nie zmieniły. Dalej występuje różnica \\(\\approx 0.08\\). To pokazuje, że sama regularyzacja nie wystarczy do usunięcia tego przeuczenia (swoją drogą nie jest ono bardzo duże). Na koniec aby przekonać się jak modele (raw i ridge) są wrażliwe na podział zbioru dokonamy ich porównania z wykorzystaniem walidacji krzyżowej.\n\n\nKod\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import RepeatedKFold, cross_val_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression, Ridge\n\n# ----------------------------\n# 1) RepeatedKFold (powtarzana walidacja krzyżowa)\n# ----------------------------\ncv = RepeatedKFold(n_splits=5, n_repeats=20, random_state=42)\n\n# ----------------------------\n# 2) Modele do porównania\n#    - OLS: LinearRegression (bez regularyzacji)\n#    - Ridge: regularyzacja L2; skalowanie w pipeline (ważne)\n# ----------------------------\nols = Pipeline(steps=[\n    (\"scaler\", StandardScaler()),\n    (\"model\", LinearRegression())\n])\n\nridge = Pipeline(steps=[\n    (\"scaler\", StandardScaler()),\n    (\"model\", Ridge(alpha=0.01, random_state=42))\n])\n\n# ----------------------------\n# 3) R^2 w RepeatedKFold\n# ----------------------------\nscores_ols = cross_val_score(ols, X, y, cv=cv, scoring=\"r2\", n_jobs=-1)\nscores_ridge = cross_val_score(ridge, X, y, cv=cv, scoring=\"r2\", n_jobs=-1)\n\n# Podsumowanie liczbowe: średnia, odchylenie, kwartyle\nsummary = pd.DataFrame({\n    \"model\": [\"LinearRegression (OLS)\", \"Ridge (alpha=0.01)\"],\n    \"mean_R2\": [scores_ols.mean(), scores_ridge.mean()],\n    \"std_R2\": [scores_ols.std(ddof=1), scores_ridge.std(ddof=1)],\n    \"q25\": [np.quantile(scores_ols, 0.25), np.quantile(scores_ridge, 0.25)],\n    \"median\": [np.quantile(scores_ols, 0.50), np.quantile(scores_ridge, 0.50)],\n    \"q75\": [np.quantile(scores_ols, 0.75), np.quantile(scores_ridge, 0.75)],\n})\n\nprint(summary.to_string(index=False))\n\n# ----------------------------\n# 4) Boxplot wyników\n# ----------------------------\nplt.figure(figsize=(7, 4))\nplt.boxplot([scores_ols, scores_ridge], labels=[\"OLS\", \"Ridge\"], showfliers=True)\nplt.ylabel(\"R^2 (CV)\")\nplt.title(\"RepeatedKFold: rozkład R^2 dla OLS vs Ridge\")\nplt.tight_layout()\nplt.show()\n\n# ----------------------------\n# 5) „Stabilizacja” wprost: porównanie wariancji wyników\n# ----------------------------\nprint(\"\\nStabilizacja (mniejsza zmienność wyników CV jest korzystna):\")\nprint(f\"Std(R^2) OLS  : {scores_ols.std(ddof=1):.4f}\")\nprint(f\"Std(R^2) Ridge: {scores_ridge.std(ddof=1):.4f}\")\n\n\n                 model  mean_R2   std_R2      q25   median      q75\nLinearRegression (OLS) 0.712943 0.059862 0.671914 0.719919 0.762099\n    Ridge (alpha=0.01) 0.712945 0.059863 0.671911 0.719929 0.762106\n\n\n\n\n\n\n\n\n\n\nStabilizacja (mniejsza zmienność wyników CV jest korzystna):\nStd(R^2) OLS  : 0.0599\nStd(R^2) Ridge: 0.0599\n\n\nWyniki są niemal identyczne, co pokazuje, że regularyzacja modelu nie pomaga wyeliminować delikatnego przeuczenia.\nPierwszy powód to naturalna różnica między błędem treningowym a błędem generalizacji. Model jest dopasowywany tak, aby minimalizować stratę na treningu, więc na treningu prawie zawsze będzie lepiej niż na danych niewidzianych. Nawet gdy model nie jest „przeuczony” w sensie patologicznym, pojawi się luka generalizacyjna, bo test jest inną próbą z tej samej populacji i zawiera inny układ szumu losowego.\nDrugi powód to ograniczona zgodność modelu z rzeczywistą zależnością. Regresja liniowa zakłada liniowość i addytywność wpływów cech (bez nieliniowości i bez interakcji, jeśli ich nie dodasz). Jeśli prawdziwa relacja jest częściowo nieliniowa (co w danych nieruchomości jest częste), to model „radzi sobie” na treningu, ale na teście spada, bo dopasowanie do przypadkowego układu obserwacji w treningu nie przenosi się idealnie na inną próbę. To jest bardziej kwestia bias/misspecification niż „overfittingu z powodu zbyt dużej złożoności”, ale objaw w metrykach jest podobny.\nTrzeci powód to wariancja pojedynczego podziału train/test. Ta różnica (około 0.08 w \\(R^2\\)) może w dużej mierze wynikać z tego, że akurat wylosowany test jest „trudniejszy” (np. zawiera więcej obserwacji z krańców rozkładu). Właśnie dlatego RepeatedKFold jest lepszym narzędziem diagnostycznym: u nas średnie CV (\\(R^2 \\approx 0.713\\)) wskazuje, że wynik testowy 0.668 nie jest już tak odległy, tylko może być po prostu mniej korzystnym splitem.\nCzwarty potencjalny powód to współliniowość i niestabilność współczynników, która nie zawsze przekłada się na wyraźną zmianę \\(R^2\\), ale może zwiększać wrażliwość na konkretny podział danych. Ridge z bardzo małym \\(\\lambda\\) nie zmienia u nas jakości ani wariancji metryki, więc to sugeruje, że w tym konkretnym ustawieniu współliniowość nie jest dominującym źródłem luki — ale nadal może wpływać na interpretację \\(\\beta\\) i na zachowanie w innych splitach.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Klasyfikacja liniowa</span>"
    ]
  },
  {
    "objectID": "chapters/03-klasyfikacja-liniowa.html#regresja-logistyczna",
    "href": "chapters/03-klasyfikacja-liniowa.html#regresja-logistyczna",
    "title": "4  Klasyfikacja liniowa",
    "section": "4.4 Regresja logistyczna",
    "text": "4.4 Regresja logistyczna\n\n4.4.1 Definicja modelu\nRegresja logistyczna jest modelem liniowym przeznaczonym do klasyfikacji binarnej. Zakładamy, że zmienna docelowa \\(y \\in {0,1}\\), a model opisuje prawdopodobieństwo klasy wyróżnionej (1). Najpierw definiujemy predyktor liniowy:\n\\[\n\\eta_i = \\beta_0 + x_i^\\top \\beta,\n\\]\na następnie mapujemy go do \\([0,1]\\) funkcją logistyczną:\n\\[\np_i = \\mathbb{P}(y_i = 1 \\mid x_i) = \\sigma(\\eta_i) = \\frac{1}{1 + e^{-\\eta_i}}.\n\\]\nDecyzję klasyfikacyjną podejmujemy przez ustawienie progu \\(t\\) (zwykle 0.5):\n\\[\n\\hat{y}_i​=\n\\begin{cases}\n1,\\; p_i\\geq t,\\\\\n0,\\; p_i&lt;t.\n\\end{cases}\n\\]\nPonieważ \\(y_i \\in {0,1}\\), naturalnym modelem probabilistycznym dla \\(y_i\\) przy danym \\(x_i\\) jest rozkład Bernoulliego:\n\\[\ny_i \\mid x_i \\sim \\text{Bernoulli}(p_i),\n\\quad\\text{czyli}\\quad\n\\mathbb{P}(y_i\\mid x_i) = p_i^{y_i}(1-p_i)^{1-y_i}.\n\\]\nTo prowadzi do funkcji wiarygodności dla całej próby (przy założeniu niezależności obserwacji):\n\\[\nL(\\beta_0,\\beta) = \\prod_{i=1}^n p_i^{y_i}(1-p_i)^{1-y_i}.\n\\]\nW ujęciu statystycznym „uczenie” parametrów polega na maksymalizacji wiarygodności \\(L\\). Ponieważ iloczyny są niewygodne obliczeniowo, przechodzi się na logartym wiarygodności:\n\\[\n\\ell(\\beta_0,\\beta) = \\log L(\\beta_0,\\beta)\n= \\sum_{i=1}^n \\left[y_i\\log(p_i) + (1-y_i)\\log(1-p_i)\\right].\n\\]\nMaksymalizacja \\(\\ell\\) jest równoważna minimalizacji jej negacji, co w ML nazywamy stratą logarytmiczną (log loss) albo entropią krzyżową (binary cross-entropy):\n\\[\n\\min_{\\beta_0,\\beta} -\\sum_{i=1}^n \\left[y_i \\log(p_i) + (1-y_i)\\log(1-p_i)\\right].\n\\]\nIntuicyjnie ta strata „nagradza” model, gdy przypisuje wysokie prawdopodobieństwo klasie, która rzeczywiście wystąpiła, i „karze” go mocno, gdy model jest bardzo pewny, ale myli się. Na przykład, jeśli \\(y_i=1\\) i model daje \\(p_i=0.99\\), składnik straty jest mały; jeśli natomiast \\(y_i=1\\) i model daje \\(p_i=0.01\\), to \\(-\\log(0.01)\\) jest duże, więc kara jest silna. Dzięki temu log loss nie jest tylko miarą „trafione/nie trafione”, lecz ocenia jakość probabilistyczną predykcji, co jest szczególnie ważne w klasyfikacji.\nW praktyce nie istnieje prosty wzór zamknięty na \\((\\beta_0,\\beta)\\) analogiczny do regresji liniowej OLS. Dlatego optymalizację wykonuje się numerycznie. Najczęściej stosuje się metody oparte o gradient (i często pochodne drugiego rzędu). Kluczowym faktem jest, że funkcja straty w regresji logistycznej jest wypukła względem \\((\\beta_0,\\beta)\\), więc metody gradientowe mają dobre własności: przy poprawnej implementacji dążą do globalnego minimum. Dla kompletności warto zapisać postać gradientu względem \\(\\beta\\) (pomijając wyraz wolny dla czytelności). Jeśli \\(X\\) to macierz cech, a \\(p\\) wektor \\(p_i\\), to gradient ma postać:\n\\[\n\\nabla_{\\beta}  \\Big(-\\ell(\\beta)\\Big) = X^\\top (p - y),\n\\]\nczyli różnica między prognozowanymi prawdopodobieństwami a rzeczywistymi etykietami, „zebrana” przez cechy. To dobrze podkreśla charakter uczenia: gdy model przeszacowuje prawdopodobieństwo klasy 1 (duże \\(p_i\\) przy \\(y_i=0\\)), gradient pcha parametry w kierunku zmniejszenia \\(\\eta_i\\) dla tych obserwacji, i odwrotnie.\nWreszcie, jest to model nadzorowany w sensie ML, ponieważ parametry \\((\\beta_0,\\beta)\\) uczymy na oznakowanych parach \\((x_i,y_i)\\) poprzez minimalizację funkcji straty. W odróżnieniu od metod nienadzorowanych, tutaj etykieta \\(y_i\\) jest bezpośrednio składnikiem funkcji celu, a „uczenie” polega na takim doborze parametrów, aby model możliwie dobrze odtwarzał zależność między cechami a klasą docelową, nie tylko na danych uczących, ale przede wszystkim na danych niewidzianych.\n\nPrzykład 4.2  \n\n\nKod\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom datasets import load_dataset\n\nfrom sklearn.model_selection import StratifiedKFold, cross_val_predict\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.metrics import (\n    confusion_matrix,\n    accuracy_score, precision_score, recall_score, f1_score,\n    roc_auc_score, roc_curve\n)\n\n# =========================\n# 1) Wczytanie danych z Hugging Face\n# =========================\n# Dataset ma jeden split \"train\" (683 obserwacje) i target \"is_cancer\"\nds = load_dataset(\"mstz/breast\", \"cancer\")[\"train\"]  # :contentReference[oaicite:1]{index=1}\ndf = ds.to_pandas()\n\ntarget = \"is_cancer\"\ny = df[target].astype(int)\n\nX = df.drop(columns=[target])\n\n# Minimalnie: imputacja (gdyby były braki) + standaryzacja + logreg\npipe = Pipeline(steps=[\n    (\"imputer\", SimpleImputer(strategy=\"median\")),\n    (\"scaler\", StandardScaler()),\n    (\"model\", LogisticRegression(max_iter=2000, solver=\"lbfgs\"))\n])\n\n# =========================\n# 2) Walidacja krzyżowa i out-of-fold predykcje prawdopodobieństw\n# =========================\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\n# out-of-fold: każda obserwacja ma proba z modelu, który jej \"nie widział\" w treningu\nproba_oof = cross_val_predict(\n    pipe, X, y,\n    cv=cv,\n    method=\"predict_proba\"\n)[:, 1]\n\n# Predykcja klasy przy standardowym progu 0.5\nthr_default = 0.50\ny_pred_default = (proba_oof &gt;= thr_default).astype(int)\n\n# =========================\n# 3) Metryki + confusion matrix (PRZED kalibracją progu)\n# =========================\ndef compute_metrics(y_true, y_pred, proba):\n    return {\n        \"accuracy\":  accuracy_score(y_true, y_pred),\n        \"precision\": precision_score(y_true, y_pred, zero_division=0),\n        \"recall\":    recall_score(y_true, y_pred, zero_division=0),\n        \"f1\":        f1_score(y_true, y_pred, zero_division=0),\n        \"roc_auc\":   roc_auc_score(y_true, proba),\n    }\n\nmetrics_default = compute_metrics(y, y_pred_default, proba_oof)\ncm_default = confusion_matrix(y, y_pred_default)\n\nprint(\"PRZED kalibracją progu (threshold=0.50) – metryki OOF:\")\nfor k, v in metrics_default.items():\n    print(f\"  {k:9s}: {v:.4f}\")\nprint(\"\\nMacierz pomyłek (rows=true, cols=pred):\\n\", cm_default)\n\n# =========================\n# 4) ROC curve (z OOF proba)\n# =========================\nfpr, tpr, thresholds = roc_curve(y, proba_oof)\nauc = roc_auc_score(y, proba_oof)\n\nplt.figure(figsize=(6, 5))\nplt.plot(fpr, tpr, label=f\"ROC (AUC={auc:.3f})\")\nplt.plot([0, 1], [0, 1], linestyle=\"--\")\nplt.xlabel(\"False Positive Rate (1 - specificity)\")\nplt.ylabel(\"True Positive Rate (recall / sensitivity)\")\nplt.title(\"ROC curve – Logistic Regression (OOF)\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n# =========================\n# 5) Kalibracja progu (Youden’s J)\n#    J = TPR - FPR = sensitivity + specificity - 1\n# =========================\nJ = tpr - fpr\nidx = np.argmax(J)\nthr_calibrated = thresholds[idx]\n\nprint(f\"\\nSkalibrowany próg (Youden J): {thr_calibrated:.4f}\")\n\ny_pred_cal = (proba_oof &gt;= thr_calibrated).astype(int)\n\nmetrics_cal = compute_metrics(y, y_pred_cal, proba_oof)\ncm_cal = confusion_matrix(y, y_pred_cal)\n\nprint(\"\\nPO kalibracji progu (Youden J) – metryki OOF:\")\nfor k, v in metrics_cal.items():\n    print(f\"  {k:9s}: {v:.4f}\")\nprint(\"\\nMacierz pomyłek (rows=true, cols=pred):\\n\", cm_cal)\n\n# =========================\n# 6) Porównanie confusion matrices: wizualizacja\n# =========================\ndef plot_cm(cm, title):\n    plt.figure(figsize=(4.5, 4))\n    plt.imshow(cm, aspect=\"auto\")\n    plt.title(title)\n    plt.colorbar()\n    plt.xticks([0, 1], [\"pred 0\", \"pred 1\"])\n    plt.yticks([0, 1], [\"true 0\", \"true 1\"])\n    for i in range(2):\n        for j in range(2):\n            plt.text(j, i, cm[i, j], ha=\"center\", va=\"center\")\n    plt.tight_layout()\n    plt.show()\n\nplot_cm(cm_default, \"Confusion matrix – threshold=0.50 (OOF)\")\nplot_cm(cm_cal, f\"Confusion matrix – threshold={thr_calibrated:.3f} (OOF)\")\n\n\nPRZED kalibracją progu (threshold=0.50) – metryki OOF:\n  accuracy : 0.9693\n  precision: 0.9580\n  recall   : 0.9540\n  f1       : 0.9560\n  roc_auc  : 0.9951\n\nMacierz pomyłek (rows=true, cols=pred):\n [[434  10]\n [ 11 228]]\n\n\n\n\n\n\n\n\n\n\nSkalibrowany próg (Youden J): 0.1299\n\nPO kalibracji progu (Youden J) – metryki OOF:\n  accuracy : 0.9751\n  precision: 0.9370\n  recall   : 0.9958\n  f1       : 0.9655\n  roc_auc  : 0.9951\n\nMacierz pomyłek (rows=true, cols=pred):\n [[428  16]\n [  1 238]]",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Klasyfikacja liniowa</span>"
    ]
  },
  {
    "objectID": "chapters/03-klasyfikacja-liniowa.html#modele-dyskryminacyjne",
    "href": "chapters/03-klasyfikacja-liniowa.html#modele-dyskryminacyjne",
    "title": "4  Klasyfikacja liniowa",
    "section": "4.5 Modele dyskryminacyjne",
    "text": "4.5 Modele dyskryminacyjne\nAnaliza dyskryminacyjna to klasa klasycznych metod klasyfikacji, które bardzo naturalnie wpisują się w logikę uczenia nadzorowanego: mając etykiety klas \\(y \\in \\{1,\\dots,K\\}\\), uczymy parametry rozkładów cech w każdej klasie (część „generatywna”), a następnie stosujemy regułę Bayesa do przypisania nowej obserwacji do najbardziej prawdopodobnej klasy. W odróżnieniu od regresji logistycznej, która modeluje bezpośrednio \\(\\mathbb{P}(y\\mid x)\\), LDA/QDA modelują \\(\\mathbb{P}(x\\mid y)\\) oraz priory \\(\\mathbb{P}(y)\\), po czym wyznaczają \\(\\mathbb{P}(y\\mid x)\\) pośrednio.\nHistorycznie do analizy dyskryminacyjnej dochodzono co najmniej dwiema drogami. Pierwsza (zwykle kojarzona z Fisherem) wynikała z problemu znalezienia kierunku projekcji, na którym klasy są najlepiej rozdzielone w sensie stosunku wariancji „między klasami” do wariancji „wewnątrz klas” (Fisher 1936). Druga droga (WardJR i Hook 1963)ma charakter „decyzyjno-probabilistyczny”: zaczynamy od założeń o rozkładach klas (najczęściej normalnych) i z reguły Bayesa wyprowadzamy regułę klasyfikacji w postaci porównania pewnych funkcji punktacji dla klas. Ten drugi sposób jest bardzo bliski temu, jak dziś prezentuje się LDA/QDA w uczeniu maszynowym (jako klasyfikatory generatywne) 1.\n1 Fisher (1936) pokazał ideę rozdzielania klas przez znalezienie projekcji maksymalizującej separację (stosunek wariancji między klasami do wariancji wewnątrz klas) i zastosował ją do danych irysów. Ward (1963) jest często przywoływany w kontekście rozwoju metod grupowania i analiz wielowymiarowych; w praktyce dydaktycznej warto podkreślić, że równolegle do „drogi Fishera” (kryterium separacji/projekcji) rozwijała się droga „proceduralna” w analizie wielowymiarowej: najpierw grupowanie/definicja grup, potem konstrukcja funkcji dyskryminacyjnych do klasyfikacji i rozumienia różnic między grupami.\n4.5.1 Założenia modelu: normalność wielowymiarowa i rozkłady apriori klas\nW klasycznym wariancie zakładamy, że dla każdej klasy \\(k\\) wektor cech ma rozkład normalny:\n\\[\nx \\mid (y=k) \\sim \\mathcal{N}(\\mu_k, \\Sigma_k),\n\\qquad \\pi_k = \\mathbb{P}(y=k).\n\\]\nParametry \\(\\mu_k\\), \\(\\Sigma_k\\) oraz \\(\\pi_k\\) są nieznane i są uczone na danych treningowych z wykorzystaniem etykiet klas (czyli w pełni nadzorowanie). W praktyce: \\(\\pi_k\\) estymujemy jako częstość klasy w treningu, \\(\\mu_k\\) jako średnią wektora cech w klasie, a \\(\\Sigma\\) lub \\(\\Sigma_k\\) jako macierze kowariancji (wspólne lub klasowe, zależnie od wariantu).\n\n\n4.5.2 Funkcje dyskryminacyjne: po co są i co robią?\nReguła Bayesa mówi, że przy równych kosztach błędu optymalnie klasyfikujemy do klasy o największym prawdopodobieństwie a posteriori:\n\\[\n\\hat{y}(x) = \\arg\\max_k \\mathbb{P}(y=k\\mid x).\n\\]\nPonieważ\n\\[\n\\mathbb{P}(y=k\\mid x) \\propto \\pi_k \\, f_k(x),\n\\]\ngdzie \\(f_k(x)\\) to gęstość \\(\\mathcal{N}(\\mu_k,\\Sigma_k)\\), wygodniej porównywać logarytmy (rosną monotonicznie), definiując funkcję dyskryminacyjną:\n\\[\n\\delta_k(x) \\;=\\; \\log \\pi_k + \\log f_k(x) \\;+\\; \\text{(stała niezależna od }k\\text{)}.\n\\]\nFunkcja dyskryminacyjna jest więc „punktacją” klasy: im większa \\(\\delta_k(x)\\), tym bardziej model preferuje klasę \\(k\\) dla obserwacji \\(x\\). Klasyfikacja sprowadza się do:\n\\[\n\\hat{y}(x) = \\arg\\max_k \\delta_k(x).\n\\]\n\n\n4.5.3 LDA vs QDA\nQDA - zakładamy osobną macierz kowariancji dla każdej klasy: \\(\\Sigma_k\\) jest w pełni dowolna (symetryczna dodatnio określona) i estymowana osobno. Wtedy\n\\[\n\\delta_k(x)\n=\n-\\frac{1}{2}\\log|\\Sigma_k|\n-\\frac{1}{2}(x-\\mu_k)^\\top \\Sigma_k^{-1}(x-\\mu_k)\n+\\log\\pi_k.\n\\]\nPonieważ składnik \\((x-\\mu_k)^\\top \\Sigma_k^{-1}(x-\\mu_k)\\) jest formą kwadratową, granice decyzyjne między klasami są kwadratowe (nieliniowe w \\(x\\)). QDA jest bardziej elastyczna (potrafi modelować klasy o różnych „kształtach” i orientacjach w przestrzeni cech), ale płaci za to większą liczbą parametrów, co wymaga większej próbki treningowej dla stabilnej estymacji.\nW LDA zakładamy wspólną kowariancję dla klas:\n\\[\n\\Sigma_k = \\Sigma \\quad \\text{dla każdego }k.\n\\]\nWtedy człony kwadratowe w \\(x\\) „redukują się” w porównaniu między klasami i funkcja dyskryminacyjna upraszcza się do postaci liniowej:\n\\[\n\\delta_k(x) = x^\\top \\Sigma^{-1}\\mu_k - \\frac{1}{2}\\mu_k^\\top \\Sigma^{-1}\\mu_k + \\log\\pi_k.\n\\]\nGranice decyzyjne są liniowe (hiperpłaszczyzny), bo \\(\\delta_k(x)\\) jest liniowa ze względu na \\(x\\). LDA ma zwykle mniejszą wariancję estymacji (mniej parametrów niż QDA), dlatego często jest konkurencyjna nawet wtedy, gdy prawdziwa zależność nie jest idealnie zgodna z założeniami.\n\n\n4.5.4 \\(\\Sigma=I\\), \\(\\Sigma_k=\\Sigma\\), \\(\\Sigma_k\\) dowolne\nW praktyce warto widzieć analizę dyskryminacyjną jako rodzinę modeli wynikającą z tego, jak restrykcyjnie opisujemy kowariancję.\n\n\\(\\Sigma = I\\). To najbardziej restrykcyjny wariant. Oznacza, że w każdej klasie cechy są „niezależne” i mają tę samą wariancję (w odpowiedniej skali). Wtedy odległość Mahalanobisa redukuje się do euklidesowej i reguła klasyfikacji staje się bardzo bliska nearest centroid (najbliższe centrum klasy). Jest to model prosty i często zaskakująco skuteczny po standaryzacji, ale może być niedopasowany, gdy cechy są skorelowane.\n\\(\\Sigma_k = \\Sigma\\) (LDA: wspólna, ale dowolna macierz kowariancji). To kompromis: dopuszczamy korelacje i różne wariancje cech, ale zakładamy, że „kształt rozkładu” w przestrzeni cech jest taki sam dla wszystkich klas, tylko przesunięty o różne \\(\\mu_k\\). Wtedy granice są liniowe.\n\\(\\Sigma_k\\) dowolne (QDA). Najbardziej elastyczne: każda klasa ma własny „kształt” i orientację elipsoidy kowariancji. Granice są kwadratowe. Ten wariant jest najbardziej wrażliwy na małe próby (estymacja \\(\\Sigma_k^{-1}\\) bywa niestabilna), dlatego często wymaga albo większej liczby obserwacji, albo regularizacji (np. RDA – regularized discriminant analysis; klasycznie opisane przez Friedmana). ￼\n\nUwaga praktyczna: spotyka się też wariant pośredni \\(\\Sigma\\) diagonalna (brak korelacji, ale różne wariancje cech). Jest to bliskie „gaussowskiemu naiwnemu Bayesowi” w wersji z normalnymi rozkładami cech.\n\n\n4.5.5 Jak estymuje się parametry w uczeniu nadzorowanym?\nDla danych treningowych \\(\\{(x_i,y_i)\\}_{i=1}^n\\), gdzie \\(n_k\\) to liczba obserwacji w klasie \\(k\\), standardowe estymatory (MLE) mają postać:\n\\[\n\\hat{\\pi}_k = \\frac{n_k}{n},\n\\qquad\n\\hat{\\mu}_k = \\frac{1}{n_k}\\sum_{i:\\,y_i=k} x_i.\n\\]\nDla LDA estymujemy wspólną kowariancję jako kowariancję „wewnątrzklasową” (pooled covariance):\n\\[\n\\hat{\\Sigma}\n=\n\\frac{1}{n-K}\n\\sum_{k=1}^K\n\\sum_{i:\\,y_i=k}\n(x_i-\\hat{\\mu}_k)(x_i-\\hat{\\mu}_k)^\\top.\n\\]\nDla QDA estymujemy \\(\\hat{\\Sigma}_k\\) osobno w każdej klasie:\n\\[\n\\hat{\\Sigma}_k\n=\n\\frac{1}{n_k-1}\n\\sum_{i:\\,y_i=k}\n(x_i-\\hat{\\mu}_k)(x_i-\\hat{\\mu}_k)^\\top.\n\\]\nNastępnie do klasyfikacji używamy \\(\\delta_k(x)\\) z odpowiedniego wariantu (LDA/QDA). W praktyce w scikit-learn jest to realizowane wprost (fit → estymuje \\(\\pi_k\\), \\(\\mu_k\\), \\(\\Sigma\\) lub \\(\\Sigma_k\\); predict/predict_proba → liczy \\(\\delta_k\\) i normalizuje do prawdopodobieństw).\n\nPrzykład 4.3 Poniżej przykład LDA i QDA na klasycznym zbiorze Iris dostępny na Hugging Face (scikit-learn/iris). Zbiór zawiera 150 obserwacji trzech gatunków irysów, opisanych czterema cechami liczbowymi. ￼\nW przykładzie:\n\nrobimy podział train/test,\nuczymy LDA i QDA,\nraportujemy accuracy i macierz pomyłek,\npokazujemy log-loss jako miarę jakości probabilistycznej,\nnarysujemy brzegi decyzyjne obu metod.\n\n\n\nKod\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom datasets import load_dataset\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\nfrom sklearn.metrics import accuracy_score, confusion_matrix, log_loss\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n\n# 1) Wczytanie Iris z Hugging Face\nds = load_dataset(\"scikit-learn/iris\")[\"train\"]\ndf = ds.to_pandas()\ndf = df.iloc[:, 1:]  # Usunięcie pierwszej kolumny (Id)\n\ntarget = \"Species\"\nX = df.select_dtypes(include=[\"number\"]).drop(columns=[target], errors=\"ignore\")\n\n# Kodujemy klasy do liczb (wymagane m.in. do rysowania brzegów decyzyjnych przez contourf)\ny_cat = df[target].astype(\"category\")\nclass_names = list(y_cat.cat.categories)\ny = y_cat.cat.codes\n\n# 2) Podział train/test (stratyfikacja dla klas)\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.25, random_state=42, stratify=y\n)\n\n# 2b) Standaryzacja (fit TYLKO na train) – cały przykład działa na danych standaryzowanych\nscaler = StandardScaler()\nX_train_std = scaler.fit_transform(X_train)\nX_test_std = scaler.transform(X_test)\nX_train_std_df = pd.DataFrame(X_train_std, columns=X.columns, index=X_train.index)\nX_test_std_df = pd.DataFrame(X_test_std, columns=X.columns, index=X_test.index)\n\n# 3) LDA\nlda = LinearDiscriminantAnalysis()\nlda.fit(X_train_std_df, y_train)\npred_lda = lda.predict(X_test_std_df)\nproba_lda = lda.predict_proba(X_test_std_df)\n\n# 4) QDA\nqda = QuadraticDiscriminantAnalysis()\nqda.fit(X_train_std_df, y_train)\npred_qda = qda.predict(X_test_std_df)\nproba_qda = qda.predict_proba(X_test_std_df)\n\n# 5) Metryki i macierze pomyłek\nacc_lda = accuracy_score(y_test, pred_lda)\nacc_qda = accuracy_score(y_test, pred_qda)\n\ncm_lda = confusion_matrix(y_test, pred_lda)\ncm_qda = confusion_matrix(y_test, pred_qda)\n\nprint(f\"LDA  accuracy: {acc_lda:.3f}, log_loss: {log_loss(y_test, proba_lda):.3f}\")\nprint(\"LDA confusion matrix:\\n\", pd.DataFrame(cm_lda, index=class_names, columns=class_names))\n\nprint(f\"\\nQDA  accuracy: {acc_qda:.3f}, log_loss: {log_loss(y_test, proba_qda):.3f}\")\nprint(\"QDA confusion matrix:\\n\", pd.DataFrame(cm_qda, index=class_names, columns=class_names))\n\n# 6) Brzegi decyzyjne (wizualizacja 2D)\n# Uwaga: Iris ma 4 cechy, a wykres 2D wymaga wyboru 2 osi.\nfeat_cols = list(X.columns[[0,2]])\nall_feat_cols = list(X.columns)\n\ndef plot_decision_boundary_fullmodel_on_2features(\n    ax,\n    model,\n    X_std_df,\n    y_vis,\n    all_feature_names,\n    vary_feature_names,\n    title,\n    n_classes,\n):\n    # Decision boundary jako przekrój przestrzeni cech:\n    # zmieniamy tylko 2 wybrane cechy, pozostałe ustawiamy na 0 (średnia po standaryzacji).\n    x0 = X_std_df[vary_feature_names[0]].to_numpy()\n    x1 = X_std_df[vary_feature_names[1]].to_numpy()\n\n    x0_min, x0_max = x0.min() - 0.5, x0.max() + 0.5\n    x1_min, x1_max = x1.min() - 0.5, x1.max() + 0.5\n\n    xx0, xx1 = np.meshgrid(\n        np.linspace(x0_min, x0_max, 300),\n        np.linspace(x1_min, x1_max, 300),\n    )\n\n    base = np.zeros((xx0.size, len(all_feature_names)), dtype=float)\n    idx0 = all_feature_names.index(vary_feature_names[0])\n    idx1 = all_feature_names.index(vary_feature_names[1])\n    base[:, idx0] = xx0.ravel()\n    base[:, idx1] = xx1.ravel()\n\n    grid_df = pd.DataFrame(base, columns=all_feature_names)\n    zz = model.predict(grid_df).reshape(xx0.shape)\n\n    levels = np.arange(n_classes + 1) - 0.5\n    cmap = plt.get_cmap(\"tab10\", n_classes)\n\n    ax.contourf(xx0, xx1, zz, levels=levels, alpha=0.25, cmap=cmap)\n    ax.scatter(x0, x1, c=y_vis, cmap=cmap, edgecolor=\"k\", s=35)\n    ax.set_xlabel(vary_feature_names[0])\n    ax.set_ylabel(vary_feature_names[1])\n    ax.set_title(title)\n\nn_classes = int(np.unique(y).size)\n\nfig, axes = plt.subplots(1, 2, figsize=(11, 4))\nplot_decision_boundary_fullmodel_on_2features(\n    axes[0],\n    lda,\n    X_test_std_df,\n    y_test,\n    all_feature_names=all_feat_cols,\n    vary_feature_names=feat_cols,\n    title=\"LDA – brzeg decyzyjny (przekrój po 2 cechach; model pełny)\",\n    n_classes=n_classes,\n)\nplot_decision_boundary_fullmodel_on_2features(\n    axes[1],\n    qda,\n    X_test_std_df,\n    y_test,\n    all_feature_names=all_feat_cols,\n    vary_feature_names=feat_cols,\n    title=\"QDA – brzeg decyzyjny (przekrój po 2 cechach; model pełny)\",\n    n_classes=n_classes,\n)\nplt.tight_layout()\nplt.show()\n\n# 7) Brzegi decyzyjne w przestrzeni PCA(2)\n# PCA robimy po standaryzacji.\n# Brzeg decyzyjny liczymy w przestrzeni PC1/PC2, ale predykcje robi model uczony\n# na pełnych cechach: PC-grid -&gt; inverse_transform -&gt; predykcja.\npca = PCA(n_components=2, random_state=42)\nX_train_pca = pca.fit_transform(X_train_std)\nX_test_pca = pca.transform(X_test_std)\n\npc_cols = [\"PC1\", \"PC2\"]\nX_train_pca_df = pd.DataFrame(X_train_pca, columns=pc_cols, index=X_train.index)\nX_test_pca_df = pd.DataFrame(X_test_pca, columns=pc_cols, index=X_test.index)\n\ndef plot_decision_boundary_fullmodel_in_pca2(\n    ax,\n    model,\n    pca,\n    X_pca_df,\n    y_vis,\n    orig_feature_names,\n    title,\n    n_classes,\n):\n    x0 = X_pca_df[\"PC1\"].to_numpy()\n    x1 = X_pca_df[\"PC2\"].to_numpy()\n\n    x0_min, x0_max = x0.min() - 0.5, x0.max() + 0.5\n    x1_min, x1_max = x1.min() - 0.5, x1.max() + 0.5\n\n    xx0, xx1 = np.meshgrid(\n        np.linspace(x0_min, x0_max, 300),\n        np.linspace(x1_min, x1_max, 300),\n    )\n\n    pc_grid = np.c_[xx0.ravel(), xx1.ravel()]\n    grid_std = pca.inverse_transform(pc_grid)\n    grid_std_df = pd.DataFrame(grid_std, columns=orig_feature_names)\n    zz = model.predict(grid_std_df).reshape(xx0.shape)\n\n    levels = np.arange(n_classes + 1) - 0.5\n    cmap = plt.get_cmap(\"tab10\", n_classes)\n\n    ax.contourf(xx0, xx1, zz, levels=levels, alpha=0.25, cmap=cmap)\n    ax.scatter(x0, x1, c=y_vis, cmap=cmap, edgecolor=\"k\", s=35)\n    ax.set_xlabel(\"PC1\")\n    ax.set_ylabel(\"PC2\")\n    ax.set_title(title)\n\nfig, axes = plt.subplots(1, 2, figsize=(11, 4))\nplot_decision_boundary_fullmodel_in_pca2(\n    axes[0],\n    lda,\n    pca,\n    X_test_pca_df,\n    y_test,\n    orig_feature_names=all_feat_cols,\n    title=\"LDA – brzeg decyzyjny (PCA2; model pełny)\",\n    n_classes=n_classes,\n)\nplot_decision_boundary_fullmodel_in_pca2(\n    axes[1],\n    qda,\n    pca,\n    X_test_pca_df,\n    y_test,\n    orig_feature_names=all_feat_cols,\n    title=\"QDA – brzeg decyzyjny (PCA2; model pełny)\",\n    n_classes=n_classes,\n)\nplt.tight_layout()\nplt.show()\n\n\nLDA  accuracy: 1.000, log_loss: 0.024\nLDA confusion matrix:\n                  Iris-setosa  Iris-versicolor  Iris-virginica\nIris-setosa               12                0               0\nIris-versicolor            0               13               0\nIris-virginica             0                0              13\n\nQDA  accuracy: 1.000, log_loss: 0.017\nQDA confusion matrix:\n                  Iris-setosa  Iris-versicolor  Iris-virginica\nIris-setosa               12                0               0\nIris-versicolor            0               13               0\nIris-virginica             0                0              13",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Klasyfikacja liniowa</span>"
    ]
  },
  {
    "objectID": "chapters/03-klasyfikacja-liniowa.html#literatura",
    "href": "chapters/03-klasyfikacja-liniowa.html#literatura",
    "title": "4  Klasyfikacja liniowa",
    "section": "Literatura",
    "text": "Literatura\n\n\n\n\nFisher, R. A. 1936. „The Use of Multiple Measurements in Taxonomic Problems”. Annals of Eugenics 7 (2): 179–88. https://doi.org/10.1111/j.1469-1809.1936.tb02137.x.\n\n\nWardJR, Joe H., i Marion E. Hook. 1963. „Application of an Hierarchical Grouping Procedure to a Problem of Grouping Profiles”. Educational and Psychological Measurement 23 (1): 69–81. https://doi.org/10.1177/001316446302300107.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Klasyfikacja liniowa</span>"
    ]
  },
  {
    "objectID": "chapters/04-drzewa-zespoly.html",
    "href": "chapters/04-drzewa-zespoly.html",
    "title": "5  Drzewa decyzyjne i zespoly modeli",
    "section": "",
    "text": "5.1 Podstawowe drzewa decyzyjne",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Drzewa decyzyjne i zespoly modeli</span>"
    ]
  },
  {
    "objectID": "chapters/04-drzewa-zespoly.html#podstawowe-drzewa-decyzyjne",
    "href": "chapters/04-drzewa-zespoly.html#podstawowe-drzewa-decyzyjne",
    "title": "5  Drzewa decyzyjne i zespoly modeli",
    "section": "",
    "text": "5.1.1 Klasyczne rodziny algorytmów drzew i ich różnice\nW literaturze i praktyce spotyka się kilka „rodzin” algorytmów drzew decyzyjnych. Różnią się one m.in. sposobem doboru podziału (kryterium jakości), dopuszczalną liczbą gałęzi w węźle, obsługą braków danych, strategią przycinania oraz tym, czy wnioski statystyczne są wbudowane w procedurę uczenia.\nID3 (Iterative Dichotomiser 3) (Quinlan) to jeden z najwcześniejszych, wpływowych algorytmów drzew klasyfikacyjnych. Uczy drzewo wielogałęziowe (tzn. węzeł może mieć tyle gałęzi, ile kategorii ma dana cecha) i wybiera podział na podstawie zysku informacji (information gain), który jest redukcją entropii po podziale. W klasycznej postaci ID3 był projektowany głównie dla cech kategorycznych, nie posiadał pełnego, ustandaryzowanego mechanizmu przycinania i jest wrażliwy na cechy o dużej liczbie kategorii (mogą sztucznie „wygrywać” kryterium entropijne). Z perspektywy ogólnego spojrzenia na drzewa decyzyjne ważne jest to, że ID3 ustanowił wzorzec: rekurencyjne dzielenie + entropia/zysk informacji.\nC4.5 (Quinlan) jest rozwinięciem ID3 i przez wiele lat był stosowanym standardem. Wprowadza on m.in. (i) obsługę zmiennych ciągłych przez poszukiwanie progu i podziały binarne dla cech liczbowych, (ii) modyfikację kryterium jakości podziału w postaci współczynnika zysku informacji (gain ratio), który koryguje preferencję dla cech o wielu kategoriach, (iii) obsługę braków danych przez rozdzielanie obserwacji z brakami „miękko” (z wagami) między gałęzie lub przez dopasowane heurystyki, oraz (iv) przycinanie oparte o oszacowania błędu (tzw. error-based pruning). W praktyce C4.5 produkuje drzewa, które są zwykle mniejsze i bardziej uogólniające niż ID3.\nC5.0 to następca C4.5 (Quinlan), zaprojektowany jako szybszy i bardziej skalowalny, z licznymi usprawnieniami inżynieryjnymi i heurystycznymi. Typowo oferuje lepszą wydajność obliczeniową, mniejsze zużycie pamięci oraz dodatkowe możliwości praktyczne (np. kosztowną klasyfikację, mechanizmy „wzmocnienia” w stylu boosting/committee w niektórych implementacjach). Koncepcyjnie nadal jest to drzewo „w duchu Quinlana”: kryteria entropijne, sprawna obsługa cech ciągłych i braków oraz silny nacisk na praktyczne uogólnianie.\nCART (Classification and Regression Trees) (Breiman i in.) ujednolica podejście do klasyfikacji i regresji w ramach jednego formalizmu. Charakterystyczne cechy CART są następujące: (i) podziały są zazwyczaj binarne (nawet dla cech kategorycznych – kategorie dzieli się na dwie grupy), (ii) w klasyfikacji stosuje się zwykle indeks Giniego (lub entropię), a w regresji kryterium oparte o SSE/wariancję, (iii) kluczowym elementem jest przycinanie złożoności (cost-complexity pruning), tj. wybór drzewa przez kompromis między dopasowaniem i złożonością, oraz (iv) mechanizm reguł zastępczych (surrogate splits) jako systematyczna odpowiedź na braki danych. CART jest dziś szczególnie ważne, bo stanowi bazę dla wielu metod zespołowych (random forest, gradient boosting) i jest najczęściej spotykaną „architekturą” drzew w ML.\nConditional inference trees (conditional trees, ctree) (Hothorn, Hornik, Zeileis) powstały jako odpowiedź na znane uprzedzenia klasycznych kryteriów podziału (Gini/entropia/SSE), które mogą faworyzować cechy o wielu możliwych podziałach (np. zmienne ciągłe lub kategoryczne o wielu poziomach). W ctree wybór podziału ma charakter statystyczny i opiera się na testach permutacyjnych niezależności. Procedura jest dwuetapowa: najpierw w danym węźle testuje się hipotezę globalną, że zmienna docelowa jest niezależna od wszystkich cech (jeśli brak podstaw do jej odrzucenia, węzeł staje się liściem), a następnie – w razie istotności – wykonuje się testy cząstkowe dla każdej cechy i wybiera tę o najsilniejszej zależności z odpowiednią korektą na wielokrotne porównania. Typ testu zależy od typu zmiennej docelowej i predyktora: dla klasyfikacji (\\(Y\\) kategoryczne) stosuje się permutacyjne testy niezależności odpowiadające m.in. testom \\(\\chi^2\\) (gdy \\(X\\) kategoryczne) lub testom porównania rozkładów/średnich typu ANOVA/F (gdy \\(X\\) ciągłe), natomiast dla regresji (\\(Y\\) ciągłe) stosuje się permutacyjne testy zależności odpowiadające testom korelacyjnym/regresyjnym (gdy \\(X\\) ciągłe) albo testom różnic średnich typu ANOVA (gdy \\(X\\) kategoryczne) – zawsze jednak w wersji permutacyjnej. Dzięki temu conditional trees redukują tendencyjność selekcji zmiennych, a kryterium stopu jest naturalnie powiązane z istotnością statystyczną, a nie wyłącznie z heurystycznymi parametrami złożoności.\nCHAID (Chi-squared Automatic Interaction Detection) to klasyczna metoda drzew wielogałęziowych, szczególnie popularna w analizie marketingowej i badaniach społecznych. Jej znakiem rozpoznawczym są: (i) podziały wybierane na podstawie testów chi-kwadrat (dla klasyfikacji) lub testów analogicznych dla zmiennych ciągłych, (ii) możliwość łączenia kategorii cech kategorycznych w większe grupy przed wykonaniem podziału, oraz (iii) częste stosowanie podziałów wielogałęziowych (niekoniecznie binarnych). CHAID jest ceniony za interpretowalność i naturalne traktowanie interakcji w kategoriach, ale w porównaniu do CART bywa mniej „ML-owy” w sensie typowych współczesnych pipeline’ów i częściej występuje w narzędziach statystycznych/BI.\nZasada działania drzewa jest rekurencyjna: przestrzeń cech jest dzielona na coraz mniejsze obszary przez kolejne pytania o wartości cech. Każdy podział wybiera jedną cechę i warunek (np. \\(x_j \\le t\\)), który rozdziela obserwacje na dwie (lub więcej) grupy. Proces powtarza się w powstałych podzbiorach aż do spełnienia kryterium stopu (np. minimalna liczba obserwacji w węźle, maksymalna głębokość, brak istotnej poprawy jakości). W efekcie otrzymujemy model złożony z węzłów wewnętrznych (podziały) oraz liści (obszary decyzyjne).\n\n\n5.1.2 Rodzaje drzew i podstawowe elementy konstrukcji\nW praktyce wyróżnia się przede wszystkim:\n\ndrzewa klasyfikacyjne – gdy zmienna docelowa jest dyskretna (klasy),\ndrzewa regresyjne – gdy zmienna docelowa jest ciągła,\ndrzewa wieloklasowe – naturalne rozszerzenie klasyfikacji binarnej,\ndrzewa binarne vs wielogałęziowe – CART stosuje zwykle podziały binarne, natomiast w niektórych wariantach dopuszcza się podziały na wiele gałęzi (częściej dla cech kategorycznych).\n\nDrzewo składa się z:\n\nkorzenia (root) – węzła startowego zawierającego cały zbiór uczący,\nwęzłów wewnętrznych (internal nodes) – zawierają regułę podziału (cecha + warunek),\ngałęzi (branches) – odpowiadają wynikom reguły (np. „tak/nie” dla podziału binarnego),\nliści (leaves/terminal nodes) – węzłów końcowych przechowujących predykcję (klasę lub wartość),\n(opcjonalnie) wag/rozkładów w liściu – np. częstości klas lub parametry prostej regresji w liściu.\n\n\n\n\n5.1.3 Reguły podziału: klasyfikacja i regresja\nWęzeł drzewa ma za zadanie wybrać taki podział, który „poprawia jednorodność” powstałych grup. Formalnie dla węzła zawierającego zbiór obserwacji \\(S\\) rozważamy kandydatów podziału \\(s\\) (cecha \\(j\\) i próg \\(t\\), ewentualnie podział kategorii). Podział rozdziela \\(S\\) na \\(S_L\\) i \\(S_R\\). Wybieramy \\(s\\), który maksymalizuje spadek nieczystości (impurity decrease):\n\\[\n\\Delta I(s) = I(S) - \\frac{|S_L|}{|S|} I(S_L) - \\frac{|S_R|}{|S|} I(S_R).\n\\]\nDrzewa klasyfikacyjne. Nieczystość \\(I(S)\\) definiuje się najczęściej jako:\n\nindeks Giniego: \\[\nI_G(S) = 1 - \\sum_{k=1}^K p_k^2,\n\\]\nentropię (Shannon): \\[\nI_H(S) = -\\sum_{k=1}^K p_k \\log p_k,\n\\]\n\ngdzie \\(p_k\\) to odsetek obserwacji klasy \\(k\\) w węźle. Intuicyjnie, im bardziej rozkład klas jest skoncentrowany w jednej klasie, tym mniejsza nieczystość. Podział jest „dobry”, jeśli znacząco zwiększa jednorodność klas w dzieciach.\nDrzewa regresyjne. W regresji nieczystość mierzy się rozproszeniem wartości \\(y\\) w węźle. Najczęściej stosuje się wariancję lub równoważnie sumę kwadratów odchyleń od średniej (SSE):\n\\[\nI_{\\text{SSE}}(S) = \\sum_{i\\in S} (y_i - \\bar{y}_S)^2.\n\\]\nPodział wybieramy tak, aby minimalizować łączną SSE po podziale (czyli maksymalizować jej redukcję). W liściu predykcją jest zwykle \\(\\bar{y}_S\\) (średnia w liściu), co jest rozwiązaniem minimalizującym SSE w obrębie liścia.\n\n\n5.1.4 Jak szuka się optymalnego podziału\nW idealnym (globalnym) ujęciu chcielibyśmy znaleźć takie drzewo, które minimalizuje błąd predykcji przy zadanej „złożoności” (np. liczbie liści). Taki problem jest jednak obliczeniowo bardzo trudny: liczba możliwych drzew rośnie wykładniczo wraz z liczbą obserwacji i cech, a wybór optymalnej struktury wymagałby przeszukania ogromnej przestrzeni kombinatorycznej. Dlatego praktyczne algorytmy drzew (CART, C4.5, CHAID, ctree) stosują podejście zachłanne (greedy, top–down recursive partitioning): w każdym węźle wybierają najlepszy lokalnie podział według zadanego kryterium i dopiero potem powtarzają procedurę w węzłach potomnych.\nW podejściu zachłannym konstrukcja drzewa ma postać iteracji:\n\nW węźle z danymi \\(S\\) rozważamy zbiór kandydatów podziału \\(s\\) (cecha \\(j\\) i warunek podziału).\nDla każdego kandydata obliczamy spadek nieczystości \\(\\Delta I(s)\\) (dla klasyfikacji: Gini/entropia; dla regresji: SSE/wariancja).\nWybieramy \\(s^* = \\arg\\max_s \\Delta I(s)\\), wykonujemy podział \\(S \\to (S_L,S_R)\\).\nRekurencyjnie powtarzamy kroki 1–3 w węzłach potomnych, dopóki nie zajdzie warunek stopu.\n\n\n5.1.4.1 Cechy ciągłe: jak wyznacza się kandydatów progów\nDla cechy ciągłej \\(x_j\\) naturalną regułą podziału jest próg \\(t\\): \\(x_j \\le t\\) vs \\(x_j &gt; t\\). Kandydatami progów nie są wszystkie liczby rzeczywiste, lecz wartości „pomiędzy” obserwacjami. W praktyce postępuje się tak:\n\nsortuje się obserwacje według \\(x_j\\),\nrozważa się progi będące środkami między kolejnymi różnymi wartościami \\(x_j\\), tj. \\(t = (v_{(m)} + v_{(m+1)})/2\\),\nczęsto pomija się progi, które nie zmieniają przypisań (np. wiele powtórzeń wartości) lub które łamią ograniczenia typu min_samples_leaf.\n\nDzięki temu liczba kandydatów dla jednej cechy jest rzędu \\(O(n)\\), a nie nieskończona. W implementacjach produkcyjnych dodatkowo używa się trików obliczeniowych: po posortowaniu można aktualizować liczności klas (lub sumy/kwadraty sum w regresji) „przesuwając” próg krok po kroku, bez przeliczania wszystkiego od zera, co istotnie przyspiesza selekcję najlepszego \\(t\\).\n\n\n5.1.4.2 Cechy kategoryczne: podziały binarne i wielogałęziowe\nDla cech kategorycznych istnieją dwie główne szkoły:\n\npodziały wielogałęziowe (np. w ID3/C4.5/CHAID): węzeł może mieć osobną gałąź dla każdej kategorii; to bywa bardzo interpretowalne, ale może prowadzić do „fragmentacji” danych (małe liczności w gałęziach),\npodziały binarne (CART): kategorie dzieli się na dwie grupy \\(A\\) i \\(\\bar A\\), tzn. \\(x_j \\in A\\) vs \\(x_j \\notin A\\).\n\nPodziały binarne dla cechy o \\(m\\) kategoriach mają w najgorszym razie \\(2^{m-1}-1\\) możliwych podziałów, co szybko staje się niepraktyczne. Dlatego stosuje się heurystyki i uproszczenia. Przykładowo:\n\nw regresji porządkuje się kategorie według średniej \\(\\bar y\\) i rozważa rozcięcia jak dla zmiennej uporządkowanej,\nw klasyfikacji można porządkować kategorie według \\(\\hat p(y=1\\mid x_j)\\) (dla binarnej klasy) lub stosować przybliżone przeszukiwanie,\nprzy dużej liczbie poziomów stosuje się łączenie rzadkich kategorii do other lub narzuca minimalne liczności.\n\n\n\n5.1.4.3 Braki danych a wybór podziału\nW zależności od algorytmu i implementacji, braki mogą być obsługiwane na kilka sposobów: przez wcześniejszą imputację, przez traktowanie „braku” jako osobnej kategorii (częste w praktyce), przez wybór domyślnego kierunku podziału (np. brak \\(\\to\\) lewa gałąź) lub przez mechanizmy takie jak surrogate splits w CART 1. Istotne jest, że sposób obsługi braków wpływa zarówno na wynik \\(\\Delta I(s)\\), jak i na stabilność drzewa.\n1 Reguły zastępcze to mechanizm kojarzony przede wszystkim z CART, używany gdy dla obserwacji brakuje wartości cechy, według której w danym węźle wykonywany jest podział. Zamiast odrzucać obserwację lub imputować brak, drzewo może wybrać alternatywną regułę podziału opartą o inną cechę, która możliwie najlepiej „naśladuje” podział główny. W praktyce buduje się ranking reguł zastępczych na podstawie zgodności przypisań do gałęzi (np. jak często podział zastępczy wysyła obserwacje do tej samej strony co podział główny w danych treningowych). Dzięki temu drzewo zachowuje spójność działania nawet przy brakach danych.\n\n5.1.4.4 Kontrola złożoności: pre-pruning i (w CART) post-pruning\nPonieważ strategia zachłanna łatwo prowadzi do bardzo głębokich drzew, w praktyce kontroluje się złożoność na dwa sposoby:\n\npre-pruning (ograniczenia w trakcie budowy): zamiast budować bardzo głębokie drzewo, można narzucić ograniczenia już na etapie wzrostu, aby zmniejszyć wariancję i ryzyko przeuczenia.\n\nmax_depth – maksymalna głębokość drzewa (liczba krawędzi/poziomów od korzenia do liścia). Małe wartości ograniczają liczbę kolejnych podziałów, co zwykle zwiększa bias, ale zmniejsza wariancję.\nmin_samples_split – minimalna liczba obserwacji w węźle, aby w ogóle rozważać jego podział. Jeśli węzeł ma mniej obserwacji, staje się liściem.\nmin_samples_leaf – minimalna liczba obserwacji, która musi pozostać w każdym liściu po podziale. W praktyce eliminuje to podziały tworzące bardzo małe, niestabilne liście (np. podział 3 vs 97 przy min_samples_leaf=10 jest niedozwolony).\nmax_leaf_nodes – maksymalna liczba liści w całym drzewie. To bezpośrednia kontrola złożoności, bo liczba liści determinuje liczbę regionów decyzyjnych.\nmin_impurity_decrease – minimalna wymagana redukcja nieczystości \\(\\Delta I(s)\\), aby zaakceptować podział. Jeśli najlepszy możliwy podział w węźle nie daje spadku nieczystości większego od progu, algorytm przerywa wzrost i tworzy liść.\n\npost-pruning (przycinanie po zbudowaniu dużego drzewa): w CART standardowo buduje się najpierw drzewo „maksymalne” (silnie dopasowane, często aż do spełnienia minimalnych ograniczeń liczności), a następnie usuwa się jego gałęzie, wybierając takie poddrzewo, które najlepiej równoważy dopasowanie i złożoność. Klasyczne podejście to cost-complexity pruning (znane też jako weakest-link pruning), w którym rozważa się rodzinę poddrzew \\(T_\\alpha\\) minimalizujących kompromis\n\\[\nR(T) + \\alpha\\,|T|,\n\\]\ngdzie:\n\n\\(R(T)\\) jest miarą „błędu” lub „straty” drzewa (w CART często jest to błąd resubstytucji / suma nieczystości w liściach, np. suma SSE w regresji lub suma nieczystości Giniego ważona licznościami w klasyfikacji; w praktyce interesuje nas przede wszystkim zgodność tej miary z błędem generalizacji),\n\\(|T|\\) to liczba liści (terminal nodes) – prosta miara złożoności drzewa,\n\\(\\alpha \\ge 0\\) to parametr kary za złożoność: dla \\(\\alpha \\to 0\\) preferowane jest drzewo duże (mały nacisk na prostotę), a dla dużych \\(\\alpha\\) otrzymujemy coraz płytsze drzewa.\n\nJak przebiega przycinanie w CART (idea „najsłabszego ogniwa”). Dla każdego węzła wewnętrznego \\(t\\) rozważa się zastąpienie całego poddrzewa \\(T_t\\) pojedynczym liściem i ocenia się, „jak dużo poprawy dopasowania” daje to poddrzewo w przeliczeniu na „ile dodatkowych liści kosztuje”. Formalnie wyznacza się wskaźnik krytyczny (tzw. wartość \\(\\alpha\\) dla węzła):\n\\[\ng(t) = \\frac{R(t) - R(T_t)}{|T_t| - 1},\n\\]\ngdzie \\(R(t)\\) to strata, gdy \\(T_t\\) zostanie zastąpione jednym liściem, a \\(R(T_t)\\) to strata pełnego poddrzewa. W każdym kroku usuwa się to poddrzewo, które ma najmniejsze \\(g(t)\\) (czyli „najmniej opłaca się” je utrzymywać) – stąd nazwa weakest-link. Powtarzając ten krok, otrzymuje się skończoną sekwencję zagnieżdżonych poddrzew\n\\[\nT_0 \\supset T_1 \\supset \\cdots \\supset T_M,\n\\]\nodpowiadających rosnącym wartościom \\(\\alpha\\). Dzięki temu zamiast przeszukiwać wszystkie możliwe drzewa, analizujemy tylko tę sekwencję kandydatów.\nDobór \\(\\alpha\\) (czyli wybór końcowego drzewa). Parametr \\(\\alpha\\) dobiera się zwykle przez walidację krzyżową: dla każdego kandydata \\(T_m\\) estymuje się błąd generalizacji i wybiera drzewo o najlepszej jakości. Często stosuje się też zasadę 1-SE: wybiera się najmniejsze drzewo, którego błąd walidacyjny jest nie większy niż minimum powiększone o jedno odchylenie standardowe, aby preferować model prostszy i stabilniejszy.\n\n\n\n\n5.1.5 Predykcja z drzewa\nPredykcja polega na „przejściu” obserwacji przez drzewo od korzenia do liścia, wykonując po drodze kolejne testy.\n\nDrzewo klasyfikacyjne: w liściu przechowuje się rozkład klas (częstości) \\(\\hat{p}_k\\). Predykcją klasy jest zwykle \\(\\arg\\max_k \\hat{p}_k\\), a predykcją probabilistyczną – wektor \\((\\hat{p}_1,\\dots,\\hat{p}_K)\\).\nDrzewo regresyjne: w liściu przechowuje się wartość liczbową, najczęściej średnią \\(\\bar{y}_S\\) (lub medianę, zależnie od kryterium). Predykcja to ta wartość przypisana do liścia, do którego trafia obserwacja.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Drzewa decyzyjne i zespoly modeli</span>"
    ]
  },
  {
    "objectID": "chapters/04-drzewa-zespoly.html#bagging-i-lasy-losowe",
    "href": "chapters/04-drzewa-zespoly.html#bagging-i-lasy-losowe",
    "title": "5  Drzewa decyzyjne i zespoly modeli",
    "section": "5.2 Bagging i lasy losowe",
    "text": "5.2 Bagging i lasy losowe\nBagging (bootstrap aggregating) i lasy losowe (random forests) należą do metod zespołowych (ensemble methods), w których wiele słabych/średnich modeli (zwykle drzew) łączy się w jeden silniejszy predyktor. Kluczowa intuicja jest taka, że pojedyncze drzewo decyzyjne ma zwykle niskie obciążenie (bias), ale wysoką wariancję – niewielka zmiana danych uczących może prowadzić do zauważalnie innej struktury drzewa i innych predykcji. Bagging i random forest redukują wariancję przez uśrednianie (agregację) wielu „odmian” tego samego algorytmu, uczonych na lekko zmodyfikowanych próbkach.\n\n5.2.1 Rys historyczny: od bootstrapu do lasów losowych\nRozwój tych metod można zrozumieć jako sekwencję pomysłów, które stopniowo zwiększały stabilność modeli i jakość uogólniania:\n\nBootstrap (Efron, 1979)2 - technika resamplingu „z powtórzeniami” do przybliżania rozkładów estymatorów i błędu. To właśnie bootstrap dał naturalny mechanizm generowania wielu „wersji” zbioru treningowego.\n\nBagging (Breiman, 1996)3 - pomysł, aby trenować ten sam algorytm na wielu próbkach bootstrapowych i agregować wyniki. Breiman pokazał, że bagging szczególnie dobrze stabilizuje metody niestabilne (jak drzewa).\n\nRandom subspace / losowanie cech (Ho, 1998)4 - idea, aby dodatkowo losować podzbiór cech, na których uczony jest model. To zmniejsza korelację między modelami w zespole.\n\nRandom Forest (Breiman, 2001)5 - połączenie baggingu drzew z losowaniem cech w każdym węźle drzewa (a nie tylko raz na drzewo). To okazało się bardzo skutecznym i prostym „domyślnym” modelem dla danych tablicowych.\n\nExtremely Randomized Trees (Geurts i in., 2006)6 - dalsza randomizacja poprzez losowanie progów podziału, co jeszcze bardziej dekoreluje drzewa, czasem poprawiając wynik kosztem większego bias.\n\n2 B. Efron (1979), Bootstrap Methods: Another Look at the Jackknife, The Annals of Statistics.3 L. Breiman (1996), Bagging Predictors, Machine Learning.4 T. K. Ho (1998), The Random Subspace Method for Constructing Decision Forests, IEEE TPAMI.5 L. Breiman (2001), Random Forests, Machine Learning.6 P. Geurts, D. Ernst, L. Wehenkel (2006), Extremely Randomized Trees, Machine Learning.\n\n5.2.2 Koncepcja baggingu: redukcja wariancji przez agregację\nNiech \\(\\mathcal{D} = \\{(x_i, y_i)\\}_{i=1}^n\\) oznacza zbiór uczący, a \\(\\hat f(\\cdot;\\mathcal{D})\\) – model (np. drzewo) wyuczony na danych \\(\\mathcal{D}\\). W baggingu generujemy \\(B\\) próbek bootstrapowych \\(\\mathcal{D}^{(1)},\\dots,\\mathcal{D}^{(B)}\\), gdzie każda \\(\\mathcal{D}^{(b)}\\) ma rozmiar \\(n\\) i powstaje przez losowanie obserwacji z \\(\\mathcal{D}\\) z powtórzeniami.\n\nBagging w regresji (uśrednianie):\n\\[\n\\hat f_{\\text{bag}}(x) = \\frac{1}{B}\\sum_{b=1}^B \\hat f^{(b)}(x),\n\\]\ngdzie \\(\\hat f^{(b)}(x) = \\hat f(x;\\mathcal{D}^{(b)})\\).\nBagging w klasyfikacji (głosowanie większościowe):\n\\[\n\\hat y_{\\text{bag}}(x) = \\arg\\max_{k\\in\\{1,\\dots,K\\}} \\sum_{b=1}^B \\mathbb{1}\\{\\hat y^{(b)}(x)=k\\}.\n\\]\nW wersji probabilistycznej często uśrednia się estymowane prawdopodobieństwa klas.\n\nDlaczego to działa (intuicja bias–variance). Jeśli pojedynczy model ma wariancję \\(\\mathrm{Var}(\\hat f(x))\\), to uśrednienie \\(B\\) modeli obniża wariancję. W idealnym przypadku niezależności modeli:\n\\[\n\\mathrm{Var}\\big(\\hat f_{\\text{bag}}(x)\\big) = \\frac{1}{B}\\,\\mathrm{Var}(\\hat f(x)).\n\\]\nW praktyce modele nie są niezależne; jeśli ich korelacja w punkcie \\(x\\) wynosi \\(\\rho\\), to w przybliżeniu:\n\\[\n\\mathrm{Var}\\big(\\hat f_{\\text{bag}}(x)\\big) \\approx \\rho\\,\\mathrm{Var}(\\hat f(x)) + \\frac{1-\\rho}{B}\\,\\mathrm{Var}(\\hat f(x)).\n\\]\nZatem kluczowe są dwa elementy: (i) duża liczba drzew \\(B\\) oraz (ii) mała korelacja \\(\\rho\\) między drzewami. Bagging zwiększa różnorodność przez bootstrap, a random forest dodatkowo obniża korelację przez losowanie cech.\n\n\n5.2.3 Algorytm baggingu (schemat)\nDla \\(b=1,\\dots,B\\):\n\nWylosuj próbkę bootstrapową \\(\\mathcal{D}^{(b)}\\) o rozmiarze \\(n\\) ze zbioru \\(\\mathcal{D}\\).\nNaucz model bazowy \\(\\hat f^{(b)}\\) na \\(\\mathcal{D}^{(b)}\\) (często jest to drzewo głębokie, słabo przycinane).\nZapisz model \\(\\hat f^{(b)}\\).\n\nPredykcja: agreguj \\(\\{\\hat f^{(b)}\\}\\) przez średnią (regresja) lub głosowanie (klasyfikacja).\n\n\n5.2.4 Lasy losowe: bagging + losowanie cech w węzłach\nRandom forest jest specjalnym przypadkiem baggingu, gdzie modelem bazowym jest drzewo decyzyjne, ale w każdym węźle drzewa nie rozważa się wszystkich \\(p\\) cech, tylko losowy podzbiór \\(m\\) cech (często oznaczany jako mtry). Następnie wybiera się najlepszy podział tylko wśród tych \\(m\\) cech. Dzięki temu różne drzewa stają się mniej do siebie podobne (mniejsza korelacja), a zespół lepiej redukuje wariancję.\n\n5.2.4.1 Algorytm budowy lasu losowego (RF)\nDla \\(b=1,\\dots,B\\):\n\nWylosuj próbkę bootstrapową \\(\\mathcal{D}^{(b)}\\).\nUcz drzewo \\(T^{(b)}\\) rekurencyjnie:\n\nw każdym węźle losuj bez zwracania \\(m\\) cech spośród \\(p\\),\nwyznacz najlepszy podział (maksymalna redukcja nieczystości) tylko wśród tych \\(m\\) cech,\nkontynuuj aż do kryterium stopu (często drzewo rośnie „głęboko”, np. do minimalnej liczności liścia).\n\n\nPredykcja:\n\nregresja: \\(\\hat f_{\\text{RF}}(x)=\\frac{1}{B}\\sum_{b=1}^B T^{(b)}(x)\\),\nklasyfikacja: głosowanie większościowe (lub uśrednianie \\(\\hat p_k(x)\\)).\n\n\n\n\n5.2.5 Najważniejsze parametry i ich interpretacja\nPoniżej zebrano parametry typowe dla implementacji w stylu scikit-learn (nazwy mogą się różnić w innych bibliotekach, ale sens jest ten sam).\n\n5.2.5.1 Parametry wspólne (bagging drzew i random forest)\n\nn_estimators (\\(B\\)) – liczba drzew. Zwiększanie \\(B\\) zwykle poprawia stabilność i jakość (wariancja maleje), ale z malejącymi przyrostami. W praktyce dobiera się \\(B\\) tak, aby wynik się „stabilizował”.\nbootstrap / max_samples – sposób i rozmiar próbkowania. Klasycznie losuje się \\(n\\) obserwacji z powtórzeniami; max_samples pozwala użyć ułamka danych (czasem przyspiesza, czasem zwiększa różnorodność).\nParametry drzewa bazowego (kontrola złożoności): max_depth, min_samples_leaf, min_samples_split, max_leaf_nodes, min_impurity_decrease. W zespołach często pozwala się drzewom rosnąć głęboko (niski bias), a wariancję kontroluje się przez agregację.\n\n\n\n5.2.5.2 Parametry specyficzne dla random forest\n\nmax_features (\\(m\\), mtry) – liczba losowanych cech w węźle. To parametr krytyczny dla korelacji między drzewami.\n\nJeśli \\(m=p\\), random forest redukuje się do klasycznego baggingu drzew (drzewa są bardziej podobne).\nJeśli \\(m\\) jest małe, drzewa są bardziej zróżnicowane (mniejsza korelacja), ale pojedyncze drzewo jest słabsze (większy bias).\n\nPopularne heurystyki startowe: w klasyfikacji \\(m\\approx\\sqrt{p}\\), w regresji \\(m\\approx p/3\\) (to są reguły kciuka, a nie prawa).\noob_score – ocena out-of-bag (OOB). W bootstrapie ok. \\(1-1/e\\approx 63.2\\%\\) obserwacji trafia do danej próbki, a pozostałe \\(~36.8\\%\\) są „poza próbką” dla tego drzewa (out-of-bag). Można więc estymować błąd generalizacji bez osobnego zbioru walidacyjnego: dla każdej obserwacji agreguje się predykcje tylko z drzew, które jej nie widziały.\nWażność cech (feature importance). Najczęściej spotkasz dwa podejścia:\n\nMDI (mean decrease in impurity) – uśredniona redukcja nieczystości przypisana do danej cechy po wszystkich węzłach i drzewach.\nPermutation importance – mierzy spadek jakości (np. accuracy/AUC) po losowym przemieszaniu wartości danej cechy; to podejście jest zwykle bardziej wiarygodne, bo mierzy wpływ na predykcję, ale jest droższe obliczeniowo.\n\nUwaga praktyczna: MDI może faworyzować cechy ciągłe lub o wielu poziomach; permutation importance jest na to mniej wrażliwa, choć wciąż może cierpieć przy silnie skorelowanych cechach.\n\n\n\n\n5.2.6 Bagging vs random forest: podobieństwa i różnice\n\nWspólne: oba podejścia uczą wiele drzew na próbkach bootstrapowych i agregują ich predykcje. Głównym celem jest redukcja wariancji.\nRóżnica kluczowa: random forest wprowadza dodatkową losowość przez max_features w każdym węźle, co obniża korelację między drzewami i zwykle poprawia wynik względem czystego baggingu drzew.\nKiedy bagging wystarcza: gdy liczba cech jest mała i drzewa i tak są zróżnicowane, zysk z losowania cech może być niewielki.\nKiedy RF wygrywa: gdy jest wiele cech i/lub część cech dominuje (silnie predykcyjnych) – losowanie cech zapobiega sytuacji, w której wszystkie drzewa w kółko wybierają te same pierwsze podziały.\n\n\n\n5.2.7 Dodatkowe uwagi praktyczne\n\nBrak potrzeby skalowania: drzewom i ich zespołom zwykle nie przeszkadzają różne skale cech (dzielą według progów), więc standaryzacja nie jest wymagana.\nOdporność na nieliniowości i interakcje: RF i bagging drzew automatycznie modelują interakcje i nieliniowości, co czyni je mocnym baseline’em.\nInterpretowalność: pojedyncze drzewo jest czytelne, ale las jest mniej transparentny. W praktyce stosuje się ważności cech, wykresy PDP/ICE czy SHAP (na innych zajęciach) do interpretacji.\nNieregularne braki i kategorie: w zależności od implementacji potrzebujesz imputacji/kodowania kategorii; część nowoczesnych bibliotek (np. CatBoost) rozwiązuje to natywnie, ale klasyczny RF często wymaga przygotowania danych.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Drzewa decyzyjne i zespoly modeli</span>"
    ]
  },
  {
    "objectID": "chapters/04-drzewa-zespoly.html#boosting",
    "href": "chapters/04-drzewa-zespoly.html#boosting",
    "title": "5  Drzewa decyzyjne i zespoly modeli",
    "section": "5.3 Boosting",
    "text": "5.3 Boosting\nBoosting to druga fundamentalna rodzina metod zespołowych, w której modele buduje się sekwencyjnie: każdy kolejny model ma korygować błędy poprzednich. W odróżnieniu od baggingu, który przede wszystkim redukuje wariancję przez uśrednianie wielu podobnych modeli uczonych niezależnie, boosting jest projektowany tak, aby stopniowo zmniejszać błąd systematyczny (bias), budując coraz lepszy predyktor addytywny. W praktyce boosting często daje bardzo wysoką jakość na danych tablicowych, ale wymaga ostrożnej kontroli złożoności, bo potrafi łatwiej niż bagging dopasować się nadmiernie do danych.\n\n5.3.1 Rys historyczny: od „słabych uczniów” do gradientowego boostingu\nRozwój boostingu można przedstawić jako przejście od idei teoretycznej do bardzo wydajnych implementacji inżynieryjnych:\n\nIdea „wzmacniania” słabych klasyfikatorów (Schapire, 1990)7 - pokazano, że jeśli istnieje algorytm osiągający wynik minimalnie lepszy niż losowy (weak learner), to można go „wzmocnić” do klasyfikatora o dowolnie małym błędzie treningowym przez odpowiednią procedurę zespołową.\n\nAdaBoost (Freund & Schapire, 1996/1997)8 - praktyczny algorytm boostingu, w którym kolejne klasyfikatory uczą się na danych z wagami, skupiając się na obserwacjach trudnych (wcześniej błędnie klasyfikowanych).\n\nGradient Boosting / MART (Friedman, 2001)9 - uogólnienie boostingu do postaci optymalizacji funkcji straty w przestrzeni funkcji, interpretowane jako „zejście gradientowe” w modelu addytywnym.\n\nXGBoost (Chen & Guestrin, 2016)10 - bardzo wydajna implementacja gradientowego boostingu drzew z regularizacją, obsługą braków i szeregiem optymalizacji obliczeniowych (m.in. przyspieszone wyznaczanie podziałów, równoległość, przycinanie przez ograniczenia).\n\nCatBoost (Prokhorenkova i in., 2018)11 - boosting drzew z natywną obsługą zmiennych kategorycznych oraz mechanizmami ograniczającymi target leakage i przeuczenie (m.in. uporządkowane statystyki docelowe, ordered boosting).\n\n7 R. E. Schapire (1990), The Strength of Weak Learnability, Machine Learning.8 Y. Freund, R. E. Schapire (1997), A Decision-Theoretic Generalization of On-Line Learning and an Application to Boosting, Journal of Computer and System Sciences.9 J. H. Friedman (2001), Greedy Function Approximation: A Gradient Boosting Machine, Annals of Statistics.10 T. Chen, C. Guestrin (2016), XGBoost: A Scalable Tree Boosting System, KDD.11 L. Prokhorenkova, G. Gusev, A. Vorobev, A. Dorogush, A. Gulin (2018), CatBoost: unbiased boosting with categorical features, NeurIPS.\n\n5.3.2 Koncepcja: model addytywny i uczenie „na błędach”\nW większości współczesnych wariantów boosting buduje model addytywny postaci:\n\\[\nF_M(x) = \\sum_{m=0}^M \\nu\\, f_m(x),\n\\]\ngdzie \\(f_m\\) to kolejne modele bazowe (często płytkie drzewa), a \\(\\nu\\in(0,1]\\) to współczynnik uczenia (learning rate, shrinkage). Sens jest następujący: zamiast uśredniać niezależne modele (jak w baggingu), boosting dokłada kolejne składniki tak, aby minimalizować stratę \\(\\mathcal{L}(y, F(x))\\). Małe \\(\\nu\\) spowalnia uczenie, ale zwykle poprawia uogólnianie (wymaga wtedy większej liczby iteracji).\n\n\n5.3.3 AdaBoost: boosting przez wagi obserwacji (klasyfikacja)\nW klasycznej wersji AdaBoost dla klasyfikacji binarnej \\(y_i\\in\\{-1,+1\\}\\) uczymy sekwencję klasyfikatorów \\(h_m\\). Algorytm utrzymuje rozkład wag \\(w_i^{(m)}\\) na obserwacjach, który w kolejnych iteracjach zwiększa znaczenie przykładów błędnie klasyfikowanych.\n\nInicjalizacja: \\(w_i^{(1)}=1/n\\).\nDla \\(m=1,\\dots,M\\):\n\nucz \\(h_m\\) na danych z wagami \\(w^{(m)}\\),\noblicz błąd ważony:\n\\[\n\\varepsilon_m = \\frac{\\sum_{i=1}^n w_i^{(m)}\\,\\mathbb{1}\\{h_m(x_i)\\neq y_i\\}}{\\sum_{i=1}^n w_i^{(m)}}.\n\\]\nwyznacz wagę klasyfikatora:\n\\[\n\\alpha_m = \\frac{1}{2}\\log\\frac{1-\\varepsilon_m}{\\varepsilon_m}.\n\\]\nzaktualizuj wagi obserwacji:\n\\[\nw_i^{(m+1)} \\propto w_i^{(m)}\\,\\exp\\big(-\\alpha_m\\,y_i\\,h_m(x_i)\\big),\n\\]\na następnie znormalizuj tak, aby \\(\\sum_i w_i^{(m+1)}=1\\).\n\n\nKońcowy klasyfikator ma postać ważonego głosowania:\n\\[\nH(x)=\\mathrm{sign}\\Big(\\sum_{m=1}^M \\alpha_m h_m(x)\\Big).\n\\]\nInterpretacyjnie AdaBoost minimalizuje wykładniczą stratę \\(\\sum_i \\exp(-y_i F(x_i))\\), a \\(\\alpha_m\\) rośnie, gdy \\(h_m\\) jest lepszy (ma mniejszy \\(\\varepsilon_m\\)). W praktyce jako \\(h_m\\) stosuje się często bardzo proste modele, np. decision stumps (drzewa głębokości 1), co wzmacnia efekt „uczenia na błędach”.\n\n\n5.3.4 Gradient Boosting: minimalizacja straty w przestrzeni funkcji\nGradient boosting uogólnia ideę „poprawiania błędów” na dowolną funkcję straty \\(\\mathcal{L}\\). Model addytywny jest budowany iteracyjnie:\n\\[\nF_m(x) = F_{m-1}(x) + \\nu\\, f_m(x).\n\\]\nW kroku \\(m\\) dopasowujemy \\(f_m\\) do tzw. pseudo-reszt (pseudo-residuals), które są ujemnym gradientem straty względem bieżących predykcji:\n\\[\nr_{im} = -\\left.\\frac{\\partial\\,\\mathcal{L}(y_i, F(x_i))}{\\partial F(x_i)}\\right|_{F=F_{m-1}}.\n\\]\nNastępnie uczymy model bazowy \\(f_m\\) (zwykle płytkie drzewo) tak, aby dobrze aproksymował \\(r_{im}\\) jako funkcję \\(x_i\\). Dla niektórych strat wykonuje się dodatkowo krok „liniowego przeskalowania” (wyszukanie \\(\\gamma_m\\)):\n\\[\n\\gamma_m = \\arg\\min_{\\gamma} \\sum_{i=1}^n \\mathcal{L}\\big(y_i, F_{m-1}(x_i)+\\gamma f_m(x_i)\\big),\n\\]\ni aktualizuje się \\(F_m(x)=F_{m-1}(x)+\\nu\\,\\gamma_m f_m(x)\\). W wielu implementacjach drzewiastych \\(\\gamma_m\\) jest w praktyce „wbudowane” w wartości w liściach.\n\n5.3.4.1 Algorytm (schemat) gradientowego boostingu drzew\n\nUstal inicjalny model \\(F_0(x)\\) (np. stałą minimalizującą stratę: średnią dla MSE, logit priory dla log-loss).\nDla \\(m=1,\\dots,M\\):\n\noblicz pseudo-reszty \\(r_{im}\\),\ndopasuj drzewo \\(f_m\\) do par \\((x_i, r_{im})\\),\n(opcjonalnie) wyznacz \\(\\gamma_m\\) minimalizujące stratę wzdłuż kierunku \\(f_m\\),\nzaktualizuj \\(F_m(x)=F_{m-1}(x)+\\nu\\,\\gamma_m f_m(x)\\).\n\n\nWażna intuicja: w regresji z MSE pseudo-reszty są po prostu resztami \\(r_{im}=y_i-F_{m-1}(x_i)\\), więc boosting faktycznie „doucza” kolejne drzewo na błędach poprzedniego modelu.\n\n\n\n5.3.5 XGBoost: regularizacja i optymalizacje implementacyjne\nXGBoost jest implementacją gradient boosting drzew, która dodaje silną regularizację oraz usprawnienia obliczeniowe. W typowej postaci minimalizuje się funkcję celu:\n\\[\n\\sum_{i=1}^n \\mathcal{L}(y_i, \\hat y_i) + \\sum_{m=1}^M \\Omega(f_m),\n\\]\ngdzie \\(\\Omega\\) karze złożoność drzew, np. przez liczbę liści i normę wag w liściach (w praktyce: reg_alpha, reg_lambda, gamma, itp.). Implementacyjnie istotne są m.in. efektywne wyznaczanie podziałów, obsługa braków (domyślny kierunek dla missing w węzłach), subsample i colsample_* (losowanie wierszy i cech) oraz możliwość wczesnego zatrzymania (early stopping) na zbiorze walidacyjnym.\n\n\n5.3.6 CatBoost: kategorie i kontrola wycieku informacji\nCatBoost jest boostowaniem drzew ukierunkowanym na dane z licznymi zmiennymi kategorycznymi. Zamiast prostego one-hot dla wielu poziomów stosuje się statystyki docelowe (target statistics), ale liczone w sposób, który ogranicza wyciek informacji: dla danej obserwacji statystyka jest wyliczana na podstawie „wcześniejszych” obserwacji w losowej permutacji (ordered target encoding), a proces uczenia używa wariantu ordered boosting. Dzięki temu CatBoost często daje bardzo dobre wyniki na danych mieszanych (numeryczne + kategorie) bez rozbudowanego preprocessingu.\n\n\n5.3.7 Najważniejsze hiperparametry i ryzyko przeuczenia\nBoosting ma wiele „pokręteł”, ale kilka z nich dominuje praktykę:\n\nn_estimators (\\(M\\)) – liczba iteracji/drzew. Większa liczba zwiększa potencjał dopasowania; bez kontroli może prowadzić do przeuczenia.\nlearning_rate (\\(\\nu\\)) – shrinkage. Mniejsze \\(\\nu\\) zwykle poprawia uogólnianie, ale wymaga większego \\(M\\). W praktyce parę \\((M,\\nu)\\) traktuje się łącznie.\nZłożoność drzew: max_depth, max_leaf_nodes, min_samples_leaf (lub ich odpowiedniki). Płytkie drzewa (np. max_depth 2–6) są standardem w boosting (inaczej model łatwo „przepala” dane).\nLosowanie obserwacji i cech: subsample (stochastic gradient boosting), colsample_bytree, colsample_bylevel (w XGBoost). Mniejsze wartości działają jak regularizacja i zmniejszają wariancję.\nRegularizacja w XGBoost: reg_lambda (L2), reg_alpha (L1), gamma (minimalna poprawa, by wykonać split), min_child_weight (minimalna „waga”/liczność w węźle). Te parametry ograniczają tworzenie zbyt szczegółowych podziałów.\nWczesne zatrzymanie (early stopping): w praktyce często monitoruje się błąd na zbiorze walidacyjnym i przerywa uczenie, gdy brak poprawy przez określoną liczbę iteracji. To jedna z najskuteczniejszych metod kontroli przeuczenia w boosting.\n\n\n\n5.3.8 Boosting vs bagging: porównanie i typowe „pułapki”\n\nCel: bagging redukuje wariancję (średnia wielu modeli), boosting redukuje bias (sekwencyjna korekta błędów).\nRównoległość: bagging łatwo zrównoleglić (drzewa niezależne); boosting jest z natury sekwencyjny (choć implementacje optymalizują wnętrze kroku).\nWrażliwość na szum/outliery: boosting może silniej „gonić” obserwacje odstające i szum etykiet, bo kolejne kroki koncentrują się na błędach. Bagging zwykle jest pod tym względem bardziej odporny.\nNajczęstsze źródła przeuczenia w boosting: zbyt duże \\(M\\), zbyt duże drzewa, zbyt duży \\(\\nu\\) oraz brak regularizacji/losowania.\n\n\n\n5.3.9 Uwaga praktyczna: metryki, ważności cech i interpretacja\nW boostingu (podobnie jak w RF) często raportuje się „ważność cech”, ale należy pamiętać o ograniczeniach: miary oparte o redukcję nieczystości mogą faworyzować cechy ciągłe lub o wielu poziomach, a silna korelacja cech rozmywa interpretację. W praktyce bardziej wiarygodne są podejścia oparte o permutacje lub metody wyjaśnialności (np. SHAP), choć są one obliczeniowo droższe.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Drzewa decyzyjne i zespoly modeli</span>"
    ]
  },
  {
    "objectID": "chapters/05-bayes.html",
    "href": "chapters/05-bayes.html",
    "title": "6  Klasyfikatory bayesowskie",
    "section": "",
    "text": "6.1 Teoria Bayesa i dwa podejscia estymacji",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Klasyfikatory bayesowskie</span>"
    ]
  },
  {
    "objectID": "chapters/05-bayes.html#teoria-bayesa-i-dwa-podejscia-estymacji",
    "href": "chapters/05-bayes.html#teoria-bayesa-i-dwa-podejscia-estymacji",
    "title": "6  Klasyfikatory bayesowskie",
    "section": "",
    "text": "Twierdzenie Bayesa oraz pojecia: rozklad aprioryczny i a posteriori.\nMLE maksymalizuje prawdopodobienstwo danych przy danym parametrze.\nMAP maksymalizuje iloczyn prawdopodobienstwa i rozkladu apriorycznego, co wprowadza regularizacje.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Klasyfikatory bayesowskie</span>"
    ]
  },
  {
    "objectID": "chapters/05-bayes.html#klasyfikator-mapml",
    "href": "chapters/05-bayes.html#klasyfikator-mapml",
    "title": "6  Klasyfikatory bayesowskie",
    "section": "6.2 Klasyfikator MAP/ML",
    "text": "6.2 Klasyfikator MAP/ML\n\nNaiwny Bayes w wariancie ML (parametry z estymacji wiarygodnosci).\nNaiwny Bayes w wariancie MAP (parametry z uwzglednieniem priory).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Klasyfikatory bayesowskie</span>"
    ]
  },
  {
    "objectID": "chapters/05-bayes.html#naive-bayes",
    "href": "chapters/05-bayes.html#naive-bayes",
    "title": "6  Klasyfikatory bayesowskie",
    "section": "6.3 Naive Bayes",
    "text": "6.3 Naive Bayes\n\nProbabilistyczny algorytm oparty na twierdzeniu Bayesa z zalozeniem niezaleznosci cech.\nSkuteczny w klasyfikacji tekstu, filtracji spamu i analizie uczuc.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Klasyfikatory bayesowskie</span>"
    ]
  },
  {
    "objectID": "chapters/06-knn.html",
    "href": "chapters/06-knn.html",
    "title": "7  k-NN",
    "section": "",
    "text": "k-NN to nieparametryczny algorytm, ktory klasyfikuje obserwacje na podstawie wiekszosci klasy wśród k najblizszych sasiadow.\nAlgorytm przechowuje dane treningowe i wykonuje obliczenia w momencie predykcji.\nWybor metryki odleglosci (Euclidesowa, Manhattan) i liczby sasiadow wplywa na dokladnosc i zlozonosc obliczeniowa.\nWady: wysoki koszt obliczen i wrazliwosc na skale cech.\nZalety: prostota i brak zalozen o rozkladzie danych.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>k-NN</span>"
    ]
  },
  {
    "objectID": "chapters/07-splajny-gam.html",
    "href": "chapters/07-splajny-gam.html",
    "title": "8  Modele addytywne i splajny",
    "section": "",
    "text": "8.1 Regresje sklejane (splajny)",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Modele addytywne i splajny</span>"
    ]
  },
  {
    "objectID": "chapters/07-splajny-gam.html#regresje-sklejane-splajny",
    "href": "chapters/07-splajny-gam.html#regresje-sklejane-splajny",
    "title": "8  Modele addytywne i splajny",
    "section": "",
    "text": "Splajny regresyjne to wielomianowa regresja kawalkowa; przedzialy rozdzielone sa wezlami, a wielomiany lacza sie gladko na wezlach.\nLiczba wezlow steruje elastycznoscia i ryzykiem przeuczenia.\nSplajny sluza do modelowania nieliniowosci, m.in. w FDA i GAM.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Modele addytywne i splajny</span>"
    ]
  },
  {
    "objectID": "chapters/07-splajny-gam.html#generalized-additive-models-gam",
    "href": "chapters/07-splajny-gam.html#generalized-additive-models-gam",
    "title": "8  Modele addytywne i splajny",
    "section": "8.2 Generalized Additive Models (GAM)",
    "text": "8.2 Generalized Additive Models (GAM)\n\nGAM sumuja wygładzone funkcje zmiennych (np. splajny), co pozwala modelowac zlozone relacje.\nW porownaniu z regresja liniowa GAM nie wymaga scisle liniowego zwiazku, ale jest bardziej zlozony.\nZalety: elastycznosc, interpretowalnosc, uchwycenie nieliniowosci.\nWady: wieksza zlozonosc obliczeniowa i wrazliwosc na parametry wygladzania.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Modele addytywne i splajny</span>"
    ]
  },
  {
    "objectID": "chapters/08-svm.html",
    "href": "chapters/08-svm.html",
    "title": "9  Metoda SVM",
    "section": "",
    "text": "Support Vector Machines (SVM) szukaja hiperpłaszczyzny maksymalizujacej margines miedzy klasami.\nKluczowe pojecia: hiperpłaszczyzna, wektory nosne, margines i jadra.\nTwardy margines wymaga braku bledow klasyfikacji, miekkim marginesem steruje parametr regularyzacji.\nTypy jader: liniowe, wielomianowe, RBF.\nZalety: skutecznosc w danych wysokowymiarowych i przy malych probach.\nWady: koniecznosc doboru parametrow i wrazliwosc na skale cech.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Metoda SVM</span>"
    ]
  },
  {
    "objectID": "chapters/09-dalsze-zagadnienia.html",
    "href": "chapters/09-dalsze-zagadnienia.html",
    "title": "10  Inne metody",
    "section": "",
    "text": "10.1 Reinforcement learning",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Inne metody</span>"
    ]
  },
  {
    "objectID": "chapters/09-dalsze-zagadnienia.html#reinforcement-learning",
    "href": "chapters/09-dalsze-zagadnienia.html#reinforcement-learning",
    "title": "10  Inne metody",
    "section": "",
    "text": "Uczenie ze wzmocnieniem: agent uczy sie przez interakcje ze srodowiskiem, maksymalizujac sume przyszlych nagrod.\nElementy: agent, srodowisko, stan, akcja, nagroda, polityka i funkcja wartosci; modelowane jako zadanie Markowa.\nSzczegolowe algorytmy (Q-learning, SARSA, aktor-krytyk) omawiane sa w innym kursie.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Inne metody</span>"
    ]
  },
  {
    "objectID": "chapters/09-dalsze-zagadnienia.html#modele-koszykowe-market-basket-analysis",
    "href": "chapters/09-dalsze-zagadnienia.html#modele-koszykowe-market-basket-analysis",
    "title": "10  Inne metody",
    "section": "10.2 Modele koszykowe (market basket analysis)",
    "text": "10.2 Modele koszykowe (market basket analysis)\n\nAnaliza koszykowa wykrywa wzorce zakupowe i produkty kupowane razem.\nReguly asocjacyjne “jezeli-to”: antecedent i consequent.\nMiary: support, confidence i lift.\nAlgorytmy: Apriori, FP-Growth.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Inne metody</span>"
    ]
  },
  {
    "objectID": "chapters/09-dalsze-zagadnienia.html#wykrywanie-anomalii-ponowne",
    "href": "chapters/09-dalsze-zagadnienia.html#wykrywanie-anomalii-ponowne",
    "title": "10  Inne metody",
    "section": "10.3 Wykrywanie anomalii (ponowne)",
    "text": "10.3 Wykrywanie anomalii (ponowne)\n\nPrzypomnienie definicji anomalii w kontekście oszustw, awarii i bezpieczenstwa.\nTypy anomalii: punktowe, kontekstowe, zbiorowe.\nMetody detekcji omawiane sa szczegolowo w innych kursach.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Inne metody</span>"
    ]
  },
  {
    "objectID": "chapters/10-podsumowanie.html",
    "href": "chapters/10-podsumowanie.html",
    "title": "11  Podsumowanie",
    "section": "",
    "text": "W tym wykladzie przeszlismy od rozroznienia eksploracji danych i uczenia maszynowego, przez przygotowanie danych, do przegladu algorytmow uczenia nadzorowanego. Kolejnosc tematow wspiera stopniowe budowanie wiedzy: definicje, modele liniowe i dyskryminacyjne, drzewa i zespoly, klasyfikatory bayesowskie, k-NN, modele addytywne, SVM, a nastepnie krotkie wprowadzenie do reinforcement learning, analiz koszykowych i detekcji anomalii.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Podsumowanie</span>"
    ]
  }
]