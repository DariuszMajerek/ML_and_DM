[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Wstep",
    "section": "",
    "text": "Cel i charakter kursu\nKsiążka jest przygotowana dla studentów kierunków matematyka oraz inżynieria i analiza danych.\nCelem niniejszego kursu jest stworzenie studentom możliwości zapoznania się z metodami eksploracji danych oraz klasycznego uczenia maszynowego, ze szczególnym naciskiem na zrozumienie ich założeń, mechanizmów działania oraz miejsca w procesie analizy danych. Kurs ma umożliwić nie tylko poznanie konkretnych algorytmów, lecz przede wszystkim wykształcenie świadomości metodologicznej, która pozwala na świadomy dobór narzędzi analitycznych do charakteru problemu i struktury danych. Eksploracja danych i uczenie maszynowe są w ramach kursu traktowane jako spójny zbiór technik służących do wydobywania wiedzy z danych empirycznych oraz budowy modeli predykcyjnych i decyzyjnych.\nKurs jest osadzony w kontekście analizy danych tablicowych oraz klasycznych problemów klasyfikacji i regresji. Studenci będą mogli zapoznać się z metodami, które odgrywają fundamentalną rolę w praktyce analitycznej i stanowią podstawę wielu współczesnych rozwiązań stosowanych w analizie danych. Omawiane techniki, takie jak regresja logistyczna, analiza dyskryminacyjna, drzewa decyzyjne, metody zespołowe, klasyfikatory bayesowskie, algorytm k najbliższych sąsiadów, modele addytywne oraz maszyny wektorów nośnych, są prezentowane jako elementy spójnej tradycji metod eksploracyjnych i predykcyjnych. Kurs celowo pomija zagadnienia wdrażania modeli oraz głębokich sieci neuronowych, które są realizowane w ramach innych przedmiotów, aby zachować koncentrację na klasycznych fundamentach uczenia maszynowego.\nW trakcie kursu studenci będą mogli nauczyć się, że eksploracja danych nie ogranicza się wyłącznie do wizualizacji czy statystyk opisowych, lecz obejmuje również budowę modeli umożliwiających identyfikację wzorców, segmentację obserwacji oraz podejmowanie decyzji na podstawie danych. Jednocześnie będą mogli zrozumieć, że uczenie maszynowe nie sprowadza się jedynie do stosowania algorytmów optymalizacyjnych, lecz stanowi formalny aparat pozwalający na automatyczne uczenie się zależności oraz ocenę jakości predykcji. Takie ujęcie ma umożliwić lepsze zrozumienie, dlaczego te same metody znajdują zastosowanie zarówno w analizie eksploracyjnej, jak i w zadaniach stricte predykcyjnych.\nIstotnym elementem kursu jest umożliwienie studentom zapoznania się z etapem przygotowania danych, który w praktyce analizy danych ma kluczowe znaczenie dla jakości modeli. Studenci będą mogli nauczyć się zasad czyszczenia danych, obsługi braków, kodowania zmiennych, standaryzacji oraz wstępnej analizy rozkładów i zależności. Etap ten jest przedstawiany nie jako czynność czysto techniczna, lecz jako integralna część procesu eksploracji danych, wymagająca podejmowania świadomych decyzji analitycznych i uwzględniania kontekstu problemu. Takie podejście ma umożliwić lepsze przygotowanie do pracy z rzeczywistymi danymi empirycznymi.\nW dalszej części kursu studenci będą mogli zapoznać się z metodami uczenia nadzorowanego w sposób systematyczny, z naciskiem na ich interpretację, założenia statystyczne oraz różnice pomiędzy podejściami generatywnymi i dyskryminacyjnymi. Kurs ma umożliwić zrozumienie, w jakich sytuacjach zasadne jest stosowanie prostych modeli liniowych, a kiedy warto sięgnąć po metody nieliniowe lub zespołowe. Szczególna uwaga poświęcona jest zagadnieniu interpretowalności modeli oraz kompromisowi pomiędzy złożonością modelu a jego zdolnością do generalizacji, co stanowi jeden z kluczowych problemów w analizie danych stosowanej.\nChoć kurs koncentruje się na klasycznych metodach, jego celem jest również umożliwienie studentom zrozumienia szerszego kontekstu uczenia maszynowego. Krótkie wprowadzenia do reinforcement learning, modeli koszykowych oraz wykrywania anomalii mają pozwolić na zapoznanie się z podstawowymi zasadami działania innych paradygmatów uczenia. Zagadnienia te są omawiane na poziomie koncepcyjnym, tak aby studenci mogli rozpoznać, jakie typy problemów wymagają odmiennych podejść oraz jakie są granice stosowalności klasycznych metod eksploracji danych.\nOstatecznie kurs ma umożliwić studentom wykształcenie umiejętności krytycznego myślenia o danych i modelach. Po jego ukończeniu studenci będą mogli nie tylko znać podstawowe algorytmy eksploracji danych i uczenia maszynowego, lecz także rozumieć ich genezę, założenia oraz konsekwencje ich zastosowania. Kurs stanowi fundament dla dalszych zajęć z zakresu zaawansowanego uczenia maszynowego, analizy dużych zbiorów danych oraz systemów opartych na sztucznej inteligencji, dostarczając solidnych podstaw teoretycznych i metodologicznych.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Wstep</span>"
    ]
  },
  {
    "objectID": "index.html#struktura-ksiazki",
    "href": "index.html#struktura-ksiazki",
    "title": "Wstep",
    "section": "Struktura ksiazki",
    "text": "Struktura ksiazki\n\nWprowadzenie i historia\nPrzygotowanie i czyszczenie danych\nModele liniowe i dyskryminacyjne\nDrzewa decyzyjne i zespoly modeli\nKlasyfikatory bayesowskie\nk-NN\nModele addytywne\nSVM\nInne metody",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Wstep</span>"
    ]
  },
  {
    "objectID": "chapters/01-wprowadzenie.html",
    "href": "chapters/01-wprowadzenie.html",
    "title": "2  Wprowadzenie i historia",
    "section": "",
    "text": "2.1 Historia eksploracji danych i uczenia maszynowego\nHistoria eksploracji danych oraz uczenia maszynowego jest ściśle związana z rozwojem statystyki, informatyki i sztucznej inteligencji, jednak obie dziedziny wywodzą się z odmiennych tradycji badawczych. Eksploracja danych (data mining) rozwijała się przede wszystkim na gruncie statystyki matematycznej, analizy danych oraz teorii baz danych i była odpowiedzią na rosnącą dostępność dużych zbiorów danych empirycznych. Jej głównym celem było odkrywanie wzorców, struktur i zależności, które nie były bezpośrednio widoczne przy użyciu klasycznych narzędzi analizy. Uczenie maszynowe (machine learning) natomiast wyrosło z badań nad sztuczną inteligencją i algorytmami uczącymi się, kładąc nacisk na formalne modele predykcyjne, automatyczne dostosowywanie się do danych oraz własności generalizacji. W kontekście niniejszego wykładu oba podejścia spotykają się w obszarze metod klasyfikacyjnych i regresyjnych, które jednocześnie służą eksploracji danych i budowie modeli uczących się.\nPierwsze istotne fundamenty eksploracji danych pojawiły się już na początku XX wieku wraz z rozwojem statystyki wielowymiarowej. Szczególne znaczenie miały badania nad klasyfikacją i redukcją wymiaru, prowadzone w ramach wnioskowania statystycznego. Przełomową rolę odegrała praca Ronalda A. Fishera z 1936 roku, w której zaproponowano liniową analizę dyskryminacyjną jako metodę rozróżniania klas na podstawie kombinacji liniowych zmiennych. Choć Fisher nie posługiwał się pojęciem uczenia maszynowego, jego metoda stanowi bezpośredni pierwowzór współczesnych modeli dyskryminacyjnych, takich jak LDA, QDA oraz ich późniejsze uogólnienia. W tym okresie analiza danych była traktowana przede wszystkim jako narzędzie inferencyjne, służące do testowania hipotez i opisu struktury populacji.\nRównolegle do rozwoju statystyki, w połowie XX wieku zaczęły kształtować się idee sztucznej inteligencji. To właśnie na tym gruncie narodziło się uczenie maszynowe jako odrębna dziedzina. Jednym z najczęściej cytowanych momentów symbolicznych jest publikacja Arthura Samuela z 1959 roku, w której autor opisał algorytm uczący się gry w warcaby poprzez doświadczenie. Samuel zaproponował definicję uczenia maszynowego jako procesu, w którym system poprawia swoje działanie na podstawie obserwacji danych, co do dziś pozostaje centralnym elementem tej dziedziny. W tym samym czasie rozwijano probabilistyczne metody klasyfikacji, oparte na twierdzeniu Bayesa, które umożliwiały formalne łączenie danych empirycznych z wiedzą a priori. Klasyfikatory ML, MAP oraz naiwny klasyfikator Bayesa znalazły szerokie zastosowanie zarówno w eksploracji danych, jak i w uczeniu maszynowym.\nLata siedemdziesiąte i osiemdziesiąte XX wieku przyniosły intensywny rozwój metod klasyfikacyjnych, które do dziś stanowią podstawę analizy danych. Szczególnie istotne okazały się drzewa decyzyjne, ugruntowane w monografii Breimana, Friedmana, Olshena i Stone’a z 1984 roku. Metody CART wprowadziły ideę rekurencyjnego podziału przestrzeni cech oraz kryteria optymalizacji oparte na nieczystości węzłów, co pozwoliło na budowę modeli jednocześnie skutecznych i interpretowalnych. Drzewa decyzyjne szybko stały się jednym z podstawowych narzędzi eksploracji danych, zwłaszcza w analizie zbiorów o złożonej strukturze i mieszanych typach zmiennych.\nW tym samym okresie rozwijano metody oparte na podobieństwie obserwacji, w szczególności algorytm k najbliższych sąsiadów. Praca Covera i Harta z 1967 roku formalnie opisała własności klasyfikatora k-NN, który nie wymaga jawnej estymacji parametrów modelu, a decyzje podejmuje na podstawie lokalnej struktury danych. Metody te odegrały ważną rolę w eksploracji danych, ponieważ umożliwiały analizę bez silnych założeń rozkładowych i stanowiły punkt odniesienia dla późniejszych, bardziej złożonych algorytmów.\nIstotnym krokiem w stronę formalizacji uczenia maszynowego było wprowadzenie maszyn wektorów nośnych w latach dziewięćdziesiątych. Praca Cortes i Vapnika z 1995 roku zaproponowała model klasyfikacyjny oparty na maksymalizacji marginesu separacji klas, osadzony w aparacie optymalizacji wypukłej. Dzięki zastosowaniu funkcji jądrowych SVM umożliwiły efektywną klasyfikację danych nieliniowo separowalnych w przestrzeniach o bardzo wysokim wymiarze. Metoda ta stała się jednym z najlepiej ugruntowanych teoretycznie algorytmów uczenia maszynowego i do dziś pełni istotną rolę w analizie danych.\nKolejnym przełomem w eksploracji danych było pojawienie się metod zespołowych. Leo Breiman wprowadził ideę baggingu w 1996 roku, pokazując, że agregacja wielu niestabilnych modeli może znacząco poprawić jakość predykcji. Rozwinięciem tej koncepcji były lasy losowe, które połączyły bagging z losowym wyborem cech, tworząc jeden z najbardziej uniwersalnych algorytmów analizy danych. Równolegle rozwijał się nurt boostingu, zapoczątkowany przez Freund i Schapire, który polegał na sekwencyjnym uczeniu słabych klasyfikatorów i ich adaptacyjnym ważeniu. Współczesne algorytmy, takie jak Gradient Boosting, XGBoost czy CatBoost, stanowią bezpośrednie rozwinięcie tych idei i są dziś standardem w analizie danych tablicowych.\nNa styku statystyki i uczenia maszynowego rozwijały się także modele addytywne. Prace Hastiego i Tibshiraniego z lat osiemdziesiątych wprowadziły uogólnione modele addytywne, które umożliwiają elastyczne modelowanie zależności nieliniowych przy zachowaniu interpretowalnej struktury. Modele te stanowią naturalne rozszerzenie klasycznej regresji i dobrze ilustrują ewolucję metod eksploracji danych w kierunku większej elastyczności bez rezygnacji z kontroli nad złożonością modelu.\nZ perspektywy dydaktycznej historia eksploracji danych i uczenia maszynowego pokazuje, że większość współczesnych algorytmów opiera się na ideach rozwijanych od dziesięcioleci. Metody omawiane w dalszej części wykładu nie są oderwanymi narzędziami, lecz elementami spójnej ewolucji pojęć takich jak klasyfikacja, estymacja, generalizacja i kompromis między złożonością a interpretowalnością. Zrozumienie tego kontekstu historycznego pozwala lepiej interpretować zachowanie modeli, ich założenia oraz ograniczenia w praktycznej analizie danych.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Wprowadzenie i historia</span>"
    ]
  },
  {
    "objectID": "chapters/02-przygotowanie-danych.html",
    "href": "chapters/02-przygotowanie-danych.html",
    "title": "3  Przygotowanie i czyszczenie danych",
    "section": "",
    "text": "3.1 Import i czyszczenie danych\nPrzygotowanie danych stanowi jeden z kluczowych etapów procesu eksploracji danych i uczenia maszynowego. Jakość danych wejściowych w dużej mierze determinuje jakość modeli, niezależnie od stopnia ich złożoności. W praktyce analizy danych etap ten obejmuje zarówno wstępną eksplorację danych, jak i ich transformację, czyszczenie oraz konstrukcję cech, które będą następnie wykorzystywane przez algorytmy uczenia nadzorowanego. W niniejszym rozdziale przygotowanie danych jest traktowane jako proces analityczny, a nie jedynie techniczny etap przetwarzania.\nImport danych jest pierwszym momentem, w którym można świadomie zminimalizować późniejsze problemy jakościowe. W praktyce ważne jest nie tylko „wczytanie pliku”, ale kontrola takich elementów jak kodowanie znaków, separator, typy danych, format dat, niestandardowe znaczniki braków oraz interpretacja wartości logicznych. pandas daje w tym zakresie bardzo duże możliwości, a poprawna konfiguracja importu bywa istotniejsza niż późniejsze „naprawy” wykonywane ad hoc.\nNajczęściej spotykanym formatem jest CSV, który bywa myląco prosty: różne pliki mogą używać separatora , lub ;, różnych separatorów dziesiętnych (kropka/przecinek), mogą zawierać znaki narodowe (tu: polskie nazwy miast), a braki mogą być kodowane jako puste pole, NA, N/A, null, -999 itd. Dlatego przy imporcie CSV bardzo często warto jawnie ustawić: sep, encoding, na_values, parse_dates, a w razie potrzeby także dtype. Przykładowo1:\nKod\nimport pandas as pd\n\ndf = pd.read_csv(\n    \"data.csv\",\n    encoding=\"utf-8\",\n    na_values=[\"\", \"NA\", \"N/A\", \"null\", \"None\"],\n    parse_dates=[\"signup_date\", \"last_purchase_date\"]\n)\ndf.head()\n\n\n\n\n\n\n\n\n\ncustomer_id\nsignup_date\nlast_purchase_date\nage\nincome_eur\ncity\nsegment\ndevice_type\nvisits_last30\navg_basket_eur\nreturns_last90\nhas_promo\nsatisfaction_1_5\nchurned\nnotes\n\n\n\n\n0\n100000\n2022-04-08\n2022-10-19\nNaN\n72955.0\nWarszawa\nB2B\ndesktop\n8.0\n39.88\n1.0\nFalse\n4.0\n0\nklient powracający\n\n\n1\n100001\n2024-04-27\n2024-05-13\n52.0\n24863.0\nKraków\nB2C\nmobile\n12.0\n67.80\n1.0\nFalse\n3.0\n0\nbrak uwag\n\n\n2\n100002\n2023-12-18\n2024-04-05\nNaN\n25333.0\nWrocław\nB2C\ndesktop\n6.0\n27.04\n1.0\nTrue\n4.0\n0\nreklamacja\n\n\n3\n100003\n2023-04-26\n2023-10-01\n16.0\nNaN\nŁódź\nB2B\nmobile\n6.0\n149.03\n1.0\nFalse\n3.0\n1\nbrak uwag\n\n\n4\n100004\n2023-04-20\n2024-04-07\n42.0\n27954.0\nWarszawa\nB2C\ndesktop\n7.0\n49.73\n1.0\nTrue\n3.0\n0\nNaN\nW tym przykładzie na_values powoduje, że zarówno puste pola, jak i tekstowe znaczniki braków zostaną zamienione na NaN, a parse_dates zadba o automatyczną konwersję wskazanych kolumn do typu daty. To jest szczególnie ważne, bo daty wczytane jako tekst utrudniają analizę sezonowości, czasu od rejestracji, czy prostych agregacji po miesiącach.\nW przypadku danych w Excelu (.xlsx) import odbywa się przez read_excel. W praktyce warto pamiętać, że Excel często zawiera dodatkowe arkusze, nagłówki „opisowe” nad tabelą, albo mieszane typy w kolumnach. Gdy dane są w konkretnym arkuszu i zaczynają się od konkretnego wiersza, przydają się parametry sheet_name oraz skiprows. Dla dołączonego pliku:\nKod\ndf = pd.read_excel(\n    \"data.xlsx\",\n    sheet_name=0,\n    na_values=[\"\", \"NA\", \"N/A\", \"null\", \"None\"],\n    parse_dates=[\"signup_date\", \"last_purchase_date\"]\n)\n\ndf.head()\n\n\n\n\n\n\n\n\n\ncustomer_id\nsignup_date\nlast_purchase_date\nage\nincome_eur\ncity\nsegment\ndevice_type\nvisits_last30\navg_basket_eur\nreturns_last90\nhas_promo\nsatisfaction_1_5\nchurned\nnotes\n\n\n\n\n0\n100000\n2022-04-08\n2022-10-19\nNaN\n72955.0\nWarszawa\nB2B\ndesktop\n8\n39.88\n1\nFalse\n4.0\n0\nklient powracający\n\n\n1\n100001\n2024-04-27\n2024-05-13\n52.0\n24863.0\nKraków\nB2C\nmobile\n12\n67.80\n1\nFalse\n3.0\n0\nbrak uwag\n\n\n2\n100002\n2023-12-18\n2024-04-05\nNaN\n25333.0\nWrocław\nB2C\ndesktop\n6\n27.04\n1\nTrue\n4.0\n0\nreklamacja\n\n\n3\n100003\n2023-04-26\n2023-10-01\n16.0\nNaN\nŁódź\nB2B\nmobile\n6\n149.03\n1\nFalse\n3.0\n1\nbrak uwag\n\n\n4\n100004\n2023-04-20\n2024-04-07\n42.0\n27954.0\nWarszawa\nB2C\ndesktop\n7\n49.73\n1\nTrue\n3.0\n0\nNaN\nKlasyczny format JSON (.json) przechowuje dane jako jedną spójną strukturę, najczęściej listę obiektów (rekordów). Taki zapis jest szczególnie często spotykany w interfejsach API, plikach konfiguracyjnych oraz w wymianie danych pomiędzy systemami informatycznymi. Z punktu widzenia analizy danych format ten jest bardziej „opisowy” i czytelny dla człowieka, ale jednocześnie wymaga załadowania całej struktury do pamięci.\nW przygotowanym pliku data.json dane zapisane są jako lista rekordów, gdzie każdy rekord odpowiada jednej obserwacji, a klucze obiektów odpowiadają nazwom zmiennych. Taki układ bardzo naturalnie mapuje się na strukturę ramki danych w pandas. Podstawowy import danych JSON do pandas odbywa się za pomocą funkcji read_json.\nKod\ndf = pd.read_json(\"data.json\")\ndf.head()\n\n\n\n\n\n\n\n\n\ncustomer_id\nsignup_date\nlast_purchase_date\nage\nincome_eur\ncity\nsegment\ndevice_type\nvisits_last30\navg_basket_eur\nreturns_last90\nhas_promo\nsatisfaction_1_5\nchurned\nnotes\n\n\n\n\n0\n100000\n1649376000000\n1666137600000\nNaN\n72955.0\nWarszawa\nB2B\ndesktop\n8\n39.88\n1\nFalse\n4.0\n0\nklient powracający\n\n\n1\n100001\n1714176000000\n1715558400000\n52.0\n24863.0\nKraków\nB2C\nmobile\n12\n67.80\n1\nFalse\n3.0\n0\nbrak uwag\n\n\n2\n100002\n1702857600000\n1712275200000\nNaN\n25333.0\nWrocław\nB2C\ndesktop\n6\n27.04\n1\nTrue\n4.0\n0\nreklamacja\n\n\n3\n100003\n1682467200000\n1696118400000\n16.0\nNaN\nŁódź\nB2B\nmobile\n6\n149.03\n1\nFalse\n3.0\n1\nbrak uwag\n\n\n4\n100004\n1681948800000\n1712448000000\n42.0\n27954.0\nWarszawa\nB2C\ndesktop\n7\n49.73\n1\nTrue\n3.0\n0\nNA\nPo wczytaniu danych pandas automatycznie spróbuje rozpoznać typy zmiennych, jednak – podobnie jak w przypadku CSV – nie zawsze zrobi to zgodnie z oczekiwaniami analityka. W szczególności kolumny datowe są często wczytywane jako typ object, dlatego dobrą praktyką jest ich jawna konwersja:\nKod\ndf[\"signup_date\"] = pd.to_datetime(df[\"signup_date\"], errors=\"coerce\")\ndf[\"last_purchase_date\"] = pd.to_datetime(df[\"last_purchase_date\"], errors=\"coerce\")\ndf.head()\n\n\n\n\n\n\n\n\n\ncustomer_id\nsignup_date\nlast_purchase_date\nage\nincome_eur\ncity\nsegment\ndevice_type\nvisits_last30\navg_basket_eur\nreturns_last90\nhas_promo\nsatisfaction_1_5\nchurned\nnotes\n\n\n\n\n0\n100000\n1970-01-01 00:27:29.376000\n1970-01-01 00:27:46.137600\nNaN\n72955.0\nWarszawa\nB2B\ndesktop\n8\n39.88\n1\nFalse\n4.0\n0\nklient powracający\n\n\n1\n100001\n1970-01-01 00:28:34.176000\n1970-01-01 00:28:35.558400\n52.0\n24863.0\nKraków\nB2C\nmobile\n12\n67.80\n1\nFalse\n3.0\n0\nbrak uwag\n\n\n2\n100002\n1970-01-01 00:28:22.857600\n1970-01-01 00:28:32.275200\nNaN\n25333.0\nWrocław\nB2C\ndesktop\n6\n27.04\n1\nTrue\n4.0\n0\nreklamacja\n\n\n3\n100003\n1970-01-01 00:28:02.467200\n1970-01-01 00:28:16.118400\n16.0\nNaN\nŁódź\nB2B\nmobile\n6\n149.03\n1\nFalse\n3.0\n1\nbrak uwag\n\n\n4\n100004\n1970-01-01 00:28:01.948800\n1970-01-01 00:28:32.448000\n42.0\n27954.0\nWarszawa\nB2C\ndesktop\n7\n49.73\n1\nTrue\n3.0\n0\nNA\nParametr errors=\"coerce\" powoduje, że ewentualne niepoprawne formaty dat zostaną zamienione na NaT, co jest bezpieczniejsze niż przerwanie importu błędem. W kontekście eksploracji danych takie zachowanie pozwala szybko zidentyfikować problemy jakościowe bez utraty całego zbioru. Warto podkreślić, że format JSON nie posiada natywnego pojęcia braków danych w sensie znanym z analizy statystycznej. Braki mogą być reprezentowane jako null, brak klucza lub wartość tekstowa (np. NA). pandas zamienia null na NaN, ale nie rozpoznaje automatycznie tekstowych znaczników braków. Dlatego po imporcie zalecane jest jawne czyszczenie takich wartości:\nKod\ndf.replace([\"NA\", \"N/A\", \"\"], pd.NA, inplace=True)\ndf.head()\n\n\n\n\n\n\n\n\n\ncustomer_id\nsignup_date\nlast_purchase_date\nage\nincome_eur\ncity\nsegment\ndevice_type\nvisits_last30\navg_basket_eur\nreturns_last90\nhas_promo\nsatisfaction_1_5\nchurned\nnotes\n\n\n\n\n0\n100000\n1970-01-01 00:27:29.376000\n1970-01-01 00:27:46.137600\nNaN\n72955.0\nWarszawa\nB2B\ndesktop\n8\n39.88\n1\nFalse\n4.0\n0\nklient powracający\n\n\n1\n100001\n1970-01-01 00:28:34.176000\n1970-01-01 00:28:35.558400\n52.0\n24863.0\nKraków\nB2C\nmobile\n12\n67.80\n1\nFalse\n3.0\n0\nbrak uwag\n\n\n2\n100002\n1970-01-01 00:28:22.857600\n1970-01-01 00:28:32.275200\nNaN\n25333.0\nWrocław\nB2C\ndesktop\n6\n27.04\n1\nTrue\n4.0\n0\nreklamacja\n\n\n3\n100003\n1970-01-01 00:28:02.467200\n1970-01-01 00:28:16.118400\n16.0\nNaN\nŁódź\nB2B\nmobile\n6\n149.03\n1\nFalse\n3.0\n1\nbrak uwag\n\n\n4\n100004\n1970-01-01 00:28:01.948800\n1970-01-01 00:28:32.448000\n42.0\n27954.0\nWarszawa\nB2C\ndesktop\n7\n49.73\n1\nTrue\n3.0\n0\n&lt;NA&gt;\nW porównaniu do CSV, format JSON lepiej zachowuje strukturę danych (np. brak problemów z separatorami czy kodowaniem znaków), ale gorzej skaluje się dla bardzo dużych zbiorów. Z tego względu klasyczny JSON jest szczególnie przydatny w dydaktyce oraz w pracy z danymi średniej wielkości, gdzie czytelność i jednoznaczność struktury są ważniejsze niż wydajność. Z perspektywy dalszych etapów kursu istotne jest, aby rozumieć, że import danych nie jest neutralnym technicznie krokiem, lecz pierwszym momentem, w którym podejmowane są decyzje wpływające na całą analizę. Różnice pomiędzy CSV, JSONL i JSON przekładają się nie tylko na sposób wczytania danych, ale także na późniejsze możliwości ich walidacji, przetwarzania i skalowania.￼\nW praktyce import często wymaga kontroli typów. Jeśli np. identyfikator klienta ma wyglądać jak liczba, ale nie wolno dopuścić do utraty wiodących zer (częsty przypadek dla kodów), należy wymusić typ tekstowy przez dtype={\"customer_id\": \"string\"}. W tym konkretnym zbiorze customer_id jest liczbowy, ale w realnych danych biznesowych to częsty problem. Analogicznie, gdy w jednej kolumnie występują liczby i tekst (np. „brak”), pandas może ustawić typ object, a to utrudni obliczenia – lepiej wczytać z na_values i później rzutować typy jawnie.\nIstotne są też parametry wpływające na wydajność i kontrolę pamięci. Dla dużych plików CSV warto rozważyć usecols (czytać tylko potrzebne kolumny), chunksize (czytanie porcjami) oraz low_memory=False (mniej błędnych inferencji typów kosztem RAM). Przykład importu porcjami:\nKod\nchunks = pd.read_csv(\"data.csv\", chunksize=50_000)\nfor chunk in chunks:\n    # walidacje / czyszczenie / zapis częściowy\n    pass\nNa końcu, dobrą praktyką po imporcie jest natychmiastowa „kontrola jakości importu”: df.info(), df.isna().sum(), sprawdzenie liczby unikatów w kategoriach oraz szybkie oględziny podejrzanych wartości (np. wiek 120). To pozwala wcześnie odróżnić problemy wynikające z danych od problemów wynikających z błędnego importu.\nW praktyce analizy danych bardzo często spotyka się zbiory danych, których nazwy kolumn są niewygodne lub wręcz problematyczne z punktu widzenia języka Python oraz bibliotek analitycznych. Dotyczy to w szczególności nazw zawierających spacje, znaki specjalne, polskie znaki diakrytyczne, rozpoczynających się od cyfr, a także nazw bardzo długich lub opisowych. Choć pandas technicznie dopuszcza niemal dowolne nazwy kolumn, ich nieprzemyślane użycie prowadzi do błędów, nieczytelnego kodu oraz problemów w dalszych etapach analizy i modelowania. Problem ten ujawnia się szczególnie wyraźnie wtedy, gdy użytkownik próbuje korzystać z notacji kropkowej (df.column_name), budować formuły modelowe, pisać potoki przetwarzania lub eksportować dane do innych narzędzi analitycznych. Nazwy kolumn zawierające spacje, znaki -, %, (), #, rozpoczynające się od cyfr lub zawierające znaki typowe dla języków innych niż angielski (np. ś, ć, ń) nie mogą być używane jako poprawne identyfikatory w Pythonie. W efekcie kod staje się mniej czytelny i bardziej podatny na błędy.\nRozważmy przykładowy zbiór danych, w którym nazwy kolumn zostały nadane w sposób typowy dla arkuszy Excel lub raportów biznesowych:\nKod\ndf2 = pd.DataFrame({\n    \"Customer ID\": [1, 2, 3],\n    \"2023 Revenue (€)\": [12000, 34000, 18000],\n    \"Avg Basket Value\": [45.5, 78.2, 33.1],\n    \"% Return\": [0.02, 0.01, 0.05],\n    \"Very long column name describing customer behaviour in detail\": [1, 0, 1]\n})\ndf2\n\n\n\n\n\n\n\n\n\nCustomer ID\n2023 Revenue (€)\nAvg Basket Value\n% Return\nVery long column name describing customer behaviour in detail\n\n\n\n\n0\n1\n12000\n45.5\n0.02\n1\n\n\n1\n2\n34000\n78.2\n0.01\n0\n\n\n2\n3\n18000\n33.1\n0.05\n1\nZ punktu widzenia pandas taki zbiór danych jest poprawny, jednak już próba użycia notacji kropkowej zakończy się błędem:\nKod\ndf2.2023 Revenue (€) # błąd składni\n\n\n\n  Cell In[7], line 1\n    df2.2023 Revenue (€) # błąd składni\n                      ^\nSyntaxError: invalid character '€' (U+20AC)\nKażdorazowo konieczne byłoby odwoływanie się do kolumn przez nawiasy i łańcuchy znaków, co znacząco obniża czytelność kodu:\nKod\ndf2[\"2023 Revenue (€)\"].mean()\n\n\nnp.float64(21333.333333333332)\nDlatego dobrą praktyką w analizie danych jest normalizacja nazw kolumn bezpośrednio po imporcie danych. Najczęściej stosowana konwencja obejmuje użycie wyłącznie małych liter, znaków ASCII, podkreśleń zamiast spacji oraz nazw rozpoczynających się literą. Taki styl jest zgodny z konwencją snake_case powszechnie stosowaną w Pythonie. Podstawowa korekta nazw kolumn może zostać wykonana w kilku krokach. Najpierw usuwa się nadmiarowe spacje, zamienia litery na małe, a spacje na podkreślenia:\nKod\ndf2.columns = (\n    df2.columns\n    .str.strip()\n    .str.lower()\n    .str.replace(r\"\\s+\", \"_\", regex=True)\n)\nJednak w praktyce to zwykle nie wystarcza, ponieważ w nazwach mogą występować znaki specjalne, symbole walut, procenty czy nawiasy. W takim przypadku warto usunąć wszystkie znaki niedozwolone i pozostawić jedynie litery, cyfry i podkreślenia:\nKod\ndf2.columns = df2.columns.str.replace(r\"[^a-zA-Z0-9_]\", \"\", regex=True)\n\n# opcjonalnie: porządkujemy podkreślenia (np. po usunięciu znaków specjalnych)\ndf2.columns = (\n    df2.columns\n    .str.replace(r\"_+\", \"_\", regex=True)\n    .str.strip(\"_\")\n)\ndf2.columns\n\n\nIndex(['customer_id', '2023_revenue', 'avg_basket_value', 'return',\n       'very_long_column_name_describing_customer_behaviour_in_detail'],\n      dtype='object')\nWarto zauważyć, że po czyszczeniu może powstać nazwa będąca słowem kluczowym Pythona (np. return). Jeśli zależy nam na możliwości użycia notacji kropkowej, warto takie przypadki automatycznie zmienić, np. przez dodanie sufiksu:\nKod\nimport keyword\n\ndf2.columns = [\n    f\"{c}_var\" if keyword.iskeyword(c) else c\n    for c in df2.columns\n]\nWciąż jednak pozostaje problem nazw rozpoczynających się od cyfr. W Pythonie identyfikator nie może zaczynać się od liczby, dlatego zaleca się automatyczne dodanie prefiksu, na przykład x_:\nKod\ndf2.columns = [\n    f\"x_{c}\" if (c != \"\" and c[0].isdigit()) else c\n    for c in df2.columns\n]\ndf2.columns\n\n\nIndex(['customer_id', 'x_2023_revenue', 'avg_basket_value', 'return_var',\n       'very_long_column_name_describing_customer_behaviour_in_detail'],\n      dtype='object')\nKolejnym, często niedocenianym problemem są zbyt długie nazwy kolumn. Choć technicznie są poprawne, znacząco utrudniają pracę z kodem, zwłaszcza w dalszych etapach, takich jak budowa modeli, interpretacja współczynników czy wizualizacja wyników. W takich sytuacjach dobrą praktyką jest ręczne lub półautomatyczne skracanie nazw, przy zachowaniu ich semantyki:\nKod\ndf2 = df2.rename(columns={\n    \"very_long_column_name_describing_customer_behaviour_in_detail\": \"customer_behavior_flag\"\n})\ndf2.columns\n\n\nIndex(['customer_id', 'x_2023_revenue', 'avg_basket_value', 'return_var',\n       'customer_behavior_flag'],\n      dtype='object')\nW projektach analitycznych o większej skali często stosuje się funkcję pomocniczą, która automatycznie „czyści” nazwy kolumn według ustalonej reguły:\nKod\ndf2 = pd.DataFrame({\n    \"Customer ID\": [1, 2, 3],\n    \"2023 Revenue (€)\": [12000, 34000, 18000],\n    \"Avg Basket Value\": [45.5, 78.2, 33.1],\n    \"% Return\": [0.02, 0.01, 0.05],\n    \"Very long column name describing customer behaviour in detail\": [1, 0, 1]\n})\n\ndef clean_column_names(df, max_len=40):\n    cleaned = (\n        pd.Index(df.columns).map(str)\n        .str.strip()\n        .str.lower()\n        .str.replace(r\"\\s+\", \"_\", regex=True)\n        .str.replace(r\"[^a-z0-9_]\", \"\", regex=True)\n        .str.replace(r\"_+\", \"_\", regex=True)\n        .str.strip(\"_\")\n    )\n\n    seen = {}\n    final_cols = []\n    for c in cleaned:\n        # pusta nazwa po czyszczeniu\n        if c == \"\":\n            c = \"col\"\n\n        # nazwa zaczyna się od cyfry\n        if c[0].isdigit():\n            c = f\"x_{c}\"\n\n        # słowo kluczowe Pythona\n        if keyword.iskeyword(c):\n            c = f\"{c}_var\"\n\n        # opcjonalnie: skracamy bardzo długie nazwy (np. do pracy z modelami/tabelami)\n        if max_len is not None and len(c) &gt; max_len:\n            c = c[:max_len].rstrip(\"_\")\n            if c == \"\":\n                c = \"col\"\n\n        # unikamy kolizji nazw (np. dwie różne kolumny mogą się „zlać” po czyszczeniu)\n        count = seen.get(c, 0)\n        seen[c] = count + 1\n        if count &gt; 0:\n            suffix = f\"_{count+1}\"\n            base = c\n            if max_len is not None and len(base) + len(suffix) &gt; max_len:\n                base = base[: max_len - len(suffix)].rstrip(\"_\")\n                if base == \"\":\n                    base = \"col\"\n            c = f\"{base}{suffix}\"\n\n        final_cols.append(c)\n\n    df.columns = final_cols\n    return df\n\ndf2 = clean_column_names(df2, max_len = 22)\ndf2\n\n\n\n\n\n\n\n\n\ncustomer_id\nx_2023_revenue\navg_basket_value\nreturn_var\nvery_long_column_name\n\n\n\n\n0\n1\n12000\n45.5\n0.02\n1\n\n\n1\n2\n34000\n78.2\n0.01\n0\n\n\n2\n3\n18000\n33.1\n0.05\n1\nTakie podejście sprawia, że już na wczesnym etapie analizy dane zostają dostosowane do dalszej pracy z modelami, formułami i potokami przetwarzania. Co istotne, normalizacja nazw kolumn nie jest kosmetyką, lecz elementem przygotowania danych, który wpływa na czytelność, reprodukowalność oraz odporność kodu na błędy.\nW kontekście tego kursu warto podkreślić, że praca z „brudnymi” nazwami kolumn jest codziennością analityka danych. Umiejętność ich systematycznego korygowania powinna być traktowana na równi z obsługą braków danych czy standaryzacją zmiennych. Jest to jeden z pierwszych kroków w kierunku tworzenia stabilnych i profesjonalnych analiz.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Przygotowanie i czyszczenie danych</span>"
    ]
  },
  {
    "objectID": "chapters/02-przygotowanie-danych.html#import-i-czyszczenie-danych",
    "href": "chapters/02-przygotowanie-danych.html#import-i-czyszczenie-danych",
    "title": "3  Przygotowanie i czyszczenie danych",
    "section": "",
    "text": "1 Dane przypominają prosty wycinek danych klient–zakupy. Zawierają zmienne liczbowe (np. income_eur, avg_basket_eur, visits_last30), jakościowe (city, segment, device_type, notes), logiczne (has_promo), daty (signup_date, last_purchase_date) oraz zmienną docelową (churned). Celowo wprowadzono braki danych (w tym puste pola i znaczniki NA), wartości odstające (np. nienaturalnie wysokie dochody, koszyki zakupowe i liczby wizyt) oraz błędy jakościowe (np. wiek 5 i 120, ujemne zwroty, rozbieżności w kodowaniu kategorii: spacje i różne wielkości liter).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Przygotowanie i czyszczenie danych</span>"
    ]
  },
  {
    "objectID": "chapters/02-przygotowanie-danych.html#obserwacje-odstające-i-braki-danych",
    "href": "chapters/02-przygotowanie-danych.html#obserwacje-odstające-i-braki-danych",
    "title": "3  Przygotowanie i czyszczenie danych",
    "section": "3.2 Obserwacje odstające i braki danych",
    "text": "3.2 Obserwacje odstające i braki danych\nW praktyce analizy danych obserwacje odstające oraz braki danych są jednymi z najczęstszych źródeł błędnych wniosków i niestabilnych modeli. W EDA traktujemy je jako sygnał diagnostyczny: mogą oznaczać błędy pomiaru lub wprowadzania danych, ale mogą też być prawdziwymi rzadkimi zdarzeniami (np. klienci o bardzo wysokich dochodach). W modelowaniu ML odstające wartości i braki wpływają na estymację parametrów, stabilność uczenia i jakość generalizacji; dlatego kluczowe jest, aby postępowanie z nimi było konsekwentne, udokumentowane oraz wykonywane bez wycieku informacji (wszystkie „uczące się” transformacje dopasowujemy tylko na zbiorze treningowym).\n\n3.2.1 Obserwacje odstające\nObserwacje odstające (outliers) to wartości, które są „nietypowe” względem reszty danych. „Nietypowość” trzeba rozumieć w kontekście: może wynikać z rozkładu, jednostki miary, specyfiki biznesowej oraz tego, jakiego modelu używamy. W EDA zwykle rozróżnia się: (1) odstające wartości w pojedynczej zmiennej (univariate), (2) obserwacje odstające w relacji między zmiennymi (bivariate), oraz (3) odstające obserwacje wielowymiarowo (multivariate).\n\n3.2.1.1 Reguły matematyczne identyfikacji odstających (univariate)\nNajczęściej stosuje się regułę IQR (Tukeya) oraz regułę opartą o standaryzację. W regule IQR wyznaczamy kwartyle \\(Q_1\\) i \\(Q_3\\) oraz rozstęp międzykwartylowy \\(IQR = Q_3 - Q_1\\). Obserwację \\(x\\) uznaje się za odstającą, gdy:\n\\[\nx &lt; Q_1 - 1.5\\cdot IQR \\quad \\text{lub} \\quad x &gt; Q_3 + 1.5\\cdot IQR.\n\\]\nReguła standaryzacyjna używa z-score:\n\\[\nz = \\frac{x-\\mu}{\\sigma}.\n\\]\nTypowo za odstające przyjmuje się obserwacje o \\(|z| &gt; 3\\), ale jest to reguła wrażliwa na odstające wartości, ponieważ \\(\\mu\\) i \\(\\sigma\\) same ulegają zniekształceniu. W danych skośnych (np. dochody) często lepsze są metody odporne.\nOdporna standaryzacja opiera się na medianie i medianowym odchyleniu bezwzględnym MAD. Dla \\(MAD = \\mathrm{median}(|x - \\mathrm{median}(x)|)\\) stosuje się tzw. odporny z-score:\n\\[\nz_{\\mathrm{rob}} = \\frac{x-\\mathrm{median}(x)}{1.4826\\cdot MAD}.\n\\]\nWspółczynnik (1.4826 2) skaluje MAD do odpowiednika odchylenia standardowego przy rozkładzie normalnym.\n2 Liczba 1.4826 pochodzi ze „skalowania” miary MAD tak, aby była porównywalna z odchyleniem standardowym przy rozkładzie normalnym. Dla rozkładu normalnego \\(X \\sim \\mathcal N(\\mu,\\sigma)\\) zachodzi zależność \\(MAD = \\Phi^{-1}(0.75)\\,\\sigma \\approx 0.6745,\\sigma\\) bo dla \\(Z\\sim\\mathcal N(0,1)\\) wartość \\(\\mathrm{median}(|Z|)\\) to dokładnie 75. percentyl rozkładu normalnego: \\(\\Phi^{-1}(0.75)\\). Żeby z MAD zrobić estymator „w skali \\(\\sigma\\)”, mnoży się przez odwrotność tej stałej \\(\\sigma \\approx \\frac{MAD}{0.6745} \\approx 1.4826 \\cdot MAD\\) i stąd bierze się 1.4826 (dokładniej \\(1 / \\Phi^{-1}(0.75)\\)).W naszych danych sensownymi kandydatami do analizy odstających są: income_eur, avg_basket_eur, visits_last30, a także „błędy logiczne” w age i returns_last90.\n\n\nKod\nimport numpy as np\n\nnum_cols = [\"age\", \"income_eur\", \"visits_last30\", \"avg_basket_eur\", \"returns_last90\", \"satisfaction_1_5\"]\n\n# Szybka diagnostyka: podstawowe statystyki + percentyle\ndf[num_cols].describe(percentiles=[0.01, 0.05, 0.95, 0.99]).T\n\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n1%\n5%\n50%\n95%\n99%\nmax\n\n\n\n\nage\n333.0\n38.207207\n14.428125\n5.00\n16.0000\n16.6000\n38.000\n58.4000\n69.0000\n120.0\n\n\nincome_eur\n317.0\n40776.328076\n44393.786000\n4302.00\n8218.6800\n11883.6000\n32632.000\n83047.6000\n177796.6000\n490032.0\n\n\nvisits_last30\n350.0\n6.045714\n2.714320\n0.00\n1.0000\n2.0000\n6.000\n11.0000\n13.0000\n24.0\n\n\navg_basket_eur\n332.0\n52.835060\n48.248423\n16.05\n18.0016\n20.9775\n43.835\n99.6435\n150.4721\n668.8\n\n\nreturns_last90\n350.0\n0.777143\n0.880572\n-1.00\n0.0000\n0.0000\n1.000\n3.0000\n3.0000\n4.0\n\n\nsatisfaction_1_5\n327.0\n3.645260\n0.887628\n1.00\n1.0000\n2.0000\n4.000\n5.0000\n5.0000\n5.0\n\n\n\n\n\n\n\n\n\n3.2.1.2 Reguła IQR dla jednej zmiennej\n\n\nKod\ndef iqr_outliers(s: pd.Series, k: float = 1.5):\n    s = s.dropna()\n    q1, q3 = s.quantile(0.25), s.quantile(0.75)\n    iqr = q3 - q1\n    lower, upper = q1 - k * iqr, q3 + k * iqr\n    return lower, upper\n\nlower_inc, upper_inc = iqr_outliers(df[\"income_eur\"])\nout_income = df[(df[\"income_eur\"] &lt; lower_inc) | (df[\"income_eur\"] &gt; upper_inc)]\nout_income[[\"customer_id\", \"income_eur\", \"city\", \"segment\"]].head(10)\n\n\n\n\n\n\n\n\n\ncustomer_id\nincome_eur\ncity\nsegment\n\n\n\n\n39\n100039\n485953.0\nŁódź\nVIP\n\n\n55\n100055\n146032.0\nŁódź\nB2C\n\n\n59\n100059\n86006.0\nWrocław\nVIP\n\n\n66\n100066\n98746.0\nWrocław\nStudent\n\n\n67\n100067\n85420.0\nWrocław\nB2C\n\n\n82\n100082\n80700.0\nLublin\nB2C\n\n\n83\n100083\n82821.0\nNone\nB2C\n\n\n94\n100094\n490032.0\nLublin\nB2C\n\n\n95\n100095\n86004.0\nPoznań\nB2B\n\n\n151\n100151\n87909.0\nWrocław\nB2C\n\n\n\n\n\n\n\n\n\n3.2.1.3 Odporny z-score (MAD)\n\n\nKod\ndef robust_zscore(s: pd.Series):\n    s = s.astype(float)\n    med = np.nanmedian(s)\n    mad = np.nanmedian(np.abs(s - med))\n    return (s - med) / (1.4826 * mad)\n\ndf[\"income_robust_z\"] = robust_zscore(df[\"income_eur\"])\ndf.loc[df[\"income_robust_z\"].abs() &gt; 3, [\"customer_id\", \"income_eur\", \"income_robust_z\"]].head(10)\n\n\n\n\n\n\n\n\n\ncustomer_id\nincome_eur\nincome_robust_z\n\n\n\n\n39\n100039\n485953.0\n26.659763\n\n\n55\n100055\n146032.0\n6.669043\n\n\n59\n100059\n86006.0\n3.138920\n\n\n66\n100066\n98746.0\n3.888158\n\n\n67\n100067\n85420.0\n3.104457\n\n\n94\n100094\n490032.0\n26.899649\n\n\n95\n100095\n86004.0\n3.138802\n\n\n151\n100151\n87909.0\n3.250835\n\n\n153\n100153\n91242.0\n3.446848\n\n\n155\n100155\n83954.0\n3.018242\n\n\n\n\n\n\n\n\n\n3.2.1.4 „Odstające” jako błąd jakości danych\nCzęść wartości odstających to nie „rzadkie przypadki”, tylko po prostu błędne dane. W data.csv celowo pojawiają się np. wiek 5 i 120 oraz ujemna liczba zwrotów (returns_last90 == -1). To nie są outliery w sensie rozkładu, tylko naruszenia dziedziny wartości.\nW EDA warto wprost zdefiniować reguły walidacji, np.: \\(16 \\leq \\text{age} \\leq 80,\\) \\(\\text{returns\\_last90} \\geq 0.\\)\nW Pythonie:\n\n\nKod\nbad_age = df[(df[\"age\"] &lt; 16) | (df[\"age\"] &gt; 80)]\nbad_returns = df[df[\"returns_last90\"] &lt; 0]\n\nbad_age[[\"customer_id\",\"age\"]].head(), bad_returns[[\"customer_id\",\"returns_last90\"]].head()\n\n\n(     customer_id    age\n 43        100043  120.0\n 105       100105  120.0\n 144       100144    5.0\n 250       100250    5.0\n 340       100340  120.0,\n      customer_id  returns_last90\n 5         100005              -1\n 93        100093              -1\n 203       100203              -1)\n\n\nTakie przypadki najczęściej traktuje się jako: (1) poprawa na podstawie źródła (jeśli możliwa), albo (2) ustawienie na brak (NaN) i późniejsza imputacja, albo (3) usunięcie obserwacji (jeżeli jest ich mało i są ewidentnie błędne).\n\n\n3.2.1.5 Co robimy z obserwacjami odstającymi w EDA i ML\nW EDA celem jest zrozumienie przyczyny i konsekwencji odstających wartości. Zwykle robimy trzy rzeczy: po pierwsze, oceniamy skalę zjawiska (ile obserwacji, w których zmiennych), po drugie, sprawdzamy czy są to błędy (np. wartości niemożliwe), a po trzecie, analizujemy wpływ na wnioski (np. średnie, korelacje, wykresy). Warto pamiętać, że średnia i odchylenie standardowe są wrażliwe, dlatego do EDA często stosuje się medianę i IQR.\nW ML decyzja zależy od modelu i celu. Dla modeli liniowych i dyskryminacyjnych (np. regresja logistyczna, LDA/QDA) odstające wartości mogą znacząco zaburzyć dopasowanie, dlatego typowe strategie to: transformacje (np. \\(\\log\\) dla income_eur), winsoryzacja/obcięcie ogonów, zastosowanie skalowania odpornego (np. RobustScaler), albo świadome usunięcie obserwacji błędnych. Dla drzew i ich zespołów (Random Forest, boosting) pojedyncze ekstremalne wartości często są mniej groźne, ale nadal mogą wpływać na podziały lub na stabilność w małych próbkach. Dla metod odległościowych i SVM skalowanie jest kluczowe: outliery w skali potrafią zdominować metrykę lub margines.\nNajczęściej spotykane „operacyjne” podejścia to:\n\ntransformacja rozkładu (np. \\(\\log\\), Box–Cox/Yeo–Johnson),\nwinsoryzacja: obcięcie do percentyli (np. 1%–99%),\nflaga odstających jako dodatkowa cecha (model uczy się, że to przypadek nietypowy),\nusunięcie tylko wtedy, gdy jest to błąd lub obserwacja spoza domeny problemu.\n\nPrzykład winsoryzacji (percentyle) dla income_eur:\n\n\nKod\np01, p99 = df[\"income_eur\"].quantile([0.01, 0.99])\ndf[\"income_eur_capped\"] = df[\"income_eur\"].clip(lower=p01, upper=p99)\n\n\n\n\n\n3.2.2 Braki danych i imputacja\nBraki danych (missing values) mogą wynikać z problemów pomiaru, integracji źródeł, błędów ETL, ale także z „logiki procesu” (np. klient nie ma ocen satysfakcji, bo nie wypełnił ankiety). W EDA kluczowe jest rozpoznanie: (1) gdzie braki występują, (2) ile ich jest, (3) czy są zależne od innych zmiennych (braki „systematyczne”). W ML brak danych wymaga decyzji, ponieważ większość klasycznych modeli nie obsługuje NaN wprost.\nNa początek warto policzyć braki:\n\n\nKod\nna_counts = df.isna().sum().sort_values(ascending=False)\nna_share = (df.isna().mean().sort_values(ascending=False) * 100).round(2)\npd.DataFrame({\"missing_count\": na_counts, \"missing_%\": na_share})\n\n\n\n\n\n\n\n\n\nmissing_count\nmissing_%\n\n\n\n\nincome_eur_capped\n33\n9.43\n\n\nincome_robust_z\n33\n9.43\n\n\nincome_eur\n33\n9.43\n\n\nsatisfaction_1_5\n23\n6.57\n\n\ndevice_type\n21\n6.00\n\n\navg_basket_eur\n18\n5.14\n\n\nage\n17\n4.86\n\n\ncity\n17\n4.86\n\n\nnotes\n16\n4.57\n\n\nhas_promo\n0\n0.00\n\n\nchurned\n0\n0.00\n\n\ncustomer_id\n0\n0.00\n\n\nreturns_last90\n0\n0.00\n\n\nsignup_date\n0\n0.00\n\n\nsegment\n0\n0.00\n\n\nlast_purchase_date\n0\n0.00\n\n\nvisits_last30\n0\n0.00\n\n\n\n\n\n\n\n\n3.2.2.1 Co robimy z brakami w EDA i ML\nW EDA braki traktujemy jako informację o jakości danych i procesie ich powstawania. Sprawdzamy, czy braki są losowe, czy dotyczą specyficznych segmentów (np. brak income_eur tylko dla jednego segmentu), oraz czy ich usunięcie nie zmieni populacji (bias). W ML trzeba z kolei zadbać o to, aby imputacja była wykonywana bez wycieku informacji: parametry imputacji wyznaczamy na zbiorze treningowym i stosujemy do walidacyjnego/testowego.\nDecyzje praktyczne obejmują: usuwanie cech/wierszy (gdy braków jest bardzo dużo), imputację prostą (średnia/mediana/moda), imputację warunkową (np. mediana w grupach), metody oparte na podobieństwie (k-NN), metody iteracyjne (MICE), a także dodawanie wskaźników braków (missing indicators), które pozwalają modelowi uczyć się samego faktu braku jako sygnału.\n\n\n3.2.2.2 Metody imputacji\n\nImputacja stałą lub modą (kategorie i zmienne dyskretne)\n\nDla zmiennych kategorycznych często stosuje się modę lub specjalną kategorię ‘missing’. Dlaczego to bywa dobre? Ponieważ w wielu danych fakt, że czegoś brakuje, jest informacją samą w sobie. Na przykład: brak miasta może oznaczać, że klient nie podał danych adresowych, co może korelować z innymi cechami lub nawet z churned. Jeśli zamienimy brak na osobną kategorię “missing”, model może się tego „nauczyć”.\n\n\nKod\ndf[\"city\"] = df[\"city\"].fillna(\"missing\")\ndf[\"device_type\"] = df[\"device_type\"].fillna(\"missing\")\n\n\n\nImputacja medianą/średnią (zmienne liczbowe)\n\nJest to metoda szybka, ale ignoruje zależności między cechami:\n\n\nKod\ndf[\"income_eur\"] = df[\"income_eur\"].fillna(df[\"income_eur\"].median())\n\n\n\nImputacja grupowa (warunkowa)\n\nBardzo użyteczna, gdy rozkłady różnią się w segmentach, np. dochody w segmentach VIP vs Student:\n\n\nKod\ndf[\"income_eur\"] = df[\"income_eur\"].fillna(\n    df.groupby(\"segment\")[\"income_eur\"].transform(\"median\")\n)\n\n\n\nk-NN Imputer (podobieństwo obserwacji)\n\nWykorzystuje sąsiadów w przestrzeni cech do uzupełniania braków. Wymaga skalowania i pracy na liczbach.\n\n\nKod\nfrom sklearn.impute import KNNImputer\n\nnum_features = [\"age\", \"income_eur\", \"visits_last30\", \"avg_basket_eur\", \"returns_last90\", \"satisfaction_1_5\"]\nimputer = KNNImputer(n_neighbors=5, weights=\"distance\") # obserwacje imputujące będą ważone odległością\n\ndf[num_features] = imputer.fit_transform(df[num_features])\n\n\n\nIterative Imputer (MICE – imputacja modelowa)\n\nUzupełnia jedną zmienną na podstawie pozostałych (iteracyjnie), co lepiej zachowuje relacje w danych. Domyślną opcją IterativeImputer jest regresja liniowa z regularyzacją typu ridge w ujęciu bayesowskim (sklearn.linear_model.BayesianRidge). Można jednak wybrać inne techniki jako model uzupelniający\n\n\nKod\nfrom sklearn.ensemble import RandomForestRegressor\nimp = IterativeImputer(estimator=RandomForestRegressor(n_estimators=200, random_state=42))\n\n\n\n\nKod\nfrom sklearn.experimental import enable_iterative_imputer  # noqa: F401\nfrom sklearn.impute import IterativeImputer\n\nimp = IterativeImputer(random_state=42, max_iter=20)\ndf[num_features] = imp.fit_transform(df[num_features])\n\n\n\nWskaźniki braków jako cechy\n\nCzasem sam fakt braku jest informacyjny (np. brak satysfakcji może korelować z churn). Można dodać flagi:\n\n\nKod\n# ponieważ wcześniej zmienna income_eur była zastępowana medianami to poniższy kod da same 0 dla income_missing. Podobnie dla zmiennej satisfaction_1_5\ndf[\"income_missing\"] = df[\"income_eur\"].isna().astype(int)\ndf[\"satisfaction_missing\"] = df[\"satisfaction_1_5\"].isna().astype(int)\n\n\nW praktyce (szczególnie w ML) flagi braków często działają dobrze w połączeniu z imputacją, bo pozwalają modelowi rozróżnić „wartość prawdziwą” od „wartości wstawionej”.\n\n\n\n\n\n\nOstrzeżenie\n\n\n\nW modelowaniu nie imputujemy „ręcznie na całym df”, tylko budujemy potok, który dopasowuje imputery na treningu, a potem stosuje na walidacji/teście:\n\n\nKod\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.linear_model import LogisticRegression\n\nX = df.drop(columns=[\"churned\"])\ny = df[\"churned\"]\n\n# Uwaga: pandas może trzymać braki jako `pd.NA` (np. typy StringDtype/boolean).\n# scikit-learn oczekuje braków jako `np.nan` i potrafi się wysypać na `pd.NA`.\nX = X.replace({pd.NA: np.nan})\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n\nnum_cols = [\"age\", \"income_eur\", \"visits_last30\", \"avg_basket_eur\", \"returns_last90\", \"satisfaction_1_5\"]\ncat_cols = [\"city\", \"segment\", \"device_type\", \"has_promo\", \"notes\"]\n\n# (opcjonalnie) upewniamy się, że kategorie są zwykłym `object`, a nie np. `string[python]` z `pd.NA`\nX_train[cat_cols] = X_train[cat_cols].astype(\"object\")\nX_test[cat_cols] = X_test[cat_cols].astype(\"object\")\n\npreprocess = ColumnTransformer(\n    transformers=[\n        (\"num\", Pipeline(steps=[\n            (\"imputer\", SimpleImputer(strategy=\"median\"))\n        ]), num_cols),\n        (\"cat\", Pipeline(steps=[\n            (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n            (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n        ]), cat_cols),\n    ],\n    remainder=\"drop\"\n)\n\nmodel = Pipeline(steps=[\n    (\"prep\", preprocess),\n    (\"clf\", LogisticRegression(max_iter=200))\n])\n\nmodel.fit(X_train, y_train)\n\n\nPipeline(steps=[('prep',\n                 ColumnTransformer(transformers=[('num',\n                                                  Pipeline(steps=[('imputer',\n                                                                   SimpleImputer(strategy='median'))]),\n                                                  ['age', 'income_eur',\n                                                   'visits_last30',\n                                                   'avg_basket_eur',\n                                                   'returns_last90',\n                                                   'satisfaction_1_5']),\n                                                 ('cat',\n                                                  Pipeline(steps=[('imputer',\n                                                                   SimpleImputer(strategy='most_frequent')),\n                                                                  ('onehot',\n                                                                   OneHotEncoder(handle_unknown='ignore'))]),\n                                                  ['city', 'segment',\n                                                   'device_type', 'has_promo',\n                                                   'notes'])])),\n                ('clf', LogisticRegression(max_iter=200))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiFitted\n        \n            \n                Parameters\n                \n\n\n\n\n\n\n\n\n\nsteps steps: list of tuples\n\nList of (name of step, estimator) tuples that are to be chained in\nsequential order. To be compatible with the scikit-learn API, all steps\nmust define `fit`. All non-last steps must also define `transform`. See\n:ref:`Combining Estimators ` for more details.\n[('prep', ...), ('clf', ...)]\n\n\n\ntransform_input transform_input: list of str, default=None\n\nThe names of the :term:`metadata` parameters that should be transformed by the\npipeline before passing it to the step consuming it.\n\nThis enables transforming some input arguments to ``fit`` (other than ``X``)\nto be transformed by the steps of the pipeline up to the step which requires\nthem. Requirement is defined via :ref:`metadata routing `.\nFor instance, this can be used to pass a validation set through the pipeline.\n\nYou can only set this if metadata routing is enabled, which you\ncan enable using ``sklearn.set_config(enable_metadata_routing=True)``.\n\n.. versionadded:: 1.6\nNone\n\n\n\nmemory memory: str or object with the joblib.Memory interface, default=None\n\nUsed to cache the fitted transformers of the pipeline. The last step\nwill never be cached, even if it is a transformer. By default, no\ncaching is performed. If a string is given, it is the path to the\ncaching directory. Enabling caching triggers a clone of the transformers\nbefore fitting. Therefore, the transformer instance given to the\npipeline cannot be inspected directly. Use the attribute ``named_steps``\nor ``steps`` to inspect estimators within the pipeline. Caching the\ntransformers is advantageous when fitting is time consuming. See\n:ref:`sphx_glr_auto_examples_neighbors_plot_caching_nearest_neighbors.py`\nfor an example on how to enable caching.\nNone\n\n\n\nverbose verbose: bool, default=False\n\nIf True, the time elapsed while fitting each step will be printed as it\nis completed.\nFalse\n\n\n\n\n            \n        \n    prep: ColumnTransformer?Documentation for prep: ColumnTransformer\n        \n            \n                Parameters\n                \n\n\n\n\n\n\n\n\n\ntransformers transformers: list of tuples\n\nList of (name, transformer, columns) tuples specifying the\ntransformer objects to be applied to subsets of the data.\n\nname : str\nLike in Pipeline and FeatureUnion, this allows the transformer and\nits parameters to be set using ``set_params`` and searched in grid\nsearch.\ntransformer : {'drop', 'passthrough'} or estimator\nEstimator must support :term:`fit` and :term:`transform`.\nSpecial-cased strings 'drop' and 'passthrough' are accepted as\nwell, to indicate to drop the columns or to pass them through\nuntransformed, respectively.\ncolumns : str, array-like of str, int, array-like of int, array-like of bool, slice or callable\nIndexes the data on its second axis. Integers are interpreted as\npositional columns, while strings can reference DataFrame columns\nby name. A scalar string or int should be used where\n``transformer`` expects X to be a 1d array-like (vector),\notherwise a 2d array will be passed to the transformer.\nA callable is passed the input data `X` and can return any of the\nabove. To select multiple columns by name or dtype, you can use\n:obj:`make_column_selector`.\n[('num', ...), ('cat', ...)]\n\n\n\nremainder remainder: {'drop', 'passthrough'} or estimator, default='drop'\n\nBy default, only the specified columns in `transformers` are\ntransformed and combined in the output, and the non-specified\ncolumns are dropped. (default of ``'drop'``).\nBy specifying ``remainder='passthrough'``, all remaining columns that\nwere not specified in `transformers`, but present in the data passed\nto `fit` will be automatically passed through. This subset of columns\nis concatenated with the output of the transformers. For dataframes,\nextra columns not seen during `fit` will be excluded from the output\nof `transform`.\nBy setting ``remainder`` to be an estimator, the remaining\nnon-specified columns will use the ``remainder`` estimator. The\nestimator must support :term:`fit` and :term:`transform`.\nNote that using this feature requires that the DataFrame columns\ninput at :term:`fit` and :term:`transform` have identical order.\n'drop'\n\n\n\nsparse_threshold sparse_threshold: float, default=0.3\n\nIf the output of the different transformers contains sparse matrices,\nthese will be stacked as a sparse matrix if the overall density is\nlower than this value. Use ``sparse_threshold=0`` to always return\ndense. When the transformed output consists of all dense data, the\nstacked result will be dense, and this keyword will be ignored.\n0.3\n\n\n\nn_jobs n_jobs: int, default=None\n\nNumber of jobs to run in parallel.\n``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n``-1`` means using all processors. See :term:`Glossary `\nfor more details.\nNone\n\n\n\ntransformer_weights transformer_weights: dict, default=None\n\nMultiplicative weights for features per transformer. The output of the\ntransformer is multiplied by these weights. Keys are transformer names,\nvalues the weights.\nNone\n\n\n\nverbose verbose: bool, default=False\n\nIf True, the time elapsed while fitting each transformer will be\nprinted as it is completed.\nFalse\n\n\n\nverbose_feature_names_out verbose_feature_names_out: bool, str or Callable[[str, str], str], default=True\n\n- If True, :meth:`ColumnTransformer.get_feature_names_out` will prefix\nall feature names with the name of the transformer that generated that\nfeature. It is equivalent to setting\n`verbose_feature_names_out=\"{transformer_name}__{feature_name}\"`.\n- If False, :meth:`ColumnTransformer.get_feature_names_out` will not\nprefix any feature names and will error if feature names are not\nunique.\n- If ``Callable[[str, str], str]``,\n:meth:`ColumnTransformer.get_feature_names_out` will rename all the features\nusing the name of the transformer. The first argument of the callable is the\ntransformer name and the second argument is the feature name. The returned\nstring will be the new feature name.\n- If ``str``, it must be a string ready for formatting. The given string will\nbe formatted using two field names: ``transformer_name`` and ``feature_name``.\ne.g. ``\"{feature_name}__{transformer_name}\"``. See :meth:`str.format` method\nfrom the standard library for more info.\n\n.. versionadded:: 1.0\n\n.. versionchanged:: 1.6\n`verbose_feature_names_out` can be a callable or a string to be formatted.\nTrue\n\n\n\nforce_int_remainder_cols force_int_remainder_cols: bool, default=False\n\nThis parameter has no effect.\n\n.. note::\nIf you do not access the list of columns for the remainder columns\nin the `transformers_` fitted attribute, you do not need to set\nthis parameter.\n\n.. versionadded:: 1.5\n\n.. versionchanged:: 1.7\nThe default value for `force_int_remainder_cols` will change from\n`True` to `False` in version 1.7.\n\n.. deprecated:: 1.7\n`force_int_remainder_cols` is deprecated and will be removed in 1.9.\n'deprecated'\n\n\n\n\n            \n        \n    num['age', 'income_eur', 'visits_last30', 'avg_basket_eur', 'returns_last90', 'satisfaction_1_5']SimpleImputer?Documentation for SimpleImputer\n        \n            \n                Parameters\n                \n\n\n\n\n\n\n\n\n\nmissing_values missing_values: int, float, str, np.nan, None or pandas.NA, default=np.nan\n\nThe placeholder for the missing values. All occurrences of\n`missing_values` will be imputed. For pandas' dataframes with\nnullable integer dtypes with missing values, `missing_values`\ncan be set to either `np.nan` or `pd.NA`.\nnan\n\n\n\nstrategy strategy: str or Callable, default='mean'\n\nThe imputation strategy.\n\n- If \"mean\", then replace missing values using the mean along\neach column. Can only be used with numeric data.\n- If \"median\", then replace missing values using the median along\neach column. Can only be used with numeric data.\n- If \"most_frequent\", then replace missing using the most frequent\nvalue along each column. Can be used with strings or numeric data.\nIf there is more than one such value, only the smallest is returned.\n- If \"constant\", then replace missing values with fill_value. Can be\nused with strings or numeric data.\n- If an instance of Callable, then replace missing values using the\nscalar statistic returned by running the callable over a dense 1d\narray containing non-missing values of each column.\n\n.. versionadded:: 0.20\nstrategy=\"constant\" for fixed value imputation.\n\n.. versionadded:: 1.5\nstrategy=callable for custom value imputation.\n'median'\n\n\n\nfill_value fill_value: str or numerical value, default=None\n\nWhen strategy == \"constant\", `fill_value` is used to replace all\noccurrences of missing_values. For string or object data types,\n`fill_value` must be a string.\nIf `None`, `fill_value` will be 0 when imputing numerical\ndata and \"missing_value\" for strings or object data types.\nNone\n\n\n\ncopy copy: bool, default=True\n\nIf True, a copy of X will be created. If False, imputation will\nbe done in-place whenever possible. Note that, in the following cases,\na new copy will always be made, even if `copy=False`:\n\n- If `X` is not an array of floating values;\n- If `X` is encoded as a CSR matrix;\n- If `add_indicator=True`.\nTrue\n\n\n\nadd_indicator add_indicator: bool, default=False\n\nIf True, a :class:`MissingIndicator` transform will stack onto output\nof the imputer's transform. This allows a predictive estimator\nto account for missingness despite imputation. If a feature has no\nmissing values at fit/train time, the feature won't appear on\nthe missing indicator even if there are missing values at\ntransform/test time.\nFalse\n\n\n\nkeep_empty_features keep_empty_features: bool, default=False\n\nIf True, features that consist exclusively of missing values when\n`fit` is called are returned in results when `transform` is called.\nThe imputed value is always `0` except when `strategy=\"constant\"`\nin which case `fill_value` will be used instead.\n\n.. versionadded:: 1.2\nFalse\n\n\n\n\n            \n        \n    cat['city', 'segment', 'device_type', 'has_promo', 'notes']SimpleImputer?Documentation for SimpleImputer\n        \n            \n                Parameters\n                \n\n\n\n\n\n\n\n\n\nmissing_values missing_values: int, float, str, np.nan, None or pandas.NA, default=np.nan\n\nThe placeholder for the missing values. All occurrences of\n`missing_values` will be imputed. For pandas' dataframes with\nnullable integer dtypes with missing values, `missing_values`\ncan be set to either `np.nan` or `pd.NA`.\nnan\n\n\n\nstrategy strategy: str or Callable, default='mean'\n\nThe imputation strategy.\n\n- If \"mean\", then replace missing values using the mean along\neach column. Can only be used with numeric data.\n- If \"median\", then replace missing values using the median along\neach column. Can only be used with numeric data.\n- If \"most_frequent\", then replace missing using the most frequent\nvalue along each column. Can be used with strings or numeric data.\nIf there is more than one such value, only the smallest is returned.\n- If \"constant\", then replace missing values with fill_value. Can be\nused with strings or numeric data.\n- If an instance of Callable, then replace missing values using the\nscalar statistic returned by running the callable over a dense 1d\narray containing non-missing values of each column.\n\n.. versionadded:: 0.20\nstrategy=\"constant\" for fixed value imputation.\n\n.. versionadded:: 1.5\nstrategy=callable for custom value imputation.\n'most_frequent'\n\n\n\nfill_value fill_value: str or numerical value, default=None\n\nWhen strategy == \"constant\", `fill_value` is used to replace all\noccurrences of missing_values. For string or object data types,\n`fill_value` must be a string.\nIf `None`, `fill_value` will be 0 when imputing numerical\ndata and \"missing_value\" for strings or object data types.\nNone\n\n\n\ncopy copy: bool, default=True\n\nIf True, a copy of X will be created. If False, imputation will\nbe done in-place whenever possible. Note that, in the following cases,\na new copy will always be made, even if `copy=False`:\n\n- If `X` is not an array of floating values;\n- If `X` is encoded as a CSR matrix;\n- If `add_indicator=True`.\nTrue\n\n\n\nadd_indicator add_indicator: bool, default=False\n\nIf True, a :class:`MissingIndicator` transform will stack onto output\nof the imputer's transform. This allows a predictive estimator\nto account for missingness despite imputation. If a feature has no\nmissing values at fit/train time, the feature won't appear on\nthe missing indicator even if there are missing values at\ntransform/test time.\nFalse\n\n\n\nkeep_empty_features keep_empty_features: bool, default=False\n\nIf True, features that consist exclusively of missing values when\n`fit` is called are returned in results when `transform` is called.\nThe imputed value is always `0` except when `strategy=\"constant\"`\nin which case `fill_value` will be used instead.\n\n.. versionadded:: 1.2\nFalse\n\n\n\n\n            \n        \n    OneHotEncoder?Documentation for OneHotEncoder\n        \n            \n                Parameters\n                \n\n\n\n\n\n\n\n\n\ncategories categories: 'auto' or a list of array-like, default='auto'\n\nCategories (unique values) per feature:\n\n- 'auto' : Determine categories automatically from the training data.\n- list : ``categories[i]`` holds the categories expected in the ith\ncolumn. The passed categories should not mix strings and numeric\nvalues within a single feature, and should be sorted in case of\nnumeric values.\n\nThe used categories can be found in the ``categories_`` attribute.\n\n.. versionadded:: 0.20\n'auto'\n\n\n\ndrop drop: {'first', 'if_binary'} or an array-like of shape (n_features,), default=None\n\nSpecifies a methodology to use to drop one of the categories per\nfeature. This is useful in situations where perfectly collinear\nfeatures cause problems, such as when feeding the resulting data\ninto an unregularized linear regression model.\n\nHowever, dropping one category breaks the symmetry of the original\nrepresentation and can therefore induce a bias in downstream models,\nfor instance for penalized linear classification or regression models.\n\n- None : retain all features (the default).\n- 'first' : drop the first category in each feature. If only one\ncategory is present, the feature will be dropped entirely.\n- 'if_binary' : drop the first category in each feature with two\ncategories. Features with 1 or more than 2 categories are\nleft intact.\n- array : ``drop[i]`` is the category in feature ``X[:, i]`` that\nshould be dropped.\n\nWhen `max_categories` or `min_frequency` is configured to group\ninfrequent categories, the dropping behavior is handled after the\ngrouping.\n\n.. versionadded:: 0.21\nThe parameter `drop` was added in 0.21.\n\n.. versionchanged:: 0.23\nThe option `drop='if_binary'` was added in 0.23.\n\n.. versionchanged:: 1.1\nSupport for dropping infrequent categories.\nNone\n\n\n\nsparse_output sparse_output: bool, default=True\n\nWhen ``True``, it returns a :class:`scipy.sparse.csr_matrix`,\ni.e. a sparse matrix in \"Compressed Sparse Row\" (CSR) format.\n\n.. versionadded:: 1.2\n`sparse` was renamed to `sparse_output`\nTrue\n\n\n\ndtype dtype: number type, default=np.float64\n\nDesired dtype of output.\n&lt;class 'numpy.float64'&gt;\n\n\n\nhandle_unknown handle_unknown: {'error', 'ignore', 'infrequent_if_exist', 'warn'}, default='error'\n\nSpecifies the way unknown categories are handled during :meth:`transform`.\n\n- 'error' : Raise an error if an unknown category is present during transform.\n- 'ignore' : When an unknown category is encountered during\ntransform, the resulting one-hot encoded columns for this feature\nwill be all zeros. In the inverse transform, an unknown category\nwill be denoted as None.\n- 'infrequent_if_exist' : When an unknown category is encountered\nduring transform, the resulting one-hot encoded columns for this\nfeature will map to the infrequent category if it exists. The\ninfrequent category will be mapped to the last position in the\nencoding. During inverse transform, an unknown category will be\nmapped to the category denoted `'infrequent'` if it exists. If the\n`'infrequent'` category does not exist, then :meth:`transform` and\n:meth:`inverse_transform` will handle an unknown category as with\n`handle_unknown='ignore'`. Infrequent categories exist based on\n`min_frequency` and `max_categories`. Read more in the\n:ref:`User Guide `.\n- 'warn' : When an unknown category is encountered during transform\na warning is issued, and the encoding then proceeds as described for\n`handle_unknown=\"infrequent_if_exist\"`.\n\n.. versionchanged:: 1.1\n`'infrequent_if_exist'` was added to automatically handle unknown\ncategories and infrequent categories.\n\n.. versionadded:: 1.6\nThe option `\"warn\"` was added in 1.6.\n'ignore'\n\n\n\nmin_frequency min_frequency: int or float, default=None\n\nSpecifies the minimum frequency below which a category will be\nconsidered infrequent.\n\n- If `int`, categories with a smaller cardinality will be considered\ninfrequent.\n\n- If `float`, categories with a smaller cardinality than\n`min_frequency * n_samples` will be considered infrequent.\n\n.. versionadded:: 1.1\nRead more in the :ref:`User Guide `.\nNone\n\n\n\nmax_categories max_categories: int, default=None\n\nSpecifies an upper limit to the number of output features for each input\nfeature when considering infrequent categories. If there are infrequent\ncategories, `max_categories` includes the category representing the\ninfrequent categories along with the frequent categories. If `None`,\nthere is no limit to the number of output features.\n\n.. versionadded:: 1.1\nRead more in the :ref:`User Guide `.\nNone\n\n\n\nfeature_name_combiner feature_name_combiner: \"concat\" or callable, default=\"concat\"\n\nCallable with signature `def callable(input_feature, category)` that returns a\nstring. This is used to create feature names to be returned by\n:meth:`get_feature_names_out`.\n\n`\"concat\"` concatenates encoded feature name and category with\n`feature + \"_\" + str(category)`.E.g. feature X with values 1, 6, 7 create\nfeature names `X_1, X_6, X_7`.\n\n.. versionadded:: 1.3\n'concat'\n\n\n\n\n            \n        \n    LogisticRegression?Documentation for LogisticRegression\n        \n            \n                Parameters\n                \n\n\n\n\n\n\n\n\n\npenalty penalty: {'l1', 'l2', 'elasticnet', None}, default='l2'\n\nSpecify the norm of the penalty:\n\n- `None`: no penalty is added;\n- `'l2'`: add a L2 penalty term and it is the default choice;\n- `'l1'`: add a L1 penalty term;\n- `'elasticnet'`: both L1 and L2 penalty terms are added.\n\n.. warning::\nSome penalties may not work with some solvers. See the parameter\n`solver` below, to know the compatibility between the penalty and\nsolver.\n\n.. versionadded:: 0.19\nl1 penalty with SAGA solver (allowing 'multinomial' + L1)\n\n.. deprecated:: 1.8\n`penalty` was deprecated in version 1.8 and will be removed in 1.10.\nUse `l1_ratio` instead. `l1_ratio=0` for `penalty='l2'`, `l1_ratio=1` for\n`penalty='l1'` and `l1_ratio` set to any float between 0 and 1 for\n`'penalty='elasticnet'`.\n'deprecated'\n\n\n\nC C: float, default=1.0\n\nInverse of regularization strength; must be a positive float.\nLike in support vector machines, smaller values specify stronger\nregularization. `C=np.inf` results in unpenalized logistic regression.\nFor a visual example on the effect of tuning the `C` parameter\nwith an L1 penalty, see:\n:ref:`sphx_glr_auto_examples_linear_model_plot_logistic_path.py`.\n1.0\n\n\n\nl1_ratio l1_ratio: float, default=0.0\n\nThe Elastic-Net mixing parameter, with `0 &lt;= l1_ratio &lt;= 1`. Setting\n`l1_ratio=1` gives a pure L1-penalty, setting `l1_ratio=0` a pure L2-penalty.\nAny value between 0 and 1 gives an Elastic-Net penalty of the form\n`l1_ratio * L1 + (1 - l1_ratio) * L2`.\n\n.. warning::\nCertain values of `l1_ratio`, i.e. some penalties, may not work with some\nsolvers. See the parameter `solver` below, to know the compatibility between\nthe penalty and solver.\n\n.. versionchanged:: 1.8\nDefault value changed from None to 0.0.\n\n.. deprecated:: 1.8\n`None` is deprecated and will be removed in version 1.10. Always use\n`l1_ratio` to specify the penalty type.\n0.0\n\n\n\ndual dual: bool, default=False\n\nDual (constrained) or primal (regularized, see also\n:ref:`this equation `) formulation. Dual formulation\nis only implemented for l2 penalty with liblinear solver. Prefer `dual=False`\nwhen n_samples &gt; n_features.\nFalse\n\n\n\ntol tol: float, default=1e-4\n\nTolerance for stopping criteria.\n0.0001\n\n\n\nfit_intercept fit_intercept: bool, default=True\n\nSpecifies if a constant (a.k.a. bias or intercept) should be\nadded to the decision function.\nTrue\n\n\n\nintercept_scaling intercept_scaling: float, default=1\n\nUseful only when the solver `liblinear` is used\nand `self.fit_intercept` is set to `True`. In this case, `x` becomes\n`[x, self.intercept_scaling]`,\ni.e. a \"synthetic\" feature with constant value equal to\n`intercept_scaling` is appended to the instance vector.\nThe intercept becomes\n``intercept_scaling * synthetic_feature_weight``.\n\n.. note::\nThe synthetic feature weight is subject to L1 or L2\nregularization as all other features.\nTo lessen the effect of regularization on synthetic feature weight\n(and therefore on the intercept) `intercept_scaling` has to be increased.\n1\n\n\n\nclass_weight class_weight: dict or 'balanced', default=None\n\nWeights associated with classes in the form ``{class_label: weight}``.\nIf not given, all classes are supposed to have weight one.\n\nThe \"balanced\" mode uses the values of y to automatically adjust\nweights inversely proportional to class frequencies in the input data\nas ``n_samples / (n_classes * np.bincount(y))``.\n\nNote that these weights will be multiplied with sample_weight (passed\nthrough the fit method) if sample_weight is specified.\n\n.. versionadded:: 0.17\n*class_weight='balanced'*\nNone\n\n\n\nrandom_state random_state: int, RandomState instance, default=None\n\nUsed when ``solver`` == 'sag', 'saga' or 'liblinear' to shuffle the\ndata. See :term:`Glossary ` for details.\nNone\n\n\n\nsolver solver: {'lbfgs', 'liblinear', 'newton-cg', 'newton-cholesky', 'sag', 'saga'}, default='lbfgs'\n\nAlgorithm to use in the optimization problem. Default is 'lbfgs'.\nTo choose a solver, you might want to consider the following aspects:\n\n- 'lbfgs' is a good default solver because it works reasonably well for a wide\nclass of problems.\n- For :term:`multiclass` problems (`n_classes &gt;= 3`), all solvers except\n'liblinear' minimize the full multinomial loss, 'liblinear' will raise an\nerror.\n- 'newton-cholesky' is a good choice for\n`n_samples` &gt;&gt; `n_features * n_classes`, especially with one-hot encoded\ncategorical features with rare categories. Be aware that the memory usage\nof this solver has a quadratic dependency on `n_features * n_classes`\nbecause it explicitly computes the full Hessian matrix.\n- For small datasets, 'liblinear' is a good choice, whereas 'sag'\nand 'saga' are faster for large ones;\n- 'liblinear' can only handle binary classification by default. To apply a\none-versus-rest scheme for the multiclass setting one can wrap it with the\n:class:`~sklearn.multiclass.OneVsRestClassifier`.\n\n.. warning::\nThe choice of the algorithm depends on the penalty chosen (`l1_ratio=0`\nfor L2-penalty, `l1_ratio=1` for L1-penalty and `0 &lt; l1_ratio &lt; 1` for\nElastic-Net) and on (multinomial) multiclass support:\n\n================= ======================== ======================\nsolver l1_ratio multinomial multiclass\n================= ======================== ======================\n'lbfgs' l1_ratio=0 yes\n'liblinear' l1_ratio=1 or l1_ratio=0 no\n'newton-cg' l1_ratio=0 yes\n'newton-cholesky' l1_ratio=0 yes\n'sag' l1_ratio=0 yes\n'saga' 0&lt;=l1_ratio&lt;=1 yes\n================= ======================== ======================\n\n.. note::\n'sag' and 'saga' fast convergence is only guaranteed on features\nwith approximately the same scale. You can preprocess the data with\na scaler from :mod:`sklearn.preprocessing`.\n\n.. seealso::\nRefer to the :ref:`User Guide ` for more\ninformation regarding :class:`LogisticRegression` and more specifically the\n:ref:`Table `\nsummarizing solver/penalty supports.\n\n.. versionadded:: 0.17\nStochastic Average Gradient (SAG) descent solver. Multinomial support in\nversion 0.18.\n.. versionadded:: 0.19\nSAGA solver.\n.. versionchanged:: 0.22\nThe default solver changed from 'liblinear' to 'lbfgs' in 0.22.\n.. versionadded:: 1.2\nnewton-cholesky solver. Multinomial support in version 1.6.\n'lbfgs'\n\n\n\nmax_iter max_iter: int, default=100\n\nMaximum number of iterations taken for the solvers to converge.\n200\n\n\n\nverbose verbose: int, default=0\n\nFor the liblinear and lbfgs solvers set verbose to any positive\nnumber for verbosity.\n0\n\n\n\nwarm_start warm_start: bool, default=False\n\nWhen set to True, reuse the solution of the previous call to fit as\ninitialization, otherwise, just erase the previous solution.\nUseless for liblinear solver. See :term:`the Glossary `.\n\n.. versionadded:: 0.17\n*warm_start* to support *lbfgs*, *newton-cg*, *sag*, *saga* solvers.\nFalse\n\n\n\nn_jobs n_jobs: int, default=None\n\nDoes not have any effect.\n\n.. deprecated:: 1.8\n`n_jobs` is deprecated in version 1.8 and will be removed in 1.10.\nNone\n\n\n\n\n            \n        \n    \n\n\nTo podejście jest spójne z „dobrą praktyką” w ML: wszystkie transformacje uczące się parametrów (imputacja, skalowanie, kodowanie) są częścią pipeline i nie „podglądają” testu.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Przygotowanie i czyszczenie danych</span>"
    ]
  },
  {
    "objectID": "chapters/02-przygotowanie-danych.html#eda-i-preprocessing-danych",
    "href": "chapters/02-przygotowanie-danych.html#eda-i-preprocessing-danych",
    "title": "3  Przygotowanie i czyszczenie danych",
    "section": "3.3 EDA i preprocessing danych",
    "text": "3.3 EDA i preprocessing danych\nTen rozdział porządkuje praktyczny przebieg pracy „od surowego pliku do macierzy cech gotowej dla modelu”. Najpierw wykonujemy EDA (ang. exploratory data analysis), aby zrozumieć dane (struktura, rozkłady, zależności, potencjalne problemy jakościowe), a dopiero potem przechodzimy do preprocessingu, który ma zapewnić poprawne działanie modeli i powtarzalność całego procesu. Kwestie braków danych i obserwacji odstających są tu jedynie sygnalizowane (szczegółowe strategie były w osobnych podrozdziałach).\n\n3.3.1 Szybki „sanity check” po imporcie\nPo wczytaniu danych pierwszym krokiem jest szybka kontrola struktury: rozmiar zbioru, typy kolumn, podstawowe podsumowania i liczba braków. W tym momencie warto również ujednolicić nazwy kolumn (np. snake_case), aby dalszy kod był czytelny i odporny na błędy.\n\n\nKod\ndf = pd.read_csv(\n    \"data.csv\",\n    na_values=[\"\", \"NA\", \"N/A\", \"null\", \"None\"],\n    parse_dates=[\"signup_date\", \"last_purchase_date\"],\n    encoding=\"utf-8\"\n)\n\ndf.shape\ndf.info()\ndf.head()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 350 entries, 0 to 349\nData columns (total 15 columns):\n #   Column              Non-Null Count  Dtype         \n---  ------              --------------  -----         \n 0   customer_id         350 non-null    int64         \n 1   signup_date         350 non-null    datetime64[ns]\n 2   last_purchase_date  350 non-null    datetime64[ns]\n 3   age                 333 non-null    float64       \n 4   income_eur          317 non-null    float64       \n 5   city                333 non-null    object        \n 6   segment             350 non-null    object        \n 7   device_type         329 non-null    object        \n 8   visits_last30       350 non-null    float64       \n 9   avg_basket_eur      332 non-null    float64       \n 10  returns_last90      350 non-null    float64       \n 11  has_promo           350 non-null    bool          \n 12  satisfaction_1_5    327 non-null    float64       \n 13  churned             350 non-null    int64         \n 14  notes               334 non-null    object        \ndtypes: bool(1), datetime64[ns](2), float64(6), int64(2), object(4)\nmemory usage: 38.8+ KB\n\n\n\n\n\n\n\n\n\ncustomer_id\nsignup_date\nlast_purchase_date\nage\nincome_eur\ncity\nsegment\ndevice_type\nvisits_last30\navg_basket_eur\nreturns_last90\nhas_promo\nsatisfaction_1_5\nchurned\nnotes\n\n\n\n\n0\n100000\n2022-04-08\n2022-10-19\nNaN\n72955.0\nWarszawa\nB2B\ndesktop\n8.0\n39.88\n1.0\nFalse\n4.0\n0\nklient powracający\n\n\n1\n100001\n2024-04-27\n2024-05-13\n52.0\n24863.0\nKraków\nB2C\nmobile\n12.0\n67.80\n1.0\nFalse\n3.0\n0\nbrak uwag\n\n\n2\n100002\n2023-12-18\n2024-04-05\nNaN\n25333.0\nWrocław\nB2C\ndesktop\n6.0\n27.04\n1.0\nTrue\n4.0\n0\nreklamacja\n\n\n3\n100003\n2023-04-26\n2023-10-01\n16.0\nNaN\nŁódź\nB2B\nmobile\n6.0\n149.03\n1.0\nFalse\n3.0\n1\nbrak uwag\n\n\n4\n100004\n2023-04-20\n2024-04-07\n42.0\n27954.0\nWarszawa\nB2C\ndesktop\n7.0\n49.73\n1.0\nTrue\n3.0\n0\nNaN\n\n\n\n\n\n\n\n\n\nKod\n# szybki przegląd braków\n(df.isna().mean().sort_values(ascending=False) * 100).round(2)\n\n\nincome_eur            9.43\nsatisfaction_1_5      6.57\ndevice_type           6.00\navg_basket_eur        5.14\nage                   4.86\ncity                  4.86\nnotes                 4.57\ncustomer_id           0.00\nsignup_date           0.00\nlast_purchase_date    0.00\nsegment               0.00\nvisits_last30         0.00\nreturns_last90        0.00\nhas_promo             0.00\nchurned               0.00\ndtype: float64\n\n\nNa tym etapie sprawdzamy też duplikaty oraz ewentualne naruszenia prostych reguł domenowych (np. ujemne wartości tam, gdzie nie powinny wystąpić). Jeśli widzimy ewidentne błędy, zazwyczaj oznaczamy je do późniejszej korekty, zamiast natychmiast usuwać dane „w ciemno”.\n\n\nKod\ndf.duplicated().sum()\n\n\nnp.int64(0)\n\n\n\n\n3.3.2 EDA - rozkłady i struktura zmiennych\nEDA zaczynamy od uporządkowania typów zmiennych: rozdzielenia zmiennych liczbowych i kategorycznych oraz rozpoznania, które kolumny są zmiennymi docelowymi, identyfikatorami lub metadanymi (np. daty). W naszym data.csv naturalnie wyróżniają się liczby (np. income_eur, avg_basket_eur), kategorie (np. segment, city) i daty.\n\n\nKod\nnum_cols = df.select_dtypes(include=\"number\").columns.tolist()\ncat_cols = df.select_dtypes(include=[\"object\", \"string\", \"bool\"]).columns.tolist()\n\nnum_cols, cat_cols\n\n\n(['customer_id',\n  'age',\n  'income_eur',\n  'visits_last30',\n  'avg_basket_eur',\n  'returns_last90',\n  'satisfaction_1_5',\n  'churned'],\n ['city', 'segment', 'device_type', 'has_promo', 'notes'])\n\n\nW analizie rozkładów dla zmiennych liczbowych bardzo dobre są histogramy z nakładką gęstości oraz wykresy pudełkowe. Warto pamiętać, że część zmiennych może być skośna (np. dochody), więc czasem sensowne jest pokazanie wykresu w skali logarytmicznej.\n\n\nKod\nimport matplotlib.pyplot as plt\n\ndef univariate_view(series, title):\n    fig = plt.figure(figsize=(10, 4))\n    gs = fig.add_gridspec(1, 2, width_ratios=[3, 1])\n\n    ax1 = fig.add_subplot(gs[0, 0])\n    ax1.hist(series.dropna(), bins=30)\n    ax1.set_title(f\"Histogram: {title}\")\n    ax1.set_xlabel(title)\n    ax1.set_ylabel(\"Liczność\")\n\n    ax2 = fig.add_subplot(gs[0, 1])\n    ax2.boxplot(series.dropna(), vert=True)\n    ax2.set_title(\"Boxplot\")\n    ax2.set_xticks([])\n\n    plt.tight_layout()\n    plt.show()\n\nunivariate_view(df[\"income_eur\"], \"income_eur\")\nunivariate_view(df[\"avg_basket_eur\"], \"avg_basket_eur\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDla zmiennych kategorycznych kluczowe jest porównanie częstości. Prosty wykres słupkowy umożliwia identyfikację rzadkich kategorii oraz niespójności kodowania (np. różne wielkości liter, dodatkowe spacje).\n\n\nKod\ndef top_categories(series, title, top=10):\n    counts = series.value_counts(dropna=False).head(top)\n    plt.figure(figsize=(10, 4))\n    plt.bar(counts.index.astype(str), counts.values)\n    plt.title(f\"Top {top} kategorii: {title}\")\n    plt.xticks(rotation=30, ha=\"right\")\n    plt.ylabel(\"Liczność\")\n    plt.tight_layout()\n    plt.show()\n\ntop_categories(df[\"segment\"], \"segment\")\ntop_categories(df[\"device_type\"], \"device_type\")\ntop_categories(df[\"city\"], \"city\", top=12)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.3.3 EDA - zależności między zmiennymi\nW celu odkrycia zależności pomiędzy cechami wykreślamy macierz korelacji.\n\n\nKod\n# macierz korelacji (numeryczne)\ncorr = df[num_cols].corr(numeric_only=True)\n\nfig, ax = plt.subplots(figsize=(8, 6))\nim = ax.imshow(corr, aspect=\"auto\")\nfig.colorbar(im, ax=ax)\n\nax.set_xticks(range(len(corr.columns)))\nax.set_xticklabels(corr.columns, rotation=30, ha=\"right\")\nax.set_yticks(range(len(corr.index)))\nax.set_yticklabels(corr.index)\nax.set_title(\"Macierz korelacji (numeryczne)\")\n\n# wartości korelacji w komórkach\nfor i in range(corr.shape[0]):\n    for j in range(corr.shape[1]):\n        val = corr.iloc[i, j]\n        color = \"black\" if abs(val) &gt;= 0.5 else \"white\"\n        ax.text(j, i, f\"{val:.2f}\", ha=\"center\", va=\"center\", color=color, fontsize=8)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nW kontekście klasyfikacji warto też zobaczyć, jak rozkłady cech różnią się między klasami (tu: churned). Bardzo czytelny jest wykres pudełkowy/wiolinowy „cecha vs klasa” albo porównanie histogramów. Poniżej przykład boxplotu cechy w podziale na churned.\n\n\nKod\ndef box_by_target(df, feature, target=\"churned\"):\n    groups = [df.loc[df[target] == t, feature].dropna() for t in sorted(df[target].dropna().unique())]\n    plt.figure(figsize=(7, 4))\n    plt.boxplot(groups, labels=[f\"{target}={t}\" for t in sorted(df[target].dropna().unique())])\n    plt.title(f\"{feature} względem {target}\")\n    plt.ylabel(feature)\n    plt.tight_layout()\n    plt.show()\n\nbox_by_target(df, \"avg_basket_eur\")\nbox_by_target(df, \"visits_last30\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDla zmiennych kategorycznych przydatne są wykresy udziałów klas w kategoriach (np. wskaźnik churn w segmentach). W prostym wariancie liczymy średnią churned w grupach (to jest odsetek „1” w danej kategorii) i wykreślamy słupki.\n\n\nKod\nrate = df.groupby(\"segment\")[\"churned\"].mean().sort_values(ascending=False)\n\nplt.figure(figsize=(7, 4))\nplt.bar(rate.index.astype(str), rate.values)\nplt.title(\"Średni churn w segmentach\")\nplt.ylabel(\"P(churned=1)\")\nplt.xticks(rotation=20, ha=\"right\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nJeżeli w danych występują daty, dobrze jest wykonać podstawowe przekroje czasowe: liczba rejestracji w czasie, średni koszyk w czasie, itp. Pozwala to zidentyfikować sezonowość, zmiany procesu zbierania danych, kampanie marketingowe.\n\n\nKod\ntmp = df.copy()\ntmp[\"signup_month\"] = tmp[\"signup_date\"].dt.to_period(\"M\").dt.to_timestamp()\n\ncounts = tmp.groupby(\"signup_month\")[\"customer_id\"].count()\n\nplt.figure(figsize=(10, 4))\nplt.plot(counts.index, counts.values)\nplt.title(\"Liczba rejestracji w czasie\")\nplt.ylabel(\"Liczność\")\nplt.xlabel(\"Miesiąc\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nNa tym etapie jedynie notujemy, czy widać: braki danych (np. całe grupy z brakami), wartości podejrzanie ekstremalne lub naruszenia domeny. Szczegółowe postępowanie (imputacja, outliery) realizujemy według strategii opisanych w dedykowanych podrozdziałach.\n\n3.3.3.1 Narzędzia i biblioteki\nW praktycznej EDA najczęściej bazuje się na zestawie narzędzi „rdzeniowych”: pandas służy do przeglądu struktury danych, typów, braków, agregacji i podstawowych statystyk, natomiast matplotlib i seaborn odpowiadają za wizualizacje. matplotlib daje pełną kontrolę nad wykresem i jest dobry do precyzyjnych, publikacyjnych rysunków, a seaborn pozwala szybciej tworzyć estetyczne wykresy statystyczne typowe dla EDA (rozkłady, zależności, porównania grup). Jeżeli zależy Ci na interaktywności (zoom, podgląd wartości, selekcja), warto sięgnąć po plotly albo altair, które dobrze sprawdzają się w notebookach i podczas zajęć, bo ułatwiają „badanie danych w locie”.\nDo szybkiej diagnostyki całego zbioru danych, bez ręcznego pisania wielu wykresów i tabel, przydatne są biblioteki raportujące: ydata-profiling (dawniej pandas-profiling) oraz sweetviz. Generują one raporty HTML z podsumowaniami rozkładów, braków, korelacji i potencjalnych problemów jakościowych, a sweetviz dodatkowo dobrze nadaje się do porównywania zbiorów (np. train vs test) lub analiz w odniesieniu do zmiennej docelowej. Z kolei dtale daje interfejs „jak arkusz kalkulacyjny”, umożliwiając interaktywne filtrowanie i szybkie sprawdzanie danych w przeglądarce, co bywa bardzo wygodne w dydaktyce i debugowaniu.\nJeżeli chcemy rozszerzyć EDA o wątki jakości danych i zależności specyficznych dla danych mieszanych, pomocne są narzędzia wyspecjalizowane: missingno służy do wizualizacji wzorców braków (gdzie i jak współwystępują), great_expectations pozwala formalizować reguły jakości danych i testować je w sposób powtarzalny, a phik bywa użyteczne do badania zależności również wtedy, gdy zmienne są kategoryczne lub relacje są nieliniowe. W praktyce dobry „stos” do Twojego kursu to: pandas + (matplotlib/seaborn) jako fundament, missingno do braków oraz ydata-profiling lub sweetviz do szybkiego raportu całości.\n\n\n\n3.3.4 Preprocessing - przygotowanie macierzy cech do modeli\nPo EDA przechodzimy do preprocessingu, którego cel jest stricte „modelowy”: wytworzyć wejście w formacie akceptowalnym przez algorytmy oraz zapewnić powtarzalność transformacji. Kluczowa zasada brzmi: transformacje, które uczą się parametrów (imputer, scaler, encoder), dopasowujemy na zbiorze treningowym i stosujemy do walidacji/testu – najlepiej w pipeline.\n\n3.3.4.1 Podział na zbiory treningowy i testowy\nPodział na train/test wykonujemy przed dopasowaniem transformacji. Dla klasyfikacji zwykle stosujemy stratyfikację.\n\n\nKod\nX = df.drop(columns=[\"churned\"])\ny = df[\"churned\"]\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\n\n\nJeżeli dodatkowo używamy walidacji, możemy wydzielić X_val z X_train analogicznie albo korzystać z cross-validation.\n\n\n3.3.4.2 Kodowanie kategorii\nWiększość klasycznych modeli (regresja logistyczna, SVM, LDA/QDA) wymaga liczb, więc zmienne kategoryczne kodujemy. Standardem jest one-hot encoding z obsługą nieznanych kategorii w teście.\n\n\n3.3.4.3 Skalowanie zmiennych liczbowych\nSkalowanie jest szczególnie ważne dla metod opartych na odległości i marginesie (k-NN, SVM) oraz dla modeli liniowych przy regularyzacji. Dla drzew i metod zespołowych zwykle nie jest konieczne, ale utrzymywanie jednolitego pipeline’u ułatwia porównania.\n\n\n3.3.4.4 Pipeline preprocessingu\nPoniżej wzorcowa konstrukcja preprocessingu: dla liczb imputacja (tu jako placeholder) + skalowanie; dla kategorii imputacja (placeholder) + one-hot encoding. Strategię imputacji i obsługę outlierów możesz później łatwo podmienić.\n\n\nKod\nfrom sklearn.preprocessing import StandardScaler\n\nnum_cols = [\"age\", \"income_eur\", \"visits_last30\", \"avg_basket_eur\", \"returns_last90\", \"satisfaction_1_5\"]\ncat_cols = [\"city\", \"segment\", \"device_type\", \"has_promo\", \"notes\"]\n\nnumeric_pipe = Pipeline(steps=[\n    (\"imputer\", SimpleImputer(strategy=\"median\")),   # szczegóły w rozdziale o imputacji\n    (\"scaler\", StandardScaler())\n])\n\ncategorical_pipe = Pipeline(steps=[\n    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n])\n\npreprocess = ColumnTransformer(\n    transformers=[\n        (\"num\", numeric_pipe, num_cols),\n        (\"cat\", categorical_pipe, cat_cols),\n    ],\n    remainder=\"drop\"\n)\n\n\nNastępnie łączymy preprocessing z modelem w jeden pipeline. Dzięki temu cały proces jest powtarzalny i bezpieczny względem testu:\n\n\nKod\nclf = Pipeline(steps=[\n    (\"prep\", preprocess),\n    (\"model\", LogisticRegression(max_iter=300))\n])\n\nclf.fit(X_train, y_train)\n\n\nPipeline(steps=[('prep',\n                 ColumnTransformer(transformers=[('num',\n                                                  Pipeline(steps=[('imputer',\n                                                                   SimpleImputer(strategy='median')),\n                                                                  ('scaler',\n                                                                   StandardScaler())]),\n                                                  ['age', 'income_eur',\n                                                   'visits_last30',\n                                                   'avg_basket_eur',\n                                                   'returns_last90',\n                                                   'satisfaction_1_5']),\n                                                 ('cat',\n                                                  Pipeline(steps=[('imputer',\n                                                                   SimpleImputer(strategy='most_frequent')),\n                                                                  ('onehot',\n                                                                   OneHotEncoder(handle_unknown='ignore'))]),\n                                                  ['city', 'segment',\n                                                   'device_type', 'has_promo',\n                                                   'notes'])])),\n                ('model', LogisticRegression(max_iter=300))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiFitted\n        \n            \n                Parameters\n                \n\n\n\n\n\n\n\n\n\nsteps steps: list of tuples\n\nList of (name of step, estimator) tuples that are to be chained in\nsequential order. To be compatible with the scikit-learn API, all steps\nmust define `fit`. All non-last steps must also define `transform`. See\n:ref:`Combining Estimators ` for more details.\n[('prep', ...), ('model', ...)]\n\n\n\ntransform_input transform_input: list of str, default=None\n\nThe names of the :term:`metadata` parameters that should be transformed by the\npipeline before passing it to the step consuming it.\n\nThis enables transforming some input arguments to ``fit`` (other than ``X``)\nto be transformed by the steps of the pipeline up to the step which requires\nthem. Requirement is defined via :ref:`metadata routing `.\nFor instance, this can be used to pass a validation set through the pipeline.\n\nYou can only set this if metadata routing is enabled, which you\ncan enable using ``sklearn.set_config(enable_metadata_routing=True)``.\n\n.. versionadded:: 1.6\nNone\n\n\n\nmemory memory: str or object with the joblib.Memory interface, default=None\n\nUsed to cache the fitted transformers of the pipeline. The last step\nwill never be cached, even if it is a transformer. By default, no\ncaching is performed. If a string is given, it is the path to the\ncaching directory. Enabling caching triggers a clone of the transformers\nbefore fitting. Therefore, the transformer instance given to the\npipeline cannot be inspected directly. Use the attribute ``named_steps``\nor ``steps`` to inspect estimators within the pipeline. Caching the\ntransformers is advantageous when fitting is time consuming. See\n:ref:`sphx_glr_auto_examples_neighbors_plot_caching_nearest_neighbors.py`\nfor an example on how to enable caching.\nNone\n\n\n\nverbose verbose: bool, default=False\n\nIf True, the time elapsed while fitting each step will be printed as it\nis completed.\nFalse\n\n\n\n\n            \n        \n    prep: ColumnTransformer?Documentation for prep: ColumnTransformer\n        \n            \n                Parameters\n                \n\n\n\n\n\n\n\n\n\ntransformers transformers: list of tuples\n\nList of (name, transformer, columns) tuples specifying the\ntransformer objects to be applied to subsets of the data.\n\nname : str\nLike in Pipeline and FeatureUnion, this allows the transformer and\nits parameters to be set using ``set_params`` and searched in grid\nsearch.\ntransformer : {'drop', 'passthrough'} or estimator\nEstimator must support :term:`fit` and :term:`transform`.\nSpecial-cased strings 'drop' and 'passthrough' are accepted as\nwell, to indicate to drop the columns or to pass them through\nuntransformed, respectively.\ncolumns : str, array-like of str, int, array-like of int, array-like of bool, slice or callable\nIndexes the data on its second axis. Integers are interpreted as\npositional columns, while strings can reference DataFrame columns\nby name. A scalar string or int should be used where\n``transformer`` expects X to be a 1d array-like (vector),\notherwise a 2d array will be passed to the transformer.\nA callable is passed the input data `X` and can return any of the\nabove. To select multiple columns by name or dtype, you can use\n:obj:`make_column_selector`.\n[('num', ...), ('cat', ...)]\n\n\n\nremainder remainder: {'drop', 'passthrough'} or estimator, default='drop'\n\nBy default, only the specified columns in `transformers` are\ntransformed and combined in the output, and the non-specified\ncolumns are dropped. (default of ``'drop'``).\nBy specifying ``remainder='passthrough'``, all remaining columns that\nwere not specified in `transformers`, but present in the data passed\nto `fit` will be automatically passed through. This subset of columns\nis concatenated with the output of the transformers. For dataframes,\nextra columns not seen during `fit` will be excluded from the output\nof `transform`.\nBy setting ``remainder`` to be an estimator, the remaining\nnon-specified columns will use the ``remainder`` estimator. The\nestimator must support :term:`fit` and :term:`transform`.\nNote that using this feature requires that the DataFrame columns\ninput at :term:`fit` and :term:`transform` have identical order.\n'drop'\n\n\n\nsparse_threshold sparse_threshold: float, default=0.3\n\nIf the output of the different transformers contains sparse matrices,\nthese will be stacked as a sparse matrix if the overall density is\nlower than this value. Use ``sparse_threshold=0`` to always return\ndense. When the transformed output consists of all dense data, the\nstacked result will be dense, and this keyword will be ignored.\n0.3\n\n\n\nn_jobs n_jobs: int, default=None\n\nNumber of jobs to run in parallel.\n``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n``-1`` means using all processors. See :term:`Glossary `\nfor more details.\nNone\n\n\n\ntransformer_weights transformer_weights: dict, default=None\n\nMultiplicative weights for features per transformer. The output of the\ntransformer is multiplied by these weights. Keys are transformer names,\nvalues the weights.\nNone\n\n\n\nverbose verbose: bool, default=False\n\nIf True, the time elapsed while fitting each transformer will be\nprinted as it is completed.\nFalse\n\n\n\nverbose_feature_names_out verbose_feature_names_out: bool, str or Callable[[str, str], str], default=True\n\n- If True, :meth:`ColumnTransformer.get_feature_names_out` will prefix\nall feature names with the name of the transformer that generated that\nfeature. It is equivalent to setting\n`verbose_feature_names_out=\"{transformer_name}__{feature_name}\"`.\n- If False, :meth:`ColumnTransformer.get_feature_names_out` will not\nprefix any feature names and will error if feature names are not\nunique.\n- If ``Callable[[str, str], str]``,\n:meth:`ColumnTransformer.get_feature_names_out` will rename all the features\nusing the name of the transformer. The first argument of the callable is the\ntransformer name and the second argument is the feature name. The returned\nstring will be the new feature name.\n- If ``str``, it must be a string ready for formatting. The given string will\nbe formatted using two field names: ``transformer_name`` and ``feature_name``.\ne.g. ``\"{feature_name}__{transformer_name}\"``. See :meth:`str.format` method\nfrom the standard library for more info.\n\n.. versionadded:: 1.0\n\n.. versionchanged:: 1.6\n`verbose_feature_names_out` can be a callable or a string to be formatted.\nTrue\n\n\n\nforce_int_remainder_cols force_int_remainder_cols: bool, default=False\n\nThis parameter has no effect.\n\n.. note::\nIf you do not access the list of columns for the remainder columns\nin the `transformers_` fitted attribute, you do not need to set\nthis parameter.\n\n.. versionadded:: 1.5\n\n.. versionchanged:: 1.7\nThe default value for `force_int_remainder_cols` will change from\n`True` to `False` in version 1.7.\n\n.. deprecated:: 1.7\n`force_int_remainder_cols` is deprecated and will be removed in 1.9.\n'deprecated'\n\n\n\n\n            \n        \n    num['age', 'income_eur', 'visits_last30', 'avg_basket_eur', 'returns_last90', 'satisfaction_1_5']SimpleImputer?Documentation for SimpleImputer\n        \n            \n                Parameters\n                \n\n\n\n\n\n\n\n\n\nmissing_values missing_values: int, float, str, np.nan, None or pandas.NA, default=np.nan\n\nThe placeholder for the missing values. All occurrences of\n`missing_values` will be imputed. For pandas' dataframes with\nnullable integer dtypes with missing values, `missing_values`\ncan be set to either `np.nan` or `pd.NA`.\nnan\n\n\n\nstrategy strategy: str or Callable, default='mean'\n\nThe imputation strategy.\n\n- If \"mean\", then replace missing values using the mean along\neach column. Can only be used with numeric data.\n- If \"median\", then replace missing values using the median along\neach column. Can only be used with numeric data.\n- If \"most_frequent\", then replace missing using the most frequent\nvalue along each column. Can be used with strings or numeric data.\nIf there is more than one such value, only the smallest is returned.\n- If \"constant\", then replace missing values with fill_value. Can be\nused with strings or numeric data.\n- If an instance of Callable, then replace missing values using the\nscalar statistic returned by running the callable over a dense 1d\narray containing non-missing values of each column.\n\n.. versionadded:: 0.20\nstrategy=\"constant\" for fixed value imputation.\n\n.. versionadded:: 1.5\nstrategy=callable for custom value imputation.\n'median'\n\n\n\nfill_value fill_value: str or numerical value, default=None\n\nWhen strategy == \"constant\", `fill_value` is used to replace all\noccurrences of missing_values. For string or object data types,\n`fill_value` must be a string.\nIf `None`, `fill_value` will be 0 when imputing numerical\ndata and \"missing_value\" for strings or object data types.\nNone\n\n\n\ncopy copy: bool, default=True\n\nIf True, a copy of X will be created. If False, imputation will\nbe done in-place whenever possible. Note that, in the following cases,\na new copy will always be made, even if `copy=False`:\n\n- If `X` is not an array of floating values;\n- If `X` is encoded as a CSR matrix;\n- If `add_indicator=True`.\nTrue\n\n\n\nadd_indicator add_indicator: bool, default=False\n\nIf True, a :class:`MissingIndicator` transform will stack onto output\nof the imputer's transform. This allows a predictive estimator\nto account for missingness despite imputation. If a feature has no\nmissing values at fit/train time, the feature won't appear on\nthe missing indicator even if there are missing values at\ntransform/test time.\nFalse\n\n\n\nkeep_empty_features keep_empty_features: bool, default=False\n\nIf True, features that consist exclusively of missing values when\n`fit` is called are returned in results when `transform` is called.\nThe imputed value is always `0` except when `strategy=\"constant\"`\nin which case `fill_value` will be used instead.\n\n.. versionadded:: 1.2\nFalse\n\n\n\n\n            \n        \n    StandardScaler?Documentation for StandardScaler\n        \n            \n                Parameters\n                \n\n\n\n\n\n\n\n\n\ncopy copy: bool, default=True\n\nIf False, try to avoid a copy and do inplace scaling instead.\nThis is not guaranteed to always work inplace; e.g. if the data is\nnot a NumPy array or scipy.sparse CSR matrix, a copy may still be\nreturned.\nTrue\n\n\n\nwith_mean with_mean: bool, default=True\n\nIf True, center the data before scaling.\nThis does not work (and will raise an exception) when attempted on\nsparse matrices, because centering them entails building a dense\nmatrix which in common use cases is likely to be too large to fit in\nmemory.\nTrue\n\n\n\nwith_std with_std: bool, default=True\n\nIf True, scale the data to unit variance (or equivalently,\nunit standard deviation).\nTrue\n\n\n\n\n            \n        \n    cat['city', 'segment', 'device_type', 'has_promo', 'notes']SimpleImputer?Documentation for SimpleImputer\n        \n            \n                Parameters\n                \n\n\n\n\n\n\n\n\n\nmissing_values missing_values: int, float, str, np.nan, None or pandas.NA, default=np.nan\n\nThe placeholder for the missing values. All occurrences of\n`missing_values` will be imputed. For pandas' dataframes with\nnullable integer dtypes with missing values, `missing_values`\ncan be set to either `np.nan` or `pd.NA`.\nnan\n\n\n\nstrategy strategy: str or Callable, default='mean'\n\nThe imputation strategy.\n\n- If \"mean\", then replace missing values using the mean along\neach column. Can only be used with numeric data.\n- If \"median\", then replace missing values using the median along\neach column. Can only be used with numeric data.\n- If \"most_frequent\", then replace missing using the most frequent\nvalue along each column. Can be used with strings or numeric data.\nIf there is more than one such value, only the smallest is returned.\n- If \"constant\", then replace missing values with fill_value. Can be\nused with strings or numeric data.\n- If an instance of Callable, then replace missing values using the\nscalar statistic returned by running the callable over a dense 1d\narray containing non-missing values of each column.\n\n.. versionadded:: 0.20\nstrategy=\"constant\" for fixed value imputation.\n\n.. versionadded:: 1.5\nstrategy=callable for custom value imputation.\n'most_frequent'\n\n\n\nfill_value fill_value: str or numerical value, default=None\n\nWhen strategy == \"constant\", `fill_value` is used to replace all\noccurrences of missing_values. For string or object data types,\n`fill_value` must be a string.\nIf `None`, `fill_value` will be 0 when imputing numerical\ndata and \"missing_value\" for strings or object data types.\nNone\n\n\n\ncopy copy: bool, default=True\n\nIf True, a copy of X will be created. If False, imputation will\nbe done in-place whenever possible. Note that, in the following cases,\na new copy will always be made, even if `copy=False`:\n\n- If `X` is not an array of floating values;\n- If `X` is encoded as a CSR matrix;\n- If `add_indicator=True`.\nTrue\n\n\n\nadd_indicator add_indicator: bool, default=False\n\nIf True, a :class:`MissingIndicator` transform will stack onto output\nof the imputer's transform. This allows a predictive estimator\nto account for missingness despite imputation. If a feature has no\nmissing values at fit/train time, the feature won't appear on\nthe missing indicator even if there are missing values at\ntransform/test time.\nFalse\n\n\n\nkeep_empty_features keep_empty_features: bool, default=False\n\nIf True, features that consist exclusively of missing values when\n`fit` is called are returned in results when `transform` is called.\nThe imputed value is always `0` except when `strategy=\"constant\"`\nin which case `fill_value` will be used instead.\n\n.. versionadded:: 1.2\nFalse\n\n\n\n\n            \n        \n    OneHotEncoder?Documentation for OneHotEncoder\n        \n            \n                Parameters\n                \n\n\n\n\n\n\n\n\n\ncategories categories: 'auto' or a list of array-like, default='auto'\n\nCategories (unique values) per feature:\n\n- 'auto' : Determine categories automatically from the training data.\n- list : ``categories[i]`` holds the categories expected in the ith\ncolumn. The passed categories should not mix strings and numeric\nvalues within a single feature, and should be sorted in case of\nnumeric values.\n\nThe used categories can be found in the ``categories_`` attribute.\n\n.. versionadded:: 0.20\n'auto'\n\n\n\ndrop drop: {'first', 'if_binary'} or an array-like of shape (n_features,), default=None\n\nSpecifies a methodology to use to drop one of the categories per\nfeature. This is useful in situations where perfectly collinear\nfeatures cause problems, such as when feeding the resulting data\ninto an unregularized linear regression model.\n\nHowever, dropping one category breaks the symmetry of the original\nrepresentation and can therefore induce a bias in downstream models,\nfor instance for penalized linear classification or regression models.\n\n- None : retain all features (the default).\n- 'first' : drop the first category in each feature. If only one\ncategory is present, the feature will be dropped entirely.\n- 'if_binary' : drop the first category in each feature with two\ncategories. Features with 1 or more than 2 categories are\nleft intact.\n- array : ``drop[i]`` is the category in feature ``X[:, i]`` that\nshould be dropped.\n\nWhen `max_categories` or `min_frequency` is configured to group\ninfrequent categories, the dropping behavior is handled after the\ngrouping.\n\n.. versionadded:: 0.21\nThe parameter `drop` was added in 0.21.\n\n.. versionchanged:: 0.23\nThe option `drop='if_binary'` was added in 0.23.\n\n.. versionchanged:: 1.1\nSupport for dropping infrequent categories.\nNone\n\n\n\nsparse_output sparse_output: bool, default=True\n\nWhen ``True``, it returns a :class:`scipy.sparse.csr_matrix`,\ni.e. a sparse matrix in \"Compressed Sparse Row\" (CSR) format.\n\n.. versionadded:: 1.2\n`sparse` was renamed to `sparse_output`\nTrue\n\n\n\ndtype dtype: number type, default=np.float64\n\nDesired dtype of output.\n&lt;class 'numpy.float64'&gt;\n\n\n\nhandle_unknown handle_unknown: {'error', 'ignore', 'infrequent_if_exist', 'warn'}, default='error'\n\nSpecifies the way unknown categories are handled during :meth:`transform`.\n\n- 'error' : Raise an error if an unknown category is present during transform.\n- 'ignore' : When an unknown category is encountered during\ntransform, the resulting one-hot encoded columns for this feature\nwill be all zeros. In the inverse transform, an unknown category\nwill be denoted as None.\n- 'infrequent_if_exist' : When an unknown category is encountered\nduring transform, the resulting one-hot encoded columns for this\nfeature will map to the infrequent category if it exists. The\ninfrequent category will be mapped to the last position in the\nencoding. During inverse transform, an unknown category will be\nmapped to the category denoted `'infrequent'` if it exists. If the\n`'infrequent'` category does not exist, then :meth:`transform` and\n:meth:`inverse_transform` will handle an unknown category as with\n`handle_unknown='ignore'`. Infrequent categories exist based on\n`min_frequency` and `max_categories`. Read more in the\n:ref:`User Guide `.\n- 'warn' : When an unknown category is encountered during transform\na warning is issued, and the encoding then proceeds as described for\n`handle_unknown=\"infrequent_if_exist\"`.\n\n.. versionchanged:: 1.1\n`'infrequent_if_exist'` was added to automatically handle unknown\ncategories and infrequent categories.\n\n.. versionadded:: 1.6\nThe option `\"warn\"` was added in 1.6.\n'ignore'\n\n\n\nmin_frequency min_frequency: int or float, default=None\n\nSpecifies the minimum frequency below which a category will be\nconsidered infrequent.\n\n- If `int`, categories with a smaller cardinality will be considered\ninfrequent.\n\n- If `float`, categories with a smaller cardinality than\n`min_frequency * n_samples` will be considered infrequent.\n\n.. versionadded:: 1.1\nRead more in the :ref:`User Guide `.\nNone\n\n\n\nmax_categories max_categories: int, default=None\n\nSpecifies an upper limit to the number of output features for each input\nfeature when considering infrequent categories. If there are infrequent\ncategories, `max_categories` includes the category representing the\ninfrequent categories along with the frequent categories. If `None`,\nthere is no limit to the number of output features.\n\n.. versionadded:: 1.1\nRead more in the :ref:`User Guide `.\nNone\n\n\n\nfeature_name_combiner feature_name_combiner: \"concat\" or callable, default=\"concat\"\n\nCallable with signature `def callable(input_feature, category)` that returns a\nstring. This is used to create feature names to be returned by\n:meth:`get_feature_names_out`.\n\n`\"concat\"` concatenates encoded feature name and category with\n`feature + \"_\" + str(category)`.E.g. feature X with values 1, 6, 7 create\nfeature names `X_1, X_6, X_7`.\n\n.. versionadded:: 1.3\n'concat'\n\n\n\n\n            \n        \n    LogisticRegression?Documentation for LogisticRegression\n        \n            \n                Parameters\n                \n\n\n\n\n\n\n\n\n\npenalty penalty: {'l1', 'l2', 'elasticnet', None}, default='l2'\n\nSpecify the norm of the penalty:\n\n- `None`: no penalty is added;\n- `'l2'`: add a L2 penalty term and it is the default choice;\n- `'l1'`: add a L1 penalty term;\n- `'elasticnet'`: both L1 and L2 penalty terms are added.\n\n.. warning::\nSome penalties may not work with some solvers. See the parameter\n`solver` below, to know the compatibility between the penalty and\nsolver.\n\n.. versionadded:: 0.19\nl1 penalty with SAGA solver (allowing 'multinomial' + L1)\n\n.. deprecated:: 1.8\n`penalty` was deprecated in version 1.8 and will be removed in 1.10.\nUse `l1_ratio` instead. `l1_ratio=0` for `penalty='l2'`, `l1_ratio=1` for\n`penalty='l1'` and `l1_ratio` set to any float between 0 and 1 for\n`'penalty='elasticnet'`.\n'deprecated'\n\n\n\nC C: float, default=1.0\n\nInverse of regularization strength; must be a positive float.\nLike in support vector machines, smaller values specify stronger\nregularization. `C=np.inf` results in unpenalized logistic regression.\nFor a visual example on the effect of tuning the `C` parameter\nwith an L1 penalty, see:\n:ref:`sphx_glr_auto_examples_linear_model_plot_logistic_path.py`.\n1.0\n\n\n\nl1_ratio l1_ratio: float, default=0.0\n\nThe Elastic-Net mixing parameter, with `0 &lt;= l1_ratio &lt;= 1`. Setting\n`l1_ratio=1` gives a pure L1-penalty, setting `l1_ratio=0` a pure L2-penalty.\nAny value between 0 and 1 gives an Elastic-Net penalty of the form\n`l1_ratio * L1 + (1 - l1_ratio) * L2`.\n\n.. warning::\nCertain values of `l1_ratio`, i.e. some penalties, may not work with some\nsolvers. See the parameter `solver` below, to know the compatibility between\nthe penalty and solver.\n\n.. versionchanged:: 1.8\nDefault value changed from None to 0.0.\n\n.. deprecated:: 1.8\n`None` is deprecated and will be removed in version 1.10. Always use\n`l1_ratio` to specify the penalty type.\n0.0\n\n\n\ndual dual: bool, default=False\n\nDual (constrained) or primal (regularized, see also\n:ref:`this equation `) formulation. Dual formulation\nis only implemented for l2 penalty with liblinear solver. Prefer `dual=False`\nwhen n_samples &gt; n_features.\nFalse\n\n\n\ntol tol: float, default=1e-4\n\nTolerance for stopping criteria.\n0.0001\n\n\n\nfit_intercept fit_intercept: bool, default=True\n\nSpecifies if a constant (a.k.a. bias or intercept) should be\nadded to the decision function.\nTrue\n\n\n\nintercept_scaling intercept_scaling: float, default=1\n\nUseful only when the solver `liblinear` is used\nand `self.fit_intercept` is set to `True`. In this case, `x` becomes\n`[x, self.intercept_scaling]`,\ni.e. a \"synthetic\" feature with constant value equal to\n`intercept_scaling` is appended to the instance vector.\nThe intercept becomes\n``intercept_scaling * synthetic_feature_weight``.\n\n.. note::\nThe synthetic feature weight is subject to L1 or L2\nregularization as all other features.\nTo lessen the effect of regularization on synthetic feature weight\n(and therefore on the intercept) `intercept_scaling` has to be increased.\n1\n\n\n\nclass_weight class_weight: dict or 'balanced', default=None\n\nWeights associated with classes in the form ``{class_label: weight}``.\nIf not given, all classes are supposed to have weight one.\n\nThe \"balanced\" mode uses the values of y to automatically adjust\nweights inversely proportional to class frequencies in the input data\nas ``n_samples / (n_classes * np.bincount(y))``.\n\nNote that these weights will be multiplied with sample_weight (passed\nthrough the fit method) if sample_weight is specified.\n\n.. versionadded:: 0.17\n*class_weight='balanced'*\nNone\n\n\n\nrandom_state random_state: int, RandomState instance, default=None\n\nUsed when ``solver`` == 'sag', 'saga' or 'liblinear' to shuffle the\ndata. See :term:`Glossary ` for details.\nNone\n\n\n\nsolver solver: {'lbfgs', 'liblinear', 'newton-cg', 'newton-cholesky', 'sag', 'saga'}, default='lbfgs'\n\nAlgorithm to use in the optimization problem. Default is 'lbfgs'.\nTo choose a solver, you might want to consider the following aspects:\n\n- 'lbfgs' is a good default solver because it works reasonably well for a wide\nclass of problems.\n- For :term:`multiclass` problems (`n_classes &gt;= 3`), all solvers except\n'liblinear' minimize the full multinomial loss, 'liblinear' will raise an\nerror.\n- 'newton-cholesky' is a good choice for\n`n_samples` &gt;&gt; `n_features * n_classes`, especially with one-hot encoded\ncategorical features with rare categories. Be aware that the memory usage\nof this solver has a quadratic dependency on `n_features * n_classes`\nbecause it explicitly computes the full Hessian matrix.\n- For small datasets, 'liblinear' is a good choice, whereas 'sag'\nand 'saga' are faster for large ones;\n- 'liblinear' can only handle binary classification by default. To apply a\none-versus-rest scheme for the multiclass setting one can wrap it with the\n:class:`~sklearn.multiclass.OneVsRestClassifier`.\n\n.. warning::\nThe choice of the algorithm depends on the penalty chosen (`l1_ratio=0`\nfor L2-penalty, `l1_ratio=1` for L1-penalty and `0 &lt; l1_ratio &lt; 1` for\nElastic-Net) and on (multinomial) multiclass support:\n\n================= ======================== ======================\nsolver l1_ratio multinomial multiclass\n================= ======================== ======================\n'lbfgs' l1_ratio=0 yes\n'liblinear' l1_ratio=1 or l1_ratio=0 no\n'newton-cg' l1_ratio=0 yes\n'newton-cholesky' l1_ratio=0 yes\n'sag' l1_ratio=0 yes\n'saga' 0&lt;=l1_ratio&lt;=1 yes\n================= ======================== ======================\n\n.. note::\n'sag' and 'saga' fast convergence is only guaranteed on features\nwith approximately the same scale. You can preprocess the data with\na scaler from :mod:`sklearn.preprocessing`.\n\n.. seealso::\nRefer to the :ref:`User Guide ` for more\ninformation regarding :class:`LogisticRegression` and more specifically the\n:ref:`Table `\nsummarizing solver/penalty supports.\n\n.. versionadded:: 0.17\nStochastic Average Gradient (SAG) descent solver. Multinomial support in\nversion 0.18.\n.. versionadded:: 0.19\nSAGA solver.\n.. versionchanged:: 0.22\nThe default solver changed from 'liblinear' to 'lbfgs' in 0.22.\n.. versionadded:: 1.2\nnewton-cholesky solver. Multinomial support in version 1.6.\n'lbfgs'\n\n\n\nmax_iter max_iter: int, default=100\n\nMaximum number of iterations taken for the solvers to converge.\n300\n\n\n\nverbose verbose: int, default=0\n\nFor the liblinear and lbfgs solvers set verbose to any positive\nnumber for verbosity.\n0\n\n\n\nwarm_start warm_start: bool, default=False\n\nWhen set to True, reuse the solution of the previous call to fit as\ninitialization, otherwise, just erase the previous solution.\nUseless for liblinear solver. See :term:`the Glossary `.\n\n.. versionadded:: 0.17\n*warm_start* to support *lbfgs*, *newton-cg*, *sag*, *saga* solvers.\nFalse\n\n\n\nn_jobs n_jobs: int, default=None\n\nDoes not have any effect.\n\n.. deprecated:: 1.8\n`n_jobs` is deprecated in version 1.8 and will be removed in 1.10.\nNone",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Przygotowanie i czyszczenie danych</span>"
    ]
  },
  {
    "objectID": "chapters/02-przygotowanie-danych.html#inżynieria-cech",
    "href": "chapters/02-przygotowanie-danych.html#inżynieria-cech",
    "title": "3  Przygotowanie i czyszczenie danych",
    "section": "3.4 Inżynieria cech",
    "text": "3.4 Inżynieria cech\nInżynieria cech (feature engineering) to etap, w którym surowe dane przekształcamy w reprezentację bardziej użyteczną dla modeli uczenia maszynowego. Jej celem nie jest „upiększanie” danych, lecz zwiększenie ilości informacji, jaką model może efektywnie wykorzystać, oraz dopasowanie formatu cech do wymagań algorytmów. W praktyce inżynieria cech obejmuje zarówno proste transformacje (np. logarytmowanie zmiennej skośnej), jak i budowę cech pochodnych, interakcji, agregacji czy cech czasowych. W tym kursie inżynieria cech jest szczególnie istotna, ponieważ omawiane modele klasyczne (regresja logistyczna, LDA/QDA, SVM, k-NN) są wrażliwe na sposób reprezentacji danych i często korzystają bardziej z dobrze zaprojektowanych cech niż z samej „mocy” algorytmu.\n\n3.4.1 Ujednolicanie i czyszczenie kategorii jako element inżynierii cech\nZanim zakodujemy kategorie, warto je znormalizować, ponieważ w danych rzeczywistych często występują spacje, różne wielkości liter oraz warianty zapisu tej samej kategorii. To nie jest „czyszczenie kosmetyczne” – bez tego model będzie traktował np. Mobile i mobile jako różne wartości, co rozbije informację na wiele sztucznych kategorii.\n\n\nKod\n# ujednolicenie prostych kolumn kategorycznych\nfor col in [\"city\", \"segment\", \"device_type\", \"notes\"]:\n    df[col] = (df[col]\n               .astype(\"string\")\n               .str.strip()\n               .str.lower())\n\n# przykładowo: ujednolicenie nazw urządzeń\ndf[\"device_type\"] = df[\"device_type\"].replace({\"mobile\": \"mobile\", \"desktop\": \"desktop\", \"tablet\": \"tablet\"})\n\n\nW wielu projektach opłaca się również „skleić” rzadkie kategorie do wspólnej kategorii other, aby ograniczyć wymiar po kodowaniu one-hot.\n\n\nKod\ntop_cities = df[\"city\"].value_counts().head(6).index\ndf[\"city_reduced\"] = df[\"city\"].where(df[\"city\"].isin(top_cities), other=\"other\")\n\n\n\n\n3.4.2 Cechy czasowe z dat\nDaty rzadko trafiają do modelu w postaci surowej. Zwykle tworzy się z nich cechy o sensie behawioralnym: czas od zdarzenia, miesiąc, dzień tygodnia, czy wskaźniki sezonowe. Dla danych klient–zakupy naturalne są cechy typu tenure oraz recency.\nNiech „datą odniesienia” będzie np. maksymalna obserwowana data zakupu (w realnym projekcie byłby to moment as-of).\n\n\nKod\nimport pandas as pd\n\nas_of = df[\"last_purchase_date\"].max()\n\ndf[\"tenure_days\"] = (as_of - df[\"signup_date\"]).dt.days\ndf[\"recency_days\"] = (as_of - df[\"last_purchase_date\"]).dt.days\n\n# sezonowość i kalendarz\ndf[\"signup_month\"] = df[\"signup_date\"].dt.month\ndf[\"signup_dow\"] = df[\"signup_date\"].dt.dayofweek  # 0=pon, 6=niedz\n\n\nTakie cechy są zwykle dużo bardziej informacyjne dla klasyfikacji churn niż same daty, ponieważ mają bezpośredni związek z aktywnością klienta.\n\n\n3.4.3 Transformacje rozkładów\nW danych ekonomicznych i behawioralnych (np. income_eur, avg_basket_eur) rozkłady są często prawoskośne. Dla modeli liniowych oraz metod odległościowych zyskujemy, gdy przekształcimy skalę tak, aby relacje były bardziej „liniowe” i mniej zdominowane przez ogon rozkładu.\nStandardowa transformacja logarytmiczna (z zabezpieczeniem na zera) to:\n\\[\nx' = \\log(1 + x).\n\\]\n\n\nKod\nimport numpy as np\n\ndf[\"log_income\"] = np.log1p(df[\"income_eur\"])\ndf[\"log_avg_basket\"] = np.log1p(df[\"avg_basket_eur\"])\ndf[\"log_visits\"] = np.log1p(df[\"visits_last30\"])\n\n\nW praktyce często lepsze jest utrzymywanie zarówno cechy pierwotnej, jak i przekształconej – modele drzewiaste zwykle skorzystają z obu, a modele liniowe częściej z wersji log.\n\n\n3.4.4 Cechy intensywności i „normalizacja przez czas” (rate features)\nTypowym zabiegiem jest tworzenie cech typu „na jednostkę czasu”. Jeśli klient jest w systemie krótko, jego liczby bezwzględne mogą być nieporównywalne do klienta z długim stażem. Stąd tworzy się wskaźniki intensywności:\n\n\nKod\n# zabezpieczenie przed dzieleniem przez 0\ndf[\"tenure_days_safe\"] = df[\"tenure_days\"].clip(lower=1)\n\ndf[\"visits_per_day\"] = df[\"visits_last30\"] / 30.0\ndf[\"returns_rate_90\"] = df[\"returns_last90\"] / 90.0\ndf[\"visits_per_tenure\"] = df[\"visits_last30\"] / df[\"tenure_days_safe\"]\n\n\nTe cechy są szczególnie wartościowe w klasycznych modelach liniowych, bo stabilizują skalę i często lepiej korelują z decyzją niż wartości surowe.\n\n\n3.4.5 Interakcje i cechy „biznesowe” z sensowną interpretacją\nInterakcje to bardzo ważna klasa cech w klasycznych modelach. Jeżeli nie używasz modeli, które „same” łatwo uczą się złożonych interakcji (np. boosting), to możesz jawnie dodać kilka logicznych interakcji. Przykładowo: wartość koszyka może działać inaczej dla segmentu VIP, a liczba wizyt może inaczej wpływać na churn, gdy satysfakcja jest niska.\n\n\nKod\n# przykładowe interakcje liczbowe\ndf[\"basket_x_visits\"] = df[\"avg_basket_eur\"] * df[\"visits_last30\"]\ndf[\"income_x_visits\"] = df[\"income_eur\"] * df[\"visits_last30\"]\n\n\nMożemy również budować interakcje „warunkowe”, np. flaga „niska satysfakcja” i interakcja z wizytami:\n\n\nKod\ndf[\"low_satisfaction\"] = (df[\"satisfaction_1_5\"] &lt;= 2).astype(int)\ndf[\"visits_if_low_satisfaction\"] = df[\"visits_last30\"] * df[\"low_satisfaction\"]\n\n\nTo jest szczególnie dydaktyczne, bo pokazuje, jak złożone hipotezy można przenieść do prostego modelu.\n\n\n3.4.6 Grupowanie zmiennych ciągłych do przedziałów (binning)\nCzasem warto przekształcić zmienną ciągłą do kategorii, zwłaszcza gdy zależność jest progowa. Przykładowo: recency_days może mieć silny efekt dopiero po przekroczeniu pewnego progu. Binning bywa użyteczny też w analizie dyskryminacyjnej i prostych modelach interpretowalnych.\n\n\nKod\ndf[\"recency_bin\"] = pd.cut(\n    df[\"recency_days\"],\n    bins=[-1, 7, 30, 90, 180, 365, 10_000],\n    labels=[\"&lt;=7\", \"8-30\", \"31-90\", \"91-180\", \"181-365\", \"&gt;365\"]\n)\n\n\n\n\n3.4.7 Kodowanie kategorii pod modele\nNajbardziej standardowym kodowaniem jest one-hot encoding. Możemy je zrobić w pandas, ale w modelowaniu lepszy jest pipeline w sklearn. Dla celów EDA i demonstracji:\n\n\nKod\nX_cat = pd.get_dummies(df[[\"segment\", \"device_type\", \"city_reduced\"]], dummy_na=True)\nX_cat.head()\n\n\n\n\n\n\n\n\n\nsegment_b2b\nsegment_b2c\nsegment_student\nsegment_vip\nsegment_&lt;NA&gt;\ndevice_type_desktop\ndevice_type_mobile\ndevice_type_tablet\ndevice_type_&lt;NA&gt;\ncity_reduced_katowice\ncity_reduced_kraków\ncity_reduced_lublin\ncity_reduced_other\ncity_reduced_poznań\ncity_reduced_wrocław\ncity_reduced_łódź\ncity_reduced_&lt;NA&gt;\n\n\n\n\n0\nTrue\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\n\n\n1\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n2\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\n\n\n3\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n4\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\n\n\n\n\n\n\n\n\n\n3.4.8 Składanie cech do macierzy modelowej\nPo inżynierii cech zwykle wybieramy zestaw kolumn do modelu. Na potrzeby naszych klasycznych algorytmów rozsądne jest przygotowanie „bazowego” zestawu cech liczbowych oraz zakodowanych kategorii.\n\n\nKod\nfeature_cols_num = [\n    \"tenure_days\", \"recency_days\",\n    \"income_eur\", \"avg_basket_eur\", \"visits_last30\", \"returns_last90\",\n    \"log_income\", \"log_avg_basket\", \"log_visits\",\n    \"visits_per_day\", \"returns_rate_90\",\n    \"low_satisfaction\", \"basket_x_visits\"\n]\n\nfeature_cols_cat = [\"segment\", \"device_type\", \"city_reduced\", \"recency_bin\"]\n\nX_num = df[feature_cols_num]\nX_cat = pd.get_dummies(df[feature_cols_cat], dummy_na=True)\n\nX = pd.concat([X_num, X_cat], axis=1)\ny = df[\"churned\"]\n\n\n\n\n\n\n\n\nAdnotacja\n\n\n\nPrzedstawione powyżej przykłady inżynierii cech mają charakter ilustracyjny i służą pokazaniu typowych mechanizmów oraz kierunków pracy z danymi. W kontekście konkretnego problemu analitycznego część tych kroków może zostać zmodyfikowana, uproszczona lub całkowicie pominięta, a inne – niewystępujące w przykładach – mogą okazać się kluczowe. Inżynieria cech nie jest zestawem sztywnych reguł, lecz procesem zależnym od charakteru danych, celu analizy oraz stosowanego modelu.\nW praktyce najlepszym podejściem jest realizowanie inżynierii cech w sposób systematyczny i powtarzalny, z wykorzystaniem pipeline’ów. Pozwala to połączyć imputację, skalowanie, kodowanie oraz konstrukcję cech w jeden spójny proces, który jest dopasowywany wyłącznie na zbiorze treningowym i następnie stosowany do walidacji oraz testu. Takie podejście minimalizuje ryzyko wycieku informacji, ułatwia porównywanie modeli oraz sprawia, że eksperymenty analityczne są w pełni reprodukowalne.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Przygotowanie i czyszczenie danych</span>"
    ]
  },
  {
    "objectID": "chapters/03-klasyfikacja-liniowa.html",
    "href": "chapters/03-klasyfikacja-liniowa.html",
    "title": "4  Modele liniowe i dyskryminacyjne",
    "section": "",
    "text": "4.1 Modele liniowe w uczeniu nadzorowanym\nModele liniowe stanowią fundament klasycznego uczenia maszynowego dla danych tablicowych. Ich atrakcyjność wynika z połączenia trzech cech: prostoty konstrukcji, relatywnie łatwej interpretacji oraz stabilnych własności matematycznych. W ujęciu uczenia nadzorowanego model liniowy opisuje zależność pomiędzy wektorem cech \\(x \\in \\mathbb{R}^p\\) a zmienną docelową \\(y\\), ucząc się parametrów na podstawie par \\((x_i, y_i)\\). W praktyce modele liniowe służą zarówno do regresji (gdy \\(y\\) jest zmienną ciągłą), jak i do klasyfikacji (gdy \\(y\\) jest zmienną dyskretną). W tym kursie modele liniowe są szczególnie ważne, ponieważ stanowią punkt odniesienia dla późniejszych metod (drzew, zespołów, SVM), a także umożliwiają wprowadzenie pojęć takich jak funkcja straty, estymacja parametrów, kompromis bias–variance oraz interpretacja współczynników.\nW najprostszym ujęciu model liniowy zakłada, że predykcja jest liniową kombinacją cech. Dla obserwacji \\(i\\) zapisujemy:\n\\[\n\\eta_i = \\beta_0 + \\beta_1 x_{i1} + \\cdots + \\beta_p x_{ip}\n= \\beta_0 + x_i^\\top \\beta,\n\\]\ngdzie \\(\\beta_0\\) to wyraz wolny, a \\(\\beta \\in \\mathbb{R}^p\\) to wektor parametrów. Różne modele liniowe różnią się tym, jak \\(\\eta_i\\) jest powiązane z \\(y_i\\) (funkcja łącząca) oraz jak definiowana jest funkcja straty, którą minimalizujemy w procesie uczenia.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Modele liniowe i dyskryminacyjne</span>"
    ]
  },
  {
    "objectID": "chapters/03-klasyfikacja-liniowa.html#regresja-wieloraka",
    "href": "chapters/03-klasyfikacja-liniowa.html#regresja-wieloraka",
    "title": "4  Modele liniowe i dyskryminacyjne",
    "section": "4.2 Regresja wieloraka",
    "text": "4.2 Regresja wieloraka\n\n4.2.1 Definicja modelu\nRegresja wieloraka (regresja liniowa) jest modelem nadzorowanym, w którym zmienna docelowa \\(y\\) jest ciągła. Zakładamy:\n\\[\ny_i = \\beta_0 + x_i^\\top \\beta + \\varepsilon_i,\n\\]\ngdzie \\(\\varepsilon_i\\) to składnik losowy (szum). W ML nie musimy eksponować interpretacji probabilistycznej, natomiast kluczowe jest to, że parametry \\(\\beta_0, \\beta\\) uczymy tak, aby predykcje \\(\\hat{y}_i\\) były możliwie bliskie \\(y_i\\).\n\n\n4.2.2 Estymacja jako problem uczenia nadzorowanego\nNajczęściej stosuje się minimalizację sumy kwadratów błędów (least squares):\n\\[\n\\min_{\\beta_0, \\beta} \\sum_{i=1}^n \\left(y_i - (\\beta_0 + x_i^\\top \\beta)\\right)^2.\n\\]\nJest to klasyczny przykład uczenia nadzorowanego: posiadamy etykiety \\(y_i\\), a algorytm uczy parametry minimalizujące zadaną funkcję straty.\n\nPrzykład 4.1 Poniżej jest przykład regresji na rzeczywistych danych z Kaggle (dataset, nie konkurs): Boston House Prices. Plik danych to boston.csv, a kolumna zmiennej wyjściowej to MEDV (wartość domu).\nUwaga: Kaggle zwykle wymaga konta i tokenu API (~/.kaggle/kaggle.json).\nJeśli używasz Kaggle CLI, pobierz i rozpakuj dane:\n#| eval: false\nkaggle datasets download -d fedesoriano/the-boston-houseprice-data -p chapters/kaggle_boston\n\nls -la chapters/kaggle_boston\nunzip -o chapters/kaggle_boston/*.zip -d chapters/kaggle_boston || true\nNastępnie przejdziemy do budowy modelu\n\n\nKod\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom pathlib import Path\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\n\ndata_path = Path(\"kaggle_boston/boston.csv\")\n\ndf = pd.read_csv(data_path)\n\n# Target jest znany z opisu danych\ntarget = \"MEDV\"\nif target not in df.columns:\n    raise ValueError(\n        f\"Brakuje kolumny `{target}` w danych. Dostępne kolumny: \" + \", \".join(df.columns)\n    )\n\n# Bierzemy wszystkie cechy liczbowe\ndf_num = df.select_dtypes(include=[\"number\"]).copy()\n\ntmp = df_num.dropna(subset=[target]).copy()\nX = tmp.drop(columns=[target]).dropna()\ny = tmp.loc[X.index, target]\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\nreg = LinearRegression()\nreg.fit(X_train, y_train)\n\n# predykcje\ny_pred_train = reg.predict(X_train)\ny_pred_test = reg.predict(X_test)\n\n# R^2\nr2_train = reg.score(X_train, y_train)\nr2_test = reg.score(X_test, y_test)\n\n# „klasyczne” podsumowanie regresji (liczone na zbiorze treningowym)\nn = X_train.shape[0]\np = X_train.shape[1]\ndof = n - p - 1\n\nresid = y_train.values - y_pred_train\nsse = np.sum(resid**2)  # sum of squared errors\ntss = np.sum((y_train.values - y_train.mean())**2)\nssr = tss - sse         # sum of squares regression\n\nsigma = np.sqrt(sse / dof)          # residual std. error (RSE)\nF = (ssr / p) / (sse / dof)         # F-statistic dla H0: beta_1=...=beta_p=0\n\nsummary = pd.DataFrame({\n    \"metric\": [\"R2_train\", \"R2_test\", \"sigma(RSE)_train\", \"F_train\", \"n_train\", \"p\", \"dof\"],\n    \"value\":  [r2_train, r2_test, sigma, F, n, p, dof],\n})\nprint(summary.to_string(index=False))\n\n# wykres: y_true vs y_pred (zbiór testowy)\nplt.figure(figsize=(6, 6))\nplt.scatter(y_test, y_pred_test, alpha=0.7)\nmn = min(y_test.min(), y_pred_test.min())\nmx = max(y_test.max(), y_pred_test.max())\nplt.plot([mn, mx], [mn, mx], linestyle=\"--\")\nplt.xlabel(f\"y_true ({target})\")\nplt.ylabel(\"y_pred\")\nplt.title(\"Boston Housing (Kaggle): y_pred vs y_true (test)\")\nplt.tight_layout()\nplt.show()\n\n\n          metric      value\n        R2_train   0.750886\n         R2_test   0.668759\nsigma(RSE)_train   4.734795\n         F_train  90.426617\n         n_train 404.000000\n               p  13.000000\n             dof 390.000000\n\n\n\n\n\n\n\n\n\n\nR2_train = 0.751, R2_test = 0.669 - model liniowy wyjaśnia ok. 75% wariancji zmiennej docelowej na treningu i ok. 67% na teście. Spadek jest znaczny, co może sugerować przeuczenie modelu.\nsigma (RSE)_train = 4.735 - to oszacowanie odchylenia standardowego reszt (typowy błąd predykcji) na treningu. Interpretacja jednostek zależy od tego, czym jest target: jeśli target to klasyczny MEDV w tysiącach USD, to przeciętny błąd rzędu ~4.7 tys. USD na treningu. To jest „typowa” skala odchyłki predykcji od wartości rzeczywistej w modelu liniowym.\nF_train = 90.43 przy p = 13 i dof = 390 - statystyka \\(F\\) testuje hipotezę zerową: wszystkie współczynniki nachyleń są równe 0 (model nie wnosi nic ponad średnią). Tak wysoka wartość \\(F\\) oznacza, że model jako całość jest istotny statystycznie (praktycznie na pewno p-value \\(\\ll 0.001\\)) — czyli przynajmniej część cech ma realny związek z targetem.\n\nPonieważ model wykazuje delikatne znamiona przeuczenia zastosujemy do niego regularyzacje.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Modele liniowe i dyskryminacyjne</span>"
    ]
  },
  {
    "objectID": "chapters/03-klasyfikacja-liniowa.html#regularyzacja-modeli-liniowych",
    "href": "chapters/03-klasyfikacja-liniowa.html#regularyzacja-modeli-liniowych",
    "title": "4  Modele liniowe i dyskryminacyjne",
    "section": "4.3 Regularyzacja modeli liniowych",
    "text": "4.3 Regularyzacja modeli liniowych\n\n4.3.1 Cel stosowania\nW praktyce modele liniowe mogą cierpieć na przeuczenie, niestabilność estymacji (zwłaszcza przy silnej współliniowości cech) oraz zbyt dużą wariancję współczynników. Regularyzacja dodaje do funkcji straty karę za zbyt duże wartości parametrów, co stabilizuje estymację i poprawia uogólnianie.\nOgólny zapis problemu z regularyzacją:\n\\[\n\\min_{\\beta_0, \\beta} \\mathcal{L}(y, X; \\beta_0, \\beta) + \\lambda \\, \\mathcal{P}(\\beta),\n\\]\ngdzie \\(\\mathcal{L}\\) to funkcja straty (np. MSE w regresji lub log-loss w regresji logistycznej), \\(\\mathcal{P}(\\beta)\\) to kara, a \\(\\lambda \\ge 0\\) kontroluje siłę regularyzacji. Zwykle nie karzemy wyrazu wolnego \\(\\beta_0\\).\n\n\n4.3.2 Ridge regression (kara \\(L_2\\))\nRidge regression (Tikhonov) stosuje karę \\(L_2\\):\n\\[\n\\min_{\\beta_0, \\beta} \\mathcal{L}(y, X; \\beta_0, \\beta) + \\lambda \\|\\beta\\|_2^2.\n\\]\nWspółczynniki są „kurczone” w stronę zera, ale zwykle nie stają się dokładnie równe zeru. Ridge jest szczególnie użyteczny przy współliniowości cech, bo stabilizuje estymacje i obniża wariancję.\n\n\n4.3.3 LASSO (kara \\(L_1\\))\nLASSO (ang. Least Absolute Shrinkage and Selection Operator) używa kary \\(L_1\\):\n\\[\n\\min_{\\beta_0, \\beta} \\mathcal{L}(y, X; \\beta_0, \\beta) + \\lambda \\|\\beta\\|_1.\n\\]\nKara L1 sprzyja rozwiązaniom rzadkim — część współczynników staje się dokładnie zerowa. Dzięki temu LASSO pełni jednocześnie rolę selekcji cech.\n\n\n4.3.4 Elastic Net (kara mieszana)\nElastic Net łączy zalety Ridge i LASSO:\n\\[\n\\min_{\\beta_0, \\beta}  \\mathcal{L}(y, X; \\beta_0, \\beta) + \\lambda \\left(\\alpha \\|\\beta\\|_1 + (1-\\alpha)\\|\\beta\\|_2^2\\right),\n\\]\ngdzie \\(\\alpha \\in [0,1]\\) steruje proporcją kary \\(L_1\\) do \\(L_2\\). Elastic Net jest szczególnie przydatny, gdy mamy wiele skorelowanych cech: LASSO wybiera pojedyncze zmienne z grupy, a Elastic Net częściej zachowuje całe grupy w postaci „współdzielonego” kurczenia.\n\n\n4.3.5 Interpretacja i praktyka\nRegularyzacja polega na tym, że do klasycznej funkcji straty dodajemy karę za „zbyt duże” współczynniki. W modelach liniowych i logistycznych najczęściej spotkasz kary typu (\\(L_2\\)) (ridge) albo (\\(L_1\\)) (lasso). W ujęciu matematycznym dla regresji liniowej jest to odpowiednio:\n\\[\n\\min_{\\beta_0,\\beta} \\sum_{i=1}^n (y_i - \\beta_0 - x_i^\\top \\beta)^2 + \\lambda |\\beta|_2^2\n\\quad\\text{(ridge)}\n\\]\nalbo\n\\[\n\\min_{\\beta_0,\\beta} \\sum_{i=1}^n (y_i - \\beta_0 - x_i^\\top \\beta)^2 + \\lambda |\\beta|_1\n\\quad\\text{(lasso)}\n\\]\nAnalogicznie w regresji logistycznej zamiast sumy kwadratów masz stratę logarytmiczną, ale idea kary jest identyczna. Parametr \\(\\lambda \\ge 0\\) steruje „siłą” regularyzacji: im większy, tym mocniej model preferuje małe współczynniki.\nW modelu bez regularyzacji współczynniki \\(\\hat\\beta\\) są dobierane tak, aby możliwie najlepiej dopasować dane uczące (minimalizować samą stratę). W przypadku danych z szumem, współliniowością cech lub dużą liczbą predyktorów, takie dopasowanie może prowadzić do tego, że współczynniki stają się „niestabilne”: niewielka zmiana danych (inna próba, inny podział train/test) daje wyraźnie inne \\(\\hat\\beta\\). Regularyzacja celowo ogranicza swobodę modelu, „ściągając” współczynniki w stronę zera. To powoduje, że estymator staje się obciążony (biased) – współczynniki nie są już czystym odzwierciedleniem relacji w danych, bo zostały sztucznie zmniejszone przez karę. Jednocześnie znacząco spada wariancja estymatora: współczynniki są bardziej stabilne, a predykcje częściej lepiej generalizują na dane testowe. To klasyczny kompromis bias–variance: akceptujemy pewną stronniczość w zamian za mniejszą wrażliwość na szum.\nW praktyce interpretacja jest taka: w modelu regularyzowanym nie traktujesz wartości \\(\\beta_j\\) jako „czystego” oszacowania wpływu cechy \\(x_j\\) (w sensie klasycznej regresji), tylko jako wynik kompromisu pomiędzy dopasowaniem i prostotą modelu. Szczególnie przy silnej regularyzacji współczynniki należy interpretować ostrożnie: „to jest kierunek i względna siła sygnału po uwzględnieniu kary”, a nie „dokładna zmiana oczekiwanej wartości \\(y\\) na jednostkę \\(x_j\\)”.\nGdy \\(\\lambda \\to 0\\), kara zanika i wracamy do klasycznego uczenia bez regularyzacji: w regresji liniowej do OLS, w logistycznej do MLE bez kary. Wtedy współczynniki są „najmniej ściągnięte”, ale mogą być niestabilne (szczególnie przy współliniowości i dużym \\(p\\)). Gdy \\(\\lambda\\) rośnie, kara zaczyna dominować i wymusza zmniejszanie współczynników. Dla ridge (\\(L_2\\)) współczynniki są płynnie „kurczone” w kierunku zera, ale zwykle nie są równe zero. Dla lasso (\\(L_1\\)) część współczynników może stać się dokładnie równa zero, co prowadzi do selekcji zmiennych. Przy bardzo dużym \\(\\lambda\\) model może w praktyce „zrezygnować” z większości sygnału i zbliżyć się do modelu prawie stałego (predykcja głównie przez \\(\\beta_0\\)).\nRegularyzacja karze współczynniki, a nie bezpośrednio cechy. Ponieważ współczynnik \\(\\beta_j\\) jest ściśle powiązany ze skalą \\(x_j\\), brak standaryzacji powoduje, że kara działa niesprawiedliwie względem zmiennych o różnych jednostkach.\nZałóżmy dwie cechy niosące podobną informację, ale w różnych skalach:\n\n\\(x_1\\) - „dochód” rzędu dziesiątek tysięcy,\n\\(x_2\\) - „udział zwrotów” w zakresie 0–1.\n\nAby obie cechy miały podobny wpływ na \\(\\eta\\), model bez regularyzacji może dopasować:\n\nmały współczynnik przy dochodzie (bo sama cecha ma duże liczby),\nduży współczynnik przy zmiennej 0–1 (bo cecha jest mała).\n\nJeżeli dodamy karę typu ridge \\(\\lambda \\sum_j \\beta_j^2\\), to duży współczynnik przy \\(x_2\\) zostanie ukarany dużo silniej niż mały współczynnik przy \\(x_1\\), mimo że obie cechy mogą być równie istotne. W efekcie model może preferować cechy o dużej skali nie dlatego, że są lepsze, tylko dlatego, że wymagają mniejszych \\(|\\beta_j|\\), a więc „taniej” przechodzą przez karę. To jest artefakt skali, a nie właściwość danych.\nStandardyzacja usuwa ten problem, sprowadzając każdą cechę do porównywalnej skali, najczęściej:\n\\[\nz_{ij} = \\frac{x_{ij}-\\mu_j}{\\sigma_j}.\n\\]\nPo standaryzacji „jednostka” każdej cechy jest porównywalna (1 odchylenie standardowe), więc kara na współczynnikach działa symetrycznie. Dlatego w praktyce dla regresji logistycznej, ridge/lasso oraz większości modeli liniowych z regularyzacją standaryzacja jest traktowana jako element obowiązkowy.\nW ujęciu uczenia maszynowego regularyzacja jest narzędziem kontrolowania złożoności modelu. \\(\\lambda\\) staje się hiperparametrem, który dobiera się na podstawie jakości generalizacji (np. przez walidację). Wraz ze wzrostem \\(\\lambda\\) model staje się prostszy i stabilniejszy, ale może gorzej dopasowywać dane treningowe. Standaryzacja jest integralną częścią tego procesu, bo zapewnia, że dobór \\(\\lambda\\) i sama kara mają sens niezależnie od jednostek i skali cech.\n\n\nKod\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom pathlib import Path\n\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.metrics import r2_score, mean_squared_error\n\n# ----------------------------\n# 1) Pipeline: standaryzacja + ElasticNet\n# ----------------------------\npipe = Pipeline(steps=[\n    (\"scaler\", StandardScaler()),\n    (\"model\", ElasticNet(max_iter=50_000, random_state=42))\n])\n\n# ----------------------------\n# 2) Kalibracja (GridSearch): lambda i alpha\n#    - lambda -&gt; model__alpha\n#    - alpha  -&gt; model__l1_ratio\n# ----------------------------\n# Sensowna siatka: logarytmiczna dla lambda\nlambda_grid = np.logspace(-4, 2, 25)      # 1e-4 ... 1e2\nalpha_grid = np.linspace(0.0, 1.0, 11)    # 0, 0.1, ..., 1.0\n\nparam_grid = {\n    \"model__alpha\": lambda_grid,          # lambda (siła kary)\n    \"model__l1_ratio\": alpha_grid         # alpha (mieszanka L1/L2)\n}\n\n# CV: w regresji klasycznie KFold; domyślnie GridSearchCV użyje KFold\n# Skoring: R^2, bo tak raportujesz w przykładzie\nsearch = GridSearchCV(\n    estimator=pipe,\n    param_grid=param_grid,\n    scoring=\"r2\",\n    cv=5,\n    n_jobs=-1\n)\n\nsearch.fit(X_train, y_train)\n\nbest_model = search.best_estimator_\nbest_params = search.best_params_\nbest_cv_r2 = search.best_score_\n\nprint(\"Najlepsze hiperparametry (CV):\")\nprint(f\"  lambda (model__alpha)   = {best_params['model__alpha']:.6g}\")\nprint(f\"  alpha  (model__l1_ratio)= {best_params['model__l1_ratio']:.3f}\")\nprint(f\"  CV R^2 (mean)           = {best_cv_r2:.4f}\")\n\n# ----------------------------\n# 3) Ocena na train/test + metryki jak w Twoim stylu\n# ----------------------------\ny_pred_train = best_model.predict(X_train)\ny_pred_test = best_model.predict(X_test)\n\nr2_train = r2_score(y_train, y_pred_train)\nr2_test = r2_score(y_test, y_pred_test)\n\n# Dodatkowo RMSE (często użyteczne w regresji)\nrmse_train = np.sqrt(mean_squared_error(y_train, y_pred_train))\nrmse_test = np.sqrt(mean_squared_error(y_test, y_pred_test))\n\nsummary = pd.DataFrame({\n    \"metric\": [\"R2_train\", \"R2_test\", \"RMSE_train\", \"RMSE_test\", \"best_lambda\", \"best_alpha(l1_ratio)\"],\n    \"value\":  [r2_train, r2_test, rmse_train, rmse_test, best_params[\"model__alpha\"], best_params[\"model__l1_ratio\"]],\n})\nprint(\"\\nPodsumowanie ElasticNet:\")\nprint(summary.to_string(index=False))\n\n# ----------------------------\n# 4) Współczynniki po regularyzacji (już po standaryzacji)\n# ----------------------------\n# Uwaga: współczynniki dotyczą cech po standaryzacji (porównywalne między sobą).\ncoefs = best_model.named_steps[\"model\"].coef_\ncoef_table = pd.DataFrame({\"feature\": X.columns, \"coef\": coefs}).sort_values(\"coef\", key=np.abs, ascending=False)\n\nprint(\"\\nNajwiększe (bezwzględnie) współczynniki ElasticNet:\")\nprint(coef_table.head(10).to_string(index=False))\n\n# ----------------------------\n# 5) Wykres: y_true vs y_pred (test)\n# ----------------------------\nplt.figure(figsize=(6, 6))\nplt.scatter(y_test, y_pred_test, alpha=0.7)\nmn = min(y_test.min(), y_pred_test.min())\nmx = max(y_test.max(), y_pred_test.max())\nplt.plot([mn, mx], [mn, mx], linestyle=\"--\")\nplt.xlabel(f\"y_true ({target})\")\nplt.ylabel(\"y_pred\")\nplt.title(\"Boston Housing (Kaggle): ElasticNet y_pred vs y_true (test)\")\nplt.tight_layout()\nplt.show()\n\n# ----------------------------\n# 6) (Opcjonalnie) mapa wyników: R^2 w funkcji (lambda, alpha)\n# ----------------------------\n# To jest przydatne dydaktycznie: pokazuje krajobraz hiperparametrów.\nresults = pd.DataFrame(search.cv_results_)\npivot = results.pivot_table(\n    index=\"param_model__l1_ratio\",\n    columns=\"param_model__alpha\",\n    values=\"mean_test_score\"\n).sort_index()\n\nplt.figure(figsize=(10, 4))\nplt.imshow(pivot.values, aspect=\"auto\")\nplt.colorbar(label=\"mean CV R^2\")\nplt.yticks(range(pivot.shape[0]), [f\"{v:.1f}\" for v in pivot.index])\nplt.xticks(range(pivot.shape[1]), [f\"{v:.0e}\" for v in pivot.columns], rotation=45, ha=\"right\")\nplt.ylabel(\"alpha (l1_ratio)\")\nplt.xlabel(\"lambda (model__alpha)\")\nplt.title(\"ElasticNet: średnie CV R^2 dla (lambda, alpha)\")\nplt.tight_layout()\nplt.show()\n\n\nNajlepsze hiperparametry (CV):\n  lambda (model__alpha)   = 0.01\n  alpha  (model__l1_ratio)= 0.000\n  CV R^2 (mean)           = 0.7245\n\nPodsumowanie ElasticNet:\n              metric    value\n            R2_train 0.750670\n             R2_test 0.667571\n          RMSE_train 4.654048\n           RMSE_test 4.937440\n         best_lambda 0.010000\nbest_alpha(l1_ratio) 0.000000\n\nNajwiększe (bezwzględnie) współczynniki ElasticNet:\nfeature      coef\n  LSTAT -3.562197\n     RM  3.167423\n    DIS -2.939338\nPTRATIO -1.999596\n    RAD  1.966321\n    NOX -1.901429\n    TAX -1.510458\n      B  1.120095\n   CRIM -0.964915\n   CHAS  0.732337\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWidać wyraźnie, że kalibaracja parametrów \\(\\lambda\\) i \\(\\alpha\\) sprowadziła praktycznie model do regresji grzbietowej (ridge) z parametrem \\(\\lambda=0.01\\). Wyniki porównania \\(R^2\\) pomiędzy zbiorem treningowym i testowym po regularyzacji się nie zmieniły. Dalej występuje różnica \\(\\approx 0.08\\). To pokazuje, że sama regularyzacja nie wystarczy do usunięcia tego przeuczenia (swoją drogą nie jest ono bardzo duże). Na koniec aby przekonać się jak modele (raw i ridge) są wrażliwe na podział zbioru dokonamy ich porównania z wykorzystaniem walidacji krzyżowej.\n\n\nKod\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import RepeatedKFold, cross_val_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression, Ridge\n\n# ----------------------------\n# 1) RepeatedKFold (powtarzana walidacja krzyżowa)\n# ----------------------------\ncv = RepeatedKFold(n_splits=5, n_repeats=20, random_state=42)\n\n# ----------------------------\n# 2) Modele do porównania\n#    - OLS: LinearRegression (bez regularyzacji)\n#    - Ridge: regularyzacja L2; skalowanie w pipeline (ważne)\n# ----------------------------\nols = Pipeline(steps=[\n    (\"scaler\", StandardScaler()),\n    (\"model\", LinearRegression())\n])\n\nridge = Pipeline(steps=[\n    (\"scaler\", StandardScaler()),\n    (\"model\", Ridge(alpha=0.01, random_state=42))\n])\n\n# ----------------------------\n# 3) R^2 w RepeatedKFold\n# ----------------------------\nscores_ols = cross_val_score(ols, X, y, cv=cv, scoring=\"r2\", n_jobs=-1)\nscores_ridge = cross_val_score(ridge, X, y, cv=cv, scoring=\"r2\", n_jobs=-1)\n\n# Podsumowanie liczbowe: średnia, odchylenie, kwartyle\nsummary = pd.DataFrame({\n    \"model\": [\"LinearRegression (OLS)\", \"Ridge (alpha=0.01)\"],\n    \"mean_R2\": [scores_ols.mean(), scores_ridge.mean()],\n    \"std_R2\": [scores_ols.std(ddof=1), scores_ridge.std(ddof=1)],\n    \"q25\": [np.quantile(scores_ols, 0.25), np.quantile(scores_ridge, 0.25)],\n    \"median\": [np.quantile(scores_ols, 0.50), np.quantile(scores_ridge, 0.50)],\n    \"q75\": [np.quantile(scores_ols, 0.75), np.quantile(scores_ridge, 0.75)],\n})\n\nprint(summary.to_string(index=False))\n\n# ----------------------------\n# 4) Boxplot wyników\n# ----------------------------\nplt.figure(figsize=(7, 4))\nplt.boxplot([scores_ols, scores_ridge], labels=[\"OLS\", \"Ridge\"], showfliers=True)\nplt.ylabel(\"R^2 (CV)\")\nplt.title(\"RepeatedKFold: rozkład R^2 dla OLS vs Ridge\")\nplt.tight_layout()\nplt.show()\n\n# ----------------------------\n# 5) „Stabilizacja” wprost: porównanie wariancji wyników\n# ----------------------------\nprint(\"\\nStabilizacja (mniejsza zmienność wyników CV jest korzystna):\")\nprint(f\"Std(R^2) OLS  : {scores_ols.std(ddof=1):.4f}\")\nprint(f\"Std(R^2) Ridge: {scores_ridge.std(ddof=1):.4f}\")\n\n\n                 model  mean_R2   std_R2      q25   median      q75\nLinearRegression (OLS) 0.712943 0.059862 0.671914 0.719919 0.762099\n    Ridge (alpha=0.01) 0.712945 0.059863 0.671911 0.719929 0.762106\n\n\n\n\n\n\n\n\n\n\nStabilizacja (mniejsza zmienność wyników CV jest korzystna):\nStd(R^2) OLS  : 0.0599\nStd(R^2) Ridge: 0.0599\n\n\nWyniki są niemal identyczne, co pokazuje, że regularyzacja modelu nie pomaga wyeliminować delikatnego przeuczenia.\nPierwszy powód to naturalna różnica między błędem treningowym a błędem generalizacji. Model jest dopasowywany tak, aby minimalizować stratę na treningu, więc na treningu prawie zawsze będzie lepiej niż na danych niewidzianych. Nawet gdy model nie jest „przeuczony” w sensie patologicznym, pojawi się luka generalizacyjna, bo test jest inną próbą z tej samej populacji i zawiera inny układ szumu losowego.\nDrugi powód to ograniczona zgodność modelu z rzeczywistą zależnością. Regresja liniowa zakłada liniowość i addytywność wpływów cech (bez nieliniowości i bez interakcji, jeśli ich nie dodasz). Jeśli prawdziwa relacja jest częściowo nieliniowa (co w danych nieruchomości jest częste), to model „radzi sobie” na treningu, ale na teście spada, bo dopasowanie do przypadkowego układu obserwacji w treningu nie przenosi się idealnie na inną próbę. To jest bardziej kwestia bias/misspecification niż „overfittingu z powodu zbyt dużej złożoności”, ale objaw w metrykach jest podobny.\nTrzeci powód to wariancja pojedynczego podziału train/test. Ta różnica (około 0.08 w \\(R^2\\)) może w dużej mierze wynikać z tego, że akurat wylosowany test jest „trudniejszy” (np. zawiera więcej obserwacji z krańców rozkładu). Właśnie dlatego RepeatedKFold jest lepszym narzędziem diagnostycznym: u nas średnie CV (\\(R^2 \\approx 0.713\\)) wskazuje, że wynik testowy 0.668 nie jest już tak odległy, tylko może być po prostu mniej korzystnym splitem.\nCzwarty potencjalny powód to współliniowość i niestabilność współczynników, która nie zawsze przekłada się na wyraźną zmianę \\(R^2\\), ale może zwiększać wrażliwość na konkretny podział danych. Ridge z bardzo małym \\(\\lambda\\) nie zmienia u nas jakości ani wariancji metryki, więc to sugeruje, że w tym konkretnym ustawieniu współliniowość nie jest dominującym źródłem luki — ale nadal może wpływać na interpretację \\(\\beta\\) i na zachowanie w innych splitach.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Modele liniowe i dyskryminacyjne</span>"
    ]
  },
  {
    "objectID": "chapters/03-klasyfikacja-liniowa.html#regresja-logistyczna",
    "href": "chapters/03-klasyfikacja-liniowa.html#regresja-logistyczna",
    "title": "4  Modele liniowe i dyskryminacyjne",
    "section": "4.4 Regresja logistyczna",
    "text": "4.4 Regresja logistyczna\n\n4.4.1 Definicja modelu\nRegresja logistyczna jest modelem liniowym przeznaczonym do klasyfikacji binarnej. Zakładamy, że zmienna docelowa \\(y \\in {0,1}\\), a model opisuje prawdopodobieństwo klasy wyróżnionej (1). Najpierw definiujemy predyktor liniowy:\n\\[\n\\eta_i = \\beta_0 + x_i^\\top \\beta,\n\\]\na następnie mapujemy go do \\([0,1]\\) funkcją logistyczną:\n\\[\np_i = \\mathbb{P}(y_i = 1 \\mid x_i) = \\sigma(\\eta_i) = \\frac{1}{1 + e^{-\\eta_i}}.\n\\]\nDecyzję klasyfikacyjną podejmujemy przez ustawienie progu \\(t\\) (zwykle 0.5):\n\\[\n\\hat{y}_i​=\n\\begin{cases}\n1,\\; p_i\\geq t,\\\\\n0,\\; p_i&lt;t.\n\\end{cases}\n\\]\nPonieważ \\(y_i \\in {0,1}\\), naturalnym modelem probabilistycznym dla \\(y_i\\) przy danym \\(x_i\\) jest rozkład Bernoulliego:\n\\[\ny_i \\mid x_i \\sim \\text{Bernoulli}(p_i),\n\\quad\\text{czyli}\\quad\n\\mathbb{P}(y_i\\mid x_i) = p_i^{y_i}(1-p_i)^{1-y_i}.\n\\]\nTo prowadzi do funkcji wiarygodności dla całej próby (przy założeniu niezależności obserwacji):\n\\[\nL(\\beta_0,\\beta) = \\prod_{i=1}^n p_i^{y_i}(1-p_i)^{1-y_i}.\n\\]\nW ujęciu statystycznym „uczenie” parametrów polega na maksymalizacji wiarygodności \\(L\\). Ponieważ iloczyny są niewygodne obliczeniowo, przechodzi się na logartym wiarygodności:\n\\[\n\\ell(\\beta_0,\\beta) = \\log L(\\beta_0,\\beta)\n= \\sum_{i=1}^n \\left[y_i\\log(p_i) + (1-y_i)\\log(1-p_i)\\right].\n\\]\nMaksymalizacja \\(\\ell\\) jest równoważna minimalizacji jej negacji, co w ML nazywamy stratą logarytmiczną (log loss) albo entropią krzyżową (binary cross-entropy):\n\\[\n\\min_{\\beta_0,\\beta} -\\sum_{i=1}^n \\left[y_i \\log(p_i) + (1-y_i)\\log(1-p_i)\\right].\n\\]\nIntuicyjnie ta strata „nagradza” model, gdy przypisuje wysokie prawdopodobieństwo klasie, która rzeczywiście wystąpiła, i „karze” go mocno, gdy model jest bardzo pewny, ale myli się. Na przykład, jeśli \\(y_i=1\\) i model daje \\(p_i=0.99\\), składnik straty jest mały; jeśli natomiast \\(y_i=1\\) i model daje \\(p_i=0.01\\), to \\(-\\log(0.01)\\) jest duże, więc kara jest silna. Dzięki temu log loss nie jest tylko miarą „trafione/nie trafione”, lecz ocenia jakość probabilistyczną predykcji, co jest szczególnie ważne w klasyfikacji.\nW praktyce nie istnieje prosty wzór zamknięty na \\((\\beta_0,\\beta)\\) analogiczny do regresji liniowej OLS. Dlatego optymalizację wykonuje się numerycznie. Najczęściej stosuje się metody oparte o gradient (i często pochodne drugiego rzędu). Kluczowym faktem jest, że funkcja straty w regresji logistycznej jest wypukła względem \\((\\beta_0,\\beta)\\), więc metody gradientowe mają dobre własności: przy poprawnej implementacji dążą do globalnego minimum. Dla kompletności warto zapisać postać gradientu względem \\(\\beta\\) (pomijając wyraz wolny dla czytelności). Jeśli \\(X\\) to macierz cech, a \\(p\\) wektor \\(p_i\\), to gradient ma postać:\n\\[\n\\nabla_{\\beta}  \\Big(-\\ell(\\beta)\\Big) = X^\\top (p - y),\n\\]\nczyli różnica między prognozowanymi prawdopodobieństwami a rzeczywistymi etykietami, „zebrana” przez cechy. To dobrze podkreśla charakter uczenia: gdy model przeszacowuje prawdopodobieństwo klasy 1 (duże \\(p_i\\) przy \\(y_i=0\\)), gradient pcha parametry w kierunku zmniejszenia \\(\\eta_i\\) dla tych obserwacji, i odwrotnie.\nWreszcie, jest to model nadzorowany w sensie ML, ponieważ parametry \\((\\beta_0,\\beta)\\) uczymy na oznakowanych parach \\((x_i,y_i)\\) poprzez minimalizację funkcji straty. W odróżnieniu od metod nienadzorowanych, tutaj etykieta \\(y_i\\) jest bezpośrednio składnikiem funkcji celu, a „uczenie” polega na takim doborze parametrów, aby model możliwie dobrze odtwarzał zależność między cechami a klasą docelową, nie tylko na danych uczących, ale przede wszystkim na danych niewidzianych.\n\nPrzykład 4.2  \n\n\nKod\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom datasets import load_dataset\n\nfrom sklearn.model_selection import StratifiedKFold, cross_val_predict\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.metrics import (\n    confusion_matrix,\n    accuracy_score, precision_score, recall_score, f1_score,\n    roc_auc_score, roc_curve\n)\n\n# =========================\n# 1) Wczytanie danych z Hugging Face\n# =========================\n# Dataset ma jeden split \"train\" (683 obserwacje) i target \"is_cancer\"\nds = load_dataset(\"mstz/breast\", \"cancer\")[\"train\"]  # :contentReference[oaicite:1]{index=1}\ndf = ds.to_pandas()\n\ntarget = \"is_cancer\"\ny = df[target].astype(int)\n\nX = df.drop(columns=[target])\n\n# Minimalnie: imputacja (gdyby były braki) + standaryzacja + logreg\npipe = Pipeline(steps=[\n    (\"imputer\", SimpleImputer(strategy=\"median\")),\n    (\"scaler\", StandardScaler()),\n    (\"model\", LogisticRegression(max_iter=2000, solver=\"lbfgs\"))\n])\n\n# =========================\n# 2) Walidacja krzyżowa i out-of-fold predykcje prawdopodobieństw\n# =========================\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\n# out-of-fold: każda obserwacja ma proba z modelu, który jej \"nie widział\" w treningu\nproba_oof = cross_val_predict(\n    pipe, X, y,\n    cv=cv,\n    method=\"predict_proba\"\n)[:, 1]\n\n# Predykcja klasy przy standardowym progu 0.5\nthr_default = 0.50\ny_pred_default = (proba_oof &gt;= thr_default).astype(int)\n\n# =========================\n# 3) Metryki + confusion matrix (PRZED kalibracją progu)\n# =========================\ndef compute_metrics(y_true, y_pred, proba):\n    return {\n        \"accuracy\":  accuracy_score(y_true, y_pred),\n        \"precision\": precision_score(y_true, y_pred, zero_division=0),\n        \"recall\":    recall_score(y_true, y_pred, zero_division=0),\n        \"f1\":        f1_score(y_true, y_pred, zero_division=0),\n        \"roc_auc\":   roc_auc_score(y_true, proba),\n    }\n\nmetrics_default = compute_metrics(y, y_pred_default, proba_oof)\ncm_default = confusion_matrix(y, y_pred_default)\n\nprint(\"PRZED kalibracją progu (threshold=0.50) – metryki OOF:\")\nfor k, v in metrics_default.items():\n    print(f\"  {k:9s}: {v:.4f}\")\nprint(\"\\nMacierz pomyłek (rows=true, cols=pred):\\n\", cm_default)\n\n# =========================\n# 4) ROC curve (z OOF proba)\n# =========================\nfpr, tpr, thresholds = roc_curve(y, proba_oof)\nauc = roc_auc_score(y, proba_oof)\n\nplt.figure(figsize=(6, 5))\nplt.plot(fpr, tpr, label=f\"ROC (AUC={auc:.3f})\")\nplt.plot([0, 1], [0, 1], linestyle=\"--\")\nplt.xlabel(\"False Positive Rate (1 - specificity)\")\nplt.ylabel(\"True Positive Rate (recall / sensitivity)\")\nplt.title(\"ROC curve – Logistic Regression (OOF)\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n# =========================\n# 5) Kalibracja progu (Youden’s J)\n#    J = TPR - FPR = sensitivity + specificity - 1\n# =========================\nJ = tpr - fpr\nidx = np.argmax(J)\nthr_calibrated = thresholds[idx]\n\nprint(f\"\\nSkalibrowany próg (Youden J): {thr_calibrated:.4f}\")\n\ny_pred_cal = (proba_oof &gt;= thr_calibrated).astype(int)\n\nmetrics_cal = compute_metrics(y, y_pred_cal, proba_oof)\ncm_cal = confusion_matrix(y, y_pred_cal)\n\nprint(\"\\nPO kalibracji progu (Youden J) – metryki OOF:\")\nfor k, v in metrics_cal.items():\n    print(f\"  {k:9s}: {v:.4f}\")\nprint(\"\\nMacierz pomyłek (rows=true, cols=pred):\\n\", cm_cal)\n\n# =========================\n# 6) Porównanie confusion matrices: wizualizacja\n# =========================\ndef plot_cm(cm, title):\n    plt.figure(figsize=(4.5, 4))\n    plt.imshow(cm, aspect=\"auto\")\n    plt.title(title)\n    plt.colorbar()\n    plt.xticks([0, 1], [\"pred 0\", \"pred 1\"])\n    plt.yticks([0, 1], [\"true 0\", \"true 1\"])\n    for i in range(2):\n        for j in range(2):\n            plt.text(j, i, cm[i, j], ha=\"center\", va=\"center\")\n    plt.tight_layout()\n    plt.show()\n\nplot_cm(cm_default, \"Confusion matrix – threshold=0.50 (OOF)\")\nplot_cm(cm_cal, f\"Confusion matrix – threshold={thr_calibrated:.3f} (OOF)\")\n\n\nPRZED kalibracją progu (threshold=0.50) – metryki OOF:\n  accuracy : 0.9693\n  precision: 0.9580\n  recall   : 0.9540\n  f1       : 0.9560\n  roc_auc  : 0.9951\n\nMacierz pomyłek (rows=true, cols=pred):\n [[434  10]\n [ 11 228]]\n\n\n\n\n\n\n\n\n\n\nSkalibrowany próg (Youden J): 0.1299\n\nPO kalibracji progu (Youden J) – metryki OOF:\n  accuracy : 0.9751\n  precision: 0.9370\n  recall   : 0.9958\n  f1       : 0.9655\n  roc_auc  : 0.9951\n\nMacierz pomyłek (rows=true, cols=pred):\n [[428  16]\n [  1 238]]",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Modele liniowe i dyskryminacyjne</span>"
    ]
  },
  {
    "objectID": "chapters/03-klasyfikacja-liniowa.html#modele-dyskryminacyjne",
    "href": "chapters/03-klasyfikacja-liniowa.html#modele-dyskryminacyjne",
    "title": "4  Modele liniowe i dyskryminacyjne",
    "section": "4.5 Modele dyskryminacyjne",
    "text": "4.5 Modele dyskryminacyjne\nAnaliza dyskryminacyjna to klasa klasycznych metod klasyfikacji, które bardzo naturalnie wpisują się w logikę uczenia nadzorowanego: mając etykiety klas \\(y \\in \\{1,\\dots,K\\}\\), uczymy parametry rozkładów cech w każdej klasie (część „generatywna”), a następnie stosujemy regułę Bayesa do przypisania nowej obserwacji do najbardziej prawdopodobnej klasy. W odróżnieniu od regresji logistycznej, która modeluje bezpośrednio \\(\\mathbb{P}(y\\mid x)\\), LDA/QDA modelują \\(\\mathbb{P}(x\\mid y)\\) oraz priory \\(\\mathbb{P}(y)\\), po czym wyznaczają \\(\\mathbb{P}(y\\mid x)\\) pośrednio.\nHistorycznie do analizy dyskryminacyjnej dochodzono co najmniej dwiema drogami. Pierwsza (zwykle kojarzona z Fisherem) wynikała z problemu znalezienia kierunku projekcji, na którym klasy są najlepiej rozdzielone w sensie stosunku wariancji „między klasami” do wariancji „wewnątrz klas” (Fisher 1936). Druga droga (WardJR i Hook 1963)ma charakter „decyzyjno-probabilistyczny”: zaczynamy od założeń o rozkładach klas (najczęściej normalnych) i z reguły Bayesa wyprowadzamy regułę klasyfikacji w postaci porównania pewnych funkcji punktacji dla klas. Ten drugi sposób jest bardzo bliski temu, jak dziś prezentuje się LDA/QDA w uczeniu maszynowym (jako klasyfikatory generatywne) 1.\n1 Fisher (1936) pokazał ideę rozdzielania klas przez znalezienie projekcji maksymalizującej separację (stosunek wariancji między klasami do wariancji wewnątrz klas) i zastosował ją do danych irysów. Ward (1963) jest często przywoływany w kontekście rozwoju metod grupowania i analiz wielowymiarowych; w praktyce dydaktycznej warto podkreślić, że równolegle do „drogi Fishera” (kryterium separacji/projekcji) rozwijała się droga „proceduralna” w analizie wielowymiarowej: najpierw grupowanie/definicja grup, potem konstrukcja funkcji dyskryminacyjnych do klasyfikacji i rozumienia różnic między grupami.\n4.5.1 Założenia modelu: normalność wielowymiarowa i rozkłady apriori klas\nW klasycznym wariancie zakładamy, że dla każdej klasy \\(k\\) wektor cech ma rozkład normalny:\n\\[\nx \\mid (y=k) \\sim \\mathcal{N}(\\mu_k, \\Sigma_k),\n\\qquad \\pi_k = \\mathbb{P}(y=k).\n\\]\nParametry \\(\\mu_k\\), \\(\\Sigma_k\\) oraz \\(\\pi_k\\) są nieznane i są uczone na danych treningowych z wykorzystaniem etykiet klas (czyli w pełni nadzorowanie). W praktyce: \\(\\pi_k\\) estymujemy jako częstość klasy w treningu, \\(\\mu_k\\) jako średnią wektora cech w klasie, a \\(\\Sigma\\) lub \\(\\Sigma_k\\) jako macierze kowariancji (wspólne lub klasowe, zależnie od wariantu).\n\n\n4.5.2 Funkcje dyskryminacyjne: po co są i co robią?\nReguła Bayesa mówi, że przy równych kosztach błędu optymalnie klasyfikujemy do klasy o największym prawdopodobieństwie a posteriori:\n\\[\n\\hat{y}(x) = \\arg\\max_k \\mathbb{P}(y=k\\mid x).\n\\]\nPonieważ\n\\[\n\\mathbb{P}(y=k\\mid x) \\propto \\pi_k \\, f_k(x),\n\\]\ngdzie \\(f_k(x)\\) to gęstość \\(\\mathcal{N}(\\mu_k,\\Sigma_k)\\), wygodniej porównywać logarytmy (rosną monotonicznie), definiując funkcję dyskryminacyjną:\n\\[\n\\delta_k(x) \\;=\\; \\log \\pi_k + \\log f_k(x) \\;+\\; \\text{(stała niezależna od }k\\text{)}.\n\\]\nFunkcja dyskryminacyjna jest więc „punktacją” klasy: im większa \\(\\delta_k(x)\\), tym bardziej model preferuje klasę \\(k\\) dla obserwacji \\(x\\). Klasyfikacja sprowadza się do:\n\\[\n\\hat{y}(x) = \\arg\\max_k \\delta_k(x).\n\\]\n\n\n4.5.3 LDA vs QDA\nQDA - zakładamy osobną macierz kowariancji dla każdej klasy: \\(\\Sigma_k\\) jest w pełni dowolna (symetryczna dodatnio określona) i estymowana osobno. Wtedy\n\\[\n\\delta_k(x)\n=\n-\\frac{1}{2}\\log|\\Sigma_k|\n-\\frac{1}{2}(x-\\mu_k)^\\top \\Sigma_k^{-1}(x-\\mu_k)\n+\\log\\pi_k.\n\\]\nPonieważ składnik \\((x-\\mu_k)^\\top \\Sigma_k^{-1}(x-\\mu_k)\\) jest formą kwadratową, granice decyzyjne między klasami są kwadratowe (nieliniowe w \\(x\\)). QDA jest bardziej elastyczna (potrafi modelować klasy o różnych „kształtach” i orientacjach w przestrzeni cech), ale płaci za to większą liczbą parametrów, co wymaga większej próbki treningowej dla stabilnej estymacji.\nW LDA zakładamy wspólną kowariancję dla klas:\n\\[\n\\Sigma_k = \\Sigma \\quad \\text{dla każdego }k.\n\\]\nWtedy człony kwadratowe w \\(x\\) „redukują się” w porównaniu między klasami i funkcja dyskryminacyjna upraszcza się do postaci liniowej:\n\\[\n\\delta_k(x) = x^\\top \\Sigma^{-1}\\mu_k - \\frac{1}{2}\\mu_k^\\top \\Sigma^{-1}\\mu_k + \\log\\pi_k.\n\\]\nGranice decyzyjne są liniowe (hiperpłaszczyzny), bo \\(\\delta_k(x)\\) jest liniowa ze względu na \\(x\\). LDA ma zwykle mniejszą wariancję estymacji (mniej parametrów niż QDA), dlatego często jest konkurencyjna nawet wtedy, gdy prawdziwa zależność nie jest idealnie zgodna z założeniami.\n\n\n4.5.4 \\(\\Sigma=I\\), \\(\\Sigma_k=\\Sigma\\), \\(\\Sigma_k\\) dowolne\nW praktyce warto widzieć analizę dyskryminacyjną jako rodzinę modeli wynikającą z tego, jak restrykcyjnie opisujemy kowariancję.\n\n\\(\\Sigma = I\\). To najbardziej restrykcyjny wariant. Oznacza, że w każdej klasie cechy są „niezależne” i mają tę samą wariancję (w odpowiedniej skali). Wtedy odległość Mahalanobisa redukuje się do euklidesowej i reguła klasyfikacji staje się bardzo bliska nearest centroid (najbliższe centrum klasy). Jest to model prosty i często zaskakująco skuteczny po standaryzacji, ale może być niedopasowany, gdy cechy są skorelowane.\n\\(\\Sigma_k = \\Sigma\\) (LDA: wspólna, ale dowolna macierz kowariancji). To kompromis: dopuszczamy korelacje i różne wariancje cech, ale zakładamy, że „kształt rozkładu” w przestrzeni cech jest taki sam dla wszystkich klas, tylko przesunięty o różne \\(\\mu_k\\). Wtedy granice są liniowe.\n\\(\\Sigma_k\\) dowolne (QDA). Najbardziej elastyczne: każda klasa ma własny „kształt” i orientację elipsoidy kowariancji. Granice są kwadratowe. Ten wariant jest najbardziej wrażliwy na małe próby (estymacja \\(\\Sigma_k^{-1}\\) bywa niestabilna), dlatego często wymaga albo większej liczby obserwacji, albo regularizacji (np. RDA – regularized discriminant analysis; klasycznie opisane przez Friedmana). ￼\n\nUwaga praktyczna: spotyka się też wariant pośredni \\(\\Sigma\\) diagonalna (brak korelacji, ale różne wariancje cech). Jest to bliskie „gaussowskiemu naiwnemu Bayesowi” w wersji z normalnymi rozkładami cech.\n\n\n4.5.5 Jak estymuje się parametry w uczeniu nadzorowanym?\nDla danych treningowych \\(\\{(x_i,y_i)\\}_{i=1}^n\\), gdzie \\(n_k\\) to liczba obserwacji w klasie \\(k\\), standardowe estymatory (MLE) mają postać:\n\\[\n\\hat{\\pi}_k = \\frac{n_k}{n},\n\\qquad\n\\hat{\\mu}_k = \\frac{1}{n_k}\\sum_{i:\\,y_i=k} x_i.\n\\]\nDla LDA estymujemy wspólną kowariancję jako kowariancję „wewnątrzklasową” (pooled covariance):\n\\[\n\\hat{\\Sigma}\n=\n\\frac{1}{n-K}\n\\sum_{k=1}^K\n\\sum_{i:\\,y_i=k}\n(x_i-\\hat{\\mu}_k)(x_i-\\hat{\\mu}_k)^\\top.\n\\]\nDla QDA estymujemy \\(\\hat{\\Sigma}_k\\) osobno w każdej klasie:\n\\[\n\\hat{\\Sigma}_k\n=\n\\frac{1}{n_k-1}\n\\sum_{i:\\,y_i=k}\n(x_i-\\hat{\\mu}_k)(x_i-\\hat{\\mu}_k)^\\top.\n\\]\nNastępnie do klasyfikacji używamy \\(\\delta_k(x)\\) z odpowiedniego wariantu (LDA/QDA). W praktyce w scikit-learn jest to realizowane wprost (fit → estymuje \\(\\pi_k\\), \\(\\mu_k\\), \\(\\Sigma\\) lub \\(\\Sigma_k\\); predict/predict_proba → liczy \\(\\delta_k\\) i normalizuje do prawdopodobieństw).\n\nPrzykład 4.3 Poniżej przykład LDA i QDA na klasycznym zbiorze Iris dostępny na Hugging Face (scikit-learn/iris). Zbiór zawiera 150 obserwacji trzech gatunków irysów, opisanych czterema cechami liczbowymi. ￼\nW przykładzie:\n\nrobimy podział train/test,\nuczymy LDA i QDA,\nraportujemy accuracy i macierz pomyłek,\npokazujemy log-loss jako miarę jakości probabilistycznej,\nnarysujemy brzegi decyzyjne obu metod.\n\n\n\nKod\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom datasets import load_dataset\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\nfrom sklearn.metrics import accuracy_score, confusion_matrix, log_loss\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n\n# 1) Wczytanie Iris z Hugging Face\nds = load_dataset(\"scikit-learn/iris\")[\"train\"]\ndf = ds.to_pandas()\ndf = df.iloc[:, 1:]  # Usunięcie pierwszej kolumny (Id)\n\ntarget = \"Species\"\nX = df.select_dtypes(include=[\"number\"]).drop(columns=[target], errors=\"ignore\")\n\n# Kodujemy klasy do liczb (wymagane m.in. do rysowania brzegów decyzyjnych przez contourf)\ny_cat = df[target].astype(\"category\")\nclass_names = list(y_cat.cat.categories)\ny = y_cat.cat.codes\n\n# 2) Podział train/test (stratyfikacja dla klas)\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.25, random_state=42, stratify=y\n)\n\n# 2b) Standaryzacja (fit TYLKO na train) – cały przykład działa na danych standaryzowanych\nscaler = StandardScaler()\nX_train_std = scaler.fit_transform(X_train)\nX_test_std = scaler.transform(X_test)\nX_train_std_df = pd.DataFrame(X_train_std, columns=X.columns, index=X_train.index)\nX_test_std_df = pd.DataFrame(X_test_std, columns=X.columns, index=X_test.index)\n\n# 3) LDA\nlda = LinearDiscriminantAnalysis()\nlda.fit(X_train_std_df, y_train)\npred_lda = lda.predict(X_test_std_df)\nproba_lda = lda.predict_proba(X_test_std_df)\n\n# 4) QDA\nqda = QuadraticDiscriminantAnalysis()\nqda.fit(X_train_std_df, y_train)\npred_qda = qda.predict(X_test_std_df)\nproba_qda = qda.predict_proba(X_test_std_df)\n\n# 5) Metryki i macierze pomyłek\nacc_lda = accuracy_score(y_test, pred_lda)\nacc_qda = accuracy_score(y_test, pred_qda)\n\ncm_lda = confusion_matrix(y_test, pred_lda)\ncm_qda = confusion_matrix(y_test, pred_qda)\n\nprint(f\"LDA  accuracy: {acc_lda:.3f}, log_loss: {log_loss(y_test, proba_lda):.3f}\")\nprint(\"LDA confusion matrix:\\n\", pd.DataFrame(cm_lda, index=class_names, columns=class_names))\n\nprint(f\"\\nQDA  accuracy: {acc_qda:.3f}, log_loss: {log_loss(y_test, proba_qda):.3f}\")\nprint(\"QDA confusion matrix:\\n\", pd.DataFrame(cm_qda, index=class_names, columns=class_names))\n\n# 6) Brzegi decyzyjne (wizualizacja 2D)\n# Uwaga: Iris ma 4 cechy, a wykres 2D wymaga wyboru 2 osi.\nfeat_cols = list(X.columns[[0,2]])\nall_feat_cols = list(X.columns)\n\ndef plot_decision_boundary_fullmodel_on_2features(\n    ax,\n    model,\n    X_std_df,\n    y_vis,\n    all_feature_names,\n    vary_feature_names,\n    title,\n    n_classes,\n):\n    # Decision boundary jako przekrój przestrzeni cech:\n    # zmieniamy tylko 2 wybrane cechy, pozostałe ustawiamy na 0 (średnia po standaryzacji).\n    x0 = X_std_df[vary_feature_names[0]].to_numpy()\n    x1 = X_std_df[vary_feature_names[1]].to_numpy()\n\n    x0_min, x0_max = x0.min() - 0.5, x0.max() + 0.5\n    x1_min, x1_max = x1.min() - 0.5, x1.max() + 0.5\n\n    xx0, xx1 = np.meshgrid(\n        np.linspace(x0_min, x0_max, 300),\n        np.linspace(x1_min, x1_max, 300),\n    )\n\n    base = np.zeros((xx0.size, len(all_feature_names)), dtype=float)\n    idx0 = all_feature_names.index(vary_feature_names[0])\n    idx1 = all_feature_names.index(vary_feature_names[1])\n    base[:, idx0] = xx0.ravel()\n    base[:, idx1] = xx1.ravel()\n\n    grid_df = pd.DataFrame(base, columns=all_feature_names)\n    zz = model.predict(grid_df).reshape(xx0.shape)\n\n    levels = np.arange(n_classes + 1) - 0.5\n    cmap = plt.get_cmap(\"tab10\", n_classes)\n\n    ax.contourf(xx0, xx1, zz, levels=levels, alpha=0.25, cmap=cmap)\n    ax.scatter(x0, x1, c=y_vis, cmap=cmap, edgecolor=\"k\", s=35)\n    ax.set_xlabel(vary_feature_names[0])\n    ax.set_ylabel(vary_feature_names[1])\n    ax.set_title(title)\n\nn_classes = int(np.unique(y).size)\n\nfig, axes = plt.subplots(1, 2, figsize=(11, 4))\nplot_decision_boundary_fullmodel_on_2features(\n    axes[0],\n    lda,\n    X_test_std_df,\n    y_test,\n    all_feature_names=all_feat_cols,\n    vary_feature_names=feat_cols,\n    title=\"LDA – brzeg decyzyjny (przekrój po 2 cechach; model pełny)\",\n    n_classes=n_classes,\n)\nplot_decision_boundary_fullmodel_on_2features(\n    axes[1],\n    qda,\n    X_test_std_df,\n    y_test,\n    all_feature_names=all_feat_cols,\n    vary_feature_names=feat_cols,\n    title=\"QDA – brzeg decyzyjny (przekrój po 2 cechach; model pełny)\",\n    n_classes=n_classes,\n)\nplt.tight_layout()\nplt.show()\n\n# 7) Brzegi decyzyjne w przestrzeni PCA(2)\n# PCA robimy po standaryzacji.\n# Brzeg decyzyjny liczymy w przestrzeni PC1/PC2, ale predykcje robi model uczony\n# na pełnych cechach: PC-grid -&gt; inverse_transform -&gt; predykcja.\npca = PCA(n_components=2, random_state=42)\nX_train_pca = pca.fit_transform(X_train_std)\nX_test_pca = pca.transform(X_test_std)\n\npc_cols = [\"PC1\", \"PC2\"]\nX_train_pca_df = pd.DataFrame(X_train_pca, columns=pc_cols, index=X_train.index)\nX_test_pca_df = pd.DataFrame(X_test_pca, columns=pc_cols, index=X_test.index)\n\ndef plot_decision_boundary_fullmodel_in_pca2(\n    ax,\n    model,\n    pca,\n    X_pca_df,\n    y_vis,\n    orig_feature_names,\n    title,\n    n_classes,\n):\n    x0 = X_pca_df[\"PC1\"].to_numpy()\n    x1 = X_pca_df[\"PC2\"].to_numpy()\n\n    x0_min, x0_max = x0.min() - 0.5, x0.max() + 0.5\n    x1_min, x1_max = x1.min() - 0.5, x1.max() + 0.5\n\n    xx0, xx1 = np.meshgrid(\n        np.linspace(x0_min, x0_max, 300),\n        np.linspace(x1_min, x1_max, 300),\n    )\n\n    pc_grid = np.c_[xx0.ravel(), xx1.ravel()]\n    grid_std = pca.inverse_transform(pc_grid)\n    grid_std_df = pd.DataFrame(grid_std, columns=orig_feature_names)\n    zz = model.predict(grid_std_df).reshape(xx0.shape)\n\n    levels = np.arange(n_classes + 1) - 0.5\n    cmap = plt.get_cmap(\"tab10\", n_classes)\n\n    ax.contourf(xx0, xx1, zz, levels=levels, alpha=0.25, cmap=cmap)\n    ax.scatter(x0, x1, c=y_vis, cmap=cmap, edgecolor=\"k\", s=35)\n    ax.set_xlabel(\"PC1\")\n    ax.set_ylabel(\"PC2\")\n    ax.set_title(title)\n\nfig, axes = plt.subplots(1, 2, figsize=(11, 4))\nplot_decision_boundary_fullmodel_in_pca2(\n    axes[0],\n    lda,\n    pca,\n    X_test_pca_df,\n    y_test,\n    orig_feature_names=all_feat_cols,\n    title=\"LDA – brzeg decyzyjny (PCA2; model pełny)\",\n    n_classes=n_classes,\n)\nplot_decision_boundary_fullmodel_in_pca2(\n    axes[1],\n    qda,\n    pca,\n    X_test_pca_df,\n    y_test,\n    orig_feature_names=all_feat_cols,\n    title=\"QDA – brzeg decyzyjny (PCA2; model pełny)\",\n    n_classes=n_classes,\n)\nplt.tight_layout()\nplt.show()\n\n\nLDA  accuracy: 1.000, log_loss: 0.024\nLDA confusion matrix:\n                  Iris-setosa  Iris-versicolor  Iris-virginica\nIris-setosa               12                0               0\nIris-versicolor            0               13               0\nIris-virginica             0                0              13\n\nQDA  accuracy: 1.000, log_loss: 0.017\nQDA confusion matrix:\n                  Iris-setosa  Iris-versicolor  Iris-virginica\nIris-setosa               12                0               0\nIris-versicolor            0               13               0\nIris-virginica             0                0              13\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFisher, R. A. 1936. „The Use of Multiple Measurements in Taxonomic Problems”. Annals of Eugenics 7 (2): 179–88. https://doi.org/10.1111/j.1469-1809.1936.tb02137.x.\n\n\nWardJR, Joe H., i Marion E. Hook. 1963. „Application of an Hierarchical Grouping Procedure to a Problem of Grouping Profiles”. Educational and Psychological Measurement 23 (1): 69–81. https://doi.org/10.1177/001316446302300107.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Modele liniowe i dyskryminacyjne</span>"
    ]
  },
  {
    "objectID": "chapters/04-drzewa-zespoly.html",
    "href": "chapters/04-drzewa-zespoly.html",
    "title": "5  Drzewa decyzyjne i zespoly modeli",
    "section": "",
    "text": "5.1 Podstawowe drzewa decyzyjne",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Drzewa decyzyjne i zespoly modeli</span>"
    ]
  },
  {
    "objectID": "chapters/04-drzewa-zespoly.html#podstawowe-drzewa-decyzyjne",
    "href": "chapters/04-drzewa-zespoly.html#podstawowe-drzewa-decyzyjne",
    "title": "5  Drzewa decyzyjne i zespoly modeli",
    "section": "",
    "text": "5.1.1 Klasyczne rodziny algorytmów drzew i ich różnice\nW literaturze i praktyce spotyka się kilka „rodzin” algorytmów drzew decyzyjnych. Różnią się one m.in. sposobem doboru podziału (kryterium jakości), dopuszczalną liczbą gałęzi w węźle, obsługą braków danych, strategią przycinania oraz tym, czy wnioski statystyczne są wbudowane w procedurę uczenia.\nID3 (Iterative Dichotomiser 3) (J. R. Quinlan 1986) to jeden z najwcześniejszych, wpływowych algorytmów drzew klasyfikacyjnych. Uczy drzewo wielogałęziowe (tzn. węzeł może mieć tyle gałęzi, ile kategorii ma dana cecha) i wybiera podział na podstawie zysku informacji (information gain), który jest redukcją entropii po podziale. W klasycznej postaci ID3 był projektowany głównie dla cech kategorycznych, nie posiadał pełnego, ustandaryzowanego mechanizmu przycinania i jest wrażliwy na cechy o dużej liczbie kategorii (mogą sztucznie „wygrywać” kryterium entropijne). Z perspektywy ogólnego spojrzenia na drzewa decyzyjne ważne jest to, że ID3 ustanowił wzorzec: rekurencyjne dzielenie + entropia/zysk informacji.\nC4.5 (J. Ross Quinlan 1993) jest rozwinięciem ID3 i przez wiele lat był stosowanym standardem. Wprowadza on m.in. (i) obsługę zmiennych ciągłych przez poszukiwanie progu i podziały binarne dla cech liczbowych, (ii) modyfikację kryterium jakości podziału w postaci współczynnika zysku informacji (gain ratio), który koryguje preferencję dla cech o wielu kategoriach, (iii) obsługę braków danych przez rozdzielanie obserwacji z brakami „miękko” (z wagami) między gałęzie lub przez dopasowane heurystyki, oraz (iv) przycinanie oparte o oszacowania błędu (tzw. error-based pruning). W praktyce C4.5 produkuje drzewa, które są zwykle mniejsze i bardziej uogólniające niż ID3.\nC5.0 to następca C4.5 (Kuhn i Quinlan 2018), zaprojektowany jako szybszy i bardziej skalowalny, z licznymi usprawnieniami inżynieryjnymi i heurystycznymi. Typowo oferuje lepszą wydajność obliczeniową, mniejsze zużycie pamięci oraz dodatkowe możliwości praktyczne (np. kosztowną klasyfikację, mechanizmy „wzmocnienia” w stylu boosting/committee w niektórych implementacjach). Koncepcyjnie nadal jest to drzewo „w duchu Quinlana”: kryteria entropijne, sprawna obsługa cech ciągłych i braków oraz silny nacisk na praktyczne uogólnianie.\nCART (Classification and Regression Trees) (Breiman i in. 2017) ujednolica podejście do klasyfikacji i regresji w ramach jednego formalizmu. Charakterystyczne cechy CART są następujące: (i) podziały są zazwyczaj binarne (nawet dla cech kategorycznych – kategorie dzieli się na dwie grupy), (ii) w klasyfikacji stosuje się zwykle indeks Giniego (lub entropię), a w regresji kryterium oparte o SSE/wariancję, (iii) kluczowym elementem jest przycinanie złożoności (cost-complexity pruning), tj. wybór drzewa przez kompromis między dopasowaniem i złożonością, oraz (iv) mechanizm reguł zastępczych (surrogate splits) jako systematyczna odpowiedź na braki danych. CART jest dziś szczególnie ważne, bo stanowi bazę dla wielu metod zespołowych (random forest, gradient boosting) i jest najczęściej spotykaną „architekturą” drzew w ML.\nConditional inference trees (conditional trees, ctree) (Hothorn, Hornik, i Zeileis 2006) powstały jako odpowiedź na znane uprzedzenia klasycznych kryteriów podziału (Gini/entropia/SSE), które mogą faworyzować cechy o wielu możliwych podziałach (np. zmienne ciągłe lub kategoryczne o wielu poziomach). W ctree wybór podziału ma charakter statystyczny i opiera się na testach permutacyjnych niezależności. Procedura jest dwuetapowa: najpierw w danym węźle testuje się hipotezę globalną, że zmienna docelowa jest niezależna od wszystkich cech (jeśli brak podstaw do jej odrzucenia, węzeł staje się liściem), a następnie – w razie istotności – wykonuje się testy cząstkowe dla każdej cechy i wybiera tę o najsilniejszej zależności z odpowiednią korektą na wielokrotne porównania. Typ testu zależy od typu zmiennej docelowej i predyktora: dla klasyfikacji (\\(Y\\) kategoryczne) stosuje się permutacyjne testy niezależności odpowiadające m.in. testom \\(\\chi^2\\) (gdy \\(X\\) kategoryczne) lub testom porównania rozkładów/średnich typu ANOVA/F (gdy \\(X\\) ciągłe), natomiast dla regresji (\\(Y\\) ciągłe) stosuje się permutacyjne testy zależności odpowiadające testom korelacyjnym/regresyjnym (gdy \\(X\\) ciągłe) albo testom różnic średnich typu ANOVA (gdy \\(X\\) kategoryczne) – zawsze jednak w wersji permutacyjnej. Dzięki temu conditional trees redukują tendencyjność selekcji zmiennych, a kryterium stopu jest naturalnie powiązane z istotnością statystyczną, a nie wyłącznie z heurystycznymi parametrami złożoności.\nCHAID (Chi-squared Automatic Interaction Detection) (Kass 1980) to klasyczna metoda drzew wielogałęziowych, szczególnie popularna w analizie marketingowej i badaniach społecznych. Jej znakiem rozpoznawczym są: (i) podziały wybierane na podstawie testów chi-kwadrat (dla klasyfikacji) lub testów analogicznych dla zmiennych ciągłych, (ii) możliwość łączenia kategorii cech kategorycznych w większe grupy przed wykonaniem podziału, oraz (iii) częste stosowanie podziałów wielogałęziowych (niekoniecznie binarnych). CHAID jest ceniony za interpretowalność i naturalne traktowanie interakcji w kategoriach, ale w porównaniu do CART bywa mniej „ML-owy” w sensie typowych współczesnych pipeline’ów i częściej występuje w narzędziach statystycznych/BI.\n\n\n5.1.2 Rodzaje drzew i podstawowe elementy konstrukcji\nW praktyce wyróżnia się przede wszystkim:\n\ndrzewa klasyfikacyjne – gdy zmienna docelowa jest dyskretna (klasy),\ndrzewa regresyjne – gdy zmienna docelowa jest ciągła,\ndrzewa wieloklasowe – naturalne rozszerzenie klasyfikacji binarnej,\ndrzewa binarne vs wielogałęziowe – CART stosuje zwykle podziały binarne, natomiast w niektórych wariantach dopuszcza się podziały na wiele gałęzi (częściej dla cech kategorycznych).\n\nDrzewo składa się z:\n\nkorzenia (root) – węzła startowego zawierającego cały zbiór uczący,\nwęzłów wewnętrznych (internal nodes) – zawierają regułę podziału (cecha + warunek),\ngałęzi (branches) – odpowiadają wynikom reguły (np. „tak/nie” dla podziału binarnego),\nliści (leaves/terminal nodes) – węzłów końcowych przechowujących predykcję (klasę lub wartość),\n(opcjonalnie) wag/rozkładów w liściu – np. częstości klas lub parametry prostej regresji w liściu.\n\n\n\n\n5.1.3 Reguły podziału: klasyfikacja i regresja\nWęzeł drzewa ma za zadanie wybrać taki podział, który „poprawia jednorodność” powstałych grup. Formalnie dla węzła zawierającego zbiór obserwacji \\(S\\) rozważamy kandydatów podziału \\(s\\) (cecha \\(j\\) i próg \\(t\\), ewentualnie podział kategorii). Podział rozdziela \\(S\\) na \\(S_L\\) i \\(S_R\\). Wybieramy \\(s\\), który maksymalizuje spadek nieczystości (impurity decrease):\n\\[\n\\Delta I(s) = I(S) - \\frac{|S_L|}{|S|} I(S_L) - \\frac{|S_R|}{|S|} I(S_R).\n\\]\nDrzewa klasyfikacyjne. Nieczystość \\(I(S)\\) definiuje się najczęściej jako:\n\nindeks Giniego: \\[\nI_G(S) = 1 - \\sum_{k=1}^K p_k^2,\n\\]\nentropię (Shannon): \\[\nI_H(S) = -\\sum_{k=1}^K p_k \\log p_k,\n\\]\n\ngdzie \\(p_k\\) to odsetek obserwacji klasy \\(k\\) w węźle. Intuicyjnie, im bardziej rozkład klas jest skoncentrowany w jednej klasie, tym mniejsza nieczystość. Podział jest „dobry”, jeśli znacząco zwiększa jednorodność klas w dzieciach.\nDrzewa regresyjne. W regresji nieczystość mierzy się rozproszeniem wartości \\(y\\) w węźle. Najczęściej stosuje się wariancję lub równoważnie sumę kwadratów odchyleń od średniej (SSE):\n\\[\nI_{\\text{SSE}}(S) = \\sum_{i\\in S} (y_i - \\bar{y}_S)^2.\n\\]\nPodział wybieramy tak, aby minimalizować łączną SSE po podziale (czyli maksymalizować jej redukcję). W liściu predykcją jest zwykle \\(\\bar{y}_S\\) (średnia w liściu), co jest rozwiązaniem minimalizującym SSE w obrębie liścia.\n\n\n5.1.4 Jak szuka się optymalnego podziału\nW idealnym (globalnym) ujęciu chcielibyśmy znaleźć takie drzewo, które minimalizuje błąd predykcji przy zadanej „złożoności” (np. liczbie liści). Taki problem jest jednak obliczeniowo bardzo trudny: liczba możliwych drzew rośnie wykładniczo wraz z liczbą obserwacji i cech, a wybór optymalnej struktury wymagałby przeszukania ogromnej przestrzeni kombinatorycznej. Dlatego praktyczne algorytmy drzew (CART, C4.5, CHAID, ctree) stosują podejście zachłanne (greedy, top–down recursive partitioning): w każdym węźle wybierają najlepszy lokalnie podział według zadanego kryterium i dopiero potem powtarzają procedurę w węzłach potomnych.\nW podejściu zachłannym konstrukcja drzewa ma postać iteracji:\n\nW węźle z danymi \\(S\\) rozważamy zbiór kandydatów podziału \\(s\\) (cecha \\(j\\) i warunek podziału).\nDla każdego kandydata obliczamy spadek nieczystości \\(\\Delta I(s)\\) (dla klasyfikacji: Gini/entropia; dla regresji: SSE/wariancja).\nWybieramy \\(s^* = \\arg\\max_s \\Delta I(s)\\), wykonujemy podział \\(S \\to (S_L,S_R)\\).\nRekurencyjnie powtarzamy kroki 1–3 w węzłach potomnych, dopóki nie zajdzie warunek stopu.\n\n\n5.1.4.1 Cechy ciągłe: jak wyznacza się kandydatów progów\nDla cechy ciągłej \\(x_j\\) naturalną regułą podziału jest próg \\(t\\): \\(x_j \\le t\\) vs \\(x_j &gt; t\\). Kandydatami progów nie są wszystkie liczby rzeczywiste, lecz wartości „pomiędzy” obserwacjami. W praktyce postępuje się tak:\n\nsortuje się obserwacje według \\(x_j\\),\nrozważa się progi będące środkami między kolejnymi różnymi wartościami \\(x_j\\), tj. \\(t = (v_{(m)} + v_{(m+1)})/2\\),\nczęsto pomija się progi, które nie zmieniają przypisań (np. wiele powtórzeń wartości) lub które łamią ograniczenia typu min_samples_leaf.\n\nDzięki temu liczba kandydatów dla jednej cechy jest rzędu \\(O(n)\\), a nie nieskończona. W implementacjach produkcyjnych dodatkowo używa się trików obliczeniowych: po posortowaniu można aktualizować liczności klas (lub sumy/kwadraty sum w regresji) „przesuwając” próg krok po kroku, bez przeliczania wszystkiego od zera, co istotnie przyspiesza selekcję najlepszego \\(t\\).\n\n\n5.1.4.2 Cechy kategoryczne: podziały binarne i wielogałęziowe\nDla cech kategorycznych istnieją dwie główne szkoły:\n\npodziały wielogałęziowe (np. w ID3/C4.5/CHAID): węzeł może mieć osobną gałąź dla każdej kategorii; to bywa bardzo interpretowalne, ale może prowadzić do „fragmentacji” danych (małe liczności w gałęziach),\npodziały binarne (CART): kategorie dzieli się na dwie grupy \\(A\\) i \\(\\bar A\\), tzn. \\(x_j \\in A\\) vs \\(x_j \\notin A\\).\n\nPodziały binarne dla cechy o \\(m\\) kategoriach mają w najgorszym razie \\(2^{m-1}-1\\) możliwych podziałów, co szybko staje się niepraktyczne. Dlatego stosuje się heurystyki i uproszczenia. Przykładowo:\n\nw regresji porządkuje się kategorie według średniej \\(\\bar y\\) i rozważa rozcięcia jak dla zmiennej uporządkowanej,\nw klasyfikacji można porządkować kategorie według \\(\\hat p(y=1\\mid x_j)\\) (dla binarnej klasy) lub stosować przybliżone przeszukiwanie,\nprzy dużej liczbie poziomów stosuje się łączenie rzadkich kategorii do other lub narzuca minimalne liczności.\n\n\n\n5.1.4.3 Braki danych a wybór podziału\nW zależności od algorytmu i implementacji, braki mogą być obsługiwane na kilka sposobów: przez wcześniejszą imputację, przez traktowanie „braku” jako osobnej kategorii (częste w praktyce), przez wybór domyślnego kierunku podziału (np. brak \\(\\to\\) lewa gałąź) lub przez mechanizmy takie jak surrogate splits w CART 1.\n1 Reguły zastępcze to mechanizm kojarzony przede wszystkim z CART, używany gdy dla obserwacji brakuje wartości cechy, według której w danym węźle wykonywany jest podział. Zamiast odrzucać obserwację lub imputować brak, drzewo może wybrać alternatywną regułę podziału opartą o inną cechę, która możliwie najlepiej „naśladuje” podział główny. W praktyce buduje się ranking reguł zastępczych na podstawie zgodności przypisań do gałęzi (np. jak często podział zastępczy wysyła obserwacje do tej samej strony co podział główny w danych treningowych). Dzięki temu drzewo zachowuje spójność działania nawet przy brakach danych.\n\n5.1.4.4 Kontrola złożoności: pre-pruning i (w CART) post-pruning\nPonieważ strategia zachłanna łatwo prowadzi do bardzo głębokich drzew, w praktyce kontroluje się złożoność na dwa sposoby:\n\npre-pruning (ograniczenia w trakcie budowy) - zamiast budować bardzo głębokie drzewo, można narzucić ograniczenia już na etapie wzrostu, aby zmniejszyć wariancję i ryzyko przeuczenia.\n\nmax_depth – maksymalna głębokość drzewa (liczba krawędzi/poziomów od korzenia do liścia). Małe wartości ograniczają liczbę kolejnych podziałów, co zwykle zwiększa bias, ale zmniejsza wariancję.\nmin_samples_split – minimalna liczba obserwacji w węźle, aby w ogóle rozważać jego podział. Jeśli węzeł ma mniej obserwacji, staje się liściem.\nmin_samples_leaf – minimalna liczba obserwacji, która musi pozostać w każdym liściu po podziale. W praktyce eliminuje to podziały tworzące bardzo małe, niestabilne liście (np. podział 3 vs 97 przy min_samples_leaf=10 jest niedozwolony).\nmax_leaf_nodes – maksymalna liczba liści w całym drzewie. To bezpośrednia kontrola złożoności, bo liczba liści determinuje liczbę regionów decyzyjnych.\nmin_impurity_decrease – minimalna wymagana redukcja nieczystości \\(\\Delta I(s)\\), aby zaakceptować podział. Jeśli najlepszy możliwy podział w węźle nie daje spadku nieczystości większego od progu, algorytm przerywa wzrost i tworzy liść.\n\npost-pruning (przycinanie po zbudowaniu dużego drzewa) - w CART standardowo buduje się najpierw drzewo „maksymalne” (silnie dopasowane, często aż do spełnienia minimalnych ograniczeń liczności), a następnie usuwa się jego gałęzie, wybierając takie poddrzewo, które najlepiej równoważy dopasowanie i złożoność. Klasyczne podejście to cost-complexity pruning (znane też jako weakest-link pruning), w którym rozważa się rodzinę poddrzew \\(T_\\alpha\\) minimalizujących kompromis\n\\[\nR(T) + \\alpha\\,|T|,\n\\]\ngdzie:\n\n\\(R(T)\\) jest miarą „błędu” lub „straty” drzewa (w CART często jest to błąd resubstytucji / suma nieczystości w liściach, np. suma SSE w regresji lub suma nieczystości Giniego ważona licznościami w klasyfikacji; w praktyce interesuje nas przede wszystkim zgodność tej miary z błędem generalizacji),\n\\(|T|\\) to liczba liści (terminal nodes) – prosta miara złożoności drzewa,\n\\(\\alpha \\ge 0\\) to parametr kary za złożoność: dla \\(\\alpha \\to 0\\) preferowane jest drzewo duże (mały nacisk na prostotę), a dla dużych \\(\\alpha\\) otrzymujemy coraz płytsze drzewa.\n\nJak przebiega przycinanie w CART (idea „najsłabszego ogniwa”). Dla każdego węzła wewnętrznego \\(t\\) rozważa się zastąpienie całego poddrzewa \\(T_t\\) pojedynczym liściem i ocenia się, „jak dużo poprawy dopasowania” daje to poddrzewo w przeliczeniu na „ile dodatkowych liści kosztuje”. Formalnie wyznacza się wskaźnik krytyczny (tzw. wartość \\(\\alpha\\) dla węzła):\n\\[\ng(t) = \\frac{R(t) - R(T_t)}{|T_t| - 1},\n\\]\ngdzie \\(R(t)\\) to strata, gdy \\(T_t\\) zostanie zastąpione jednym liściem, a \\(R(T_t)\\) to strata pełnego poddrzewa. W każdym kroku usuwa się to poddrzewo, które ma najmniejsze \\(g(t)\\) (czyli „najmniej opłaca się” je utrzymywać) – stąd nazwa weakest-link. Powtarzając ten krok, otrzymuje się skończoną sekwencję zagnieżdżonych poddrzew\n\\[\nT_0 \\supset T_1 \\supset \\cdots \\supset T_M,\n\\]\nodpowiadających rosnącym wartościom \\(\\alpha\\). Dzięki temu zamiast przeszukiwać wszystkie możliwe drzewa, analizujemy tylko tę sekwencję kandydatów.\nDobór \\(\\alpha\\) (czyli wybór końcowego drzewa) - parametr \\(\\alpha\\) dobiera się zwykle przez walidację krzyżową: dla każdego kandydata \\(T_m\\) estymuje się błąd generalizacji i wybiera drzewo o najlepszej jakości. Często stosuje się też zasadę 1-SE: wybiera się najmniejsze drzewo, którego błąd walidacyjny jest nie większy niż minimum powiększone o jedno odchylenie standardowe, aby preferować model prostszy i stabilniejszy.\n\n\n\n\n5.1.5 Reguły stopu (stopping rules)\nWzrost drzewa (rekurencyjne dzielenie) nie może trwać w nieskończoność. W pewnym momencie węzeł powinien zostać liściem. Z punktu widzenia konstrukcji algorytmu reguły stopu można podzielić na dwie grupy:\n\nReguły „merytoryczne” (lokalne) - mówią, że dalszy podział nie ma sensu, bo nie poprawia jakości lub nie da się go wykonać (np. wszystkie obserwacje należą do jednej klasy).\nReguły „regularizacyjne” (kontrola złożoności) - narzucają ograniczenia, aby ograniczyć przeuczenie (np. limit głębokości, minimalna liczność liścia).\n\n\n5.1.5.1 Typowe reguły stopu\nPoniżej najczęściej spotykane kryteria, które zatrzymują wzrost drzewa w danym węźle:\n\nCzystość węzła (purity).\n\nklasyfikacja: jeśli węzeł jest jednorodny, tzn. \\(p_k=1\\) dla pewnej klasy \\(k\\), to \\(I(S)=0\\) i dalszy podział nie ma sensu,\nregresja: analogicznie, jeśli wszystkie \\(y_i\\) w węźle są identyczne (wariancja = 0), podział nie poprawi SSE.\n\nBrak dopuszczalnego podziału. Dla każdej cechy może się okazać, że nie istnieje próg/kombinacja kategorii, która tworzy dwie niepuste gałęzie i spełnia ograniczenia liczności.\nMinimalna poprawa jakości (próg na \\(\\Delta I\\)). Jeżeli najlepszy możliwy podział w węźle daje redukcję nieczystości mniejszą niż ustalony próg, to przerywamy wzrost:\n\\[\n\\max_s \\Delta I(s) &lt; \\tau \\quad \\Rightarrow \\quad \\text{węzeł staje się liściem.}\n\\]\nMinimalna liczność węzła i liścia.\n\naby rozważać podział: \\(|S| \\ge n_{\\text{split}}\\),\naby zaakceptować podział: \\(|S_L|,|S_R| \\ge n_{\\text{leaf}}\\).\n\nOgraniczenie głębokości lub liczby liści. W praktyce kontroluje liczbę regionów decyzyjnych i stabilność modelu.\n\nW drzewach typu ctree (conditional inference) reguła stopu może wynikać wprost z testowania: jeśli brak istotnej zależności (po korekcji na wielokrotne porównania), węzeł staje się liściem. W klasycznym CART reguły stopu mają zwykle charakter heurystyczny/regularizacyjny (parametry złożoności), a dodatkową kontrolę zapewnia post-pruning.\n\n\n\n5.1.6 Predykcja z drzewa\nPredykcja polega na „przejściu” obserwacji przez drzewo od korzenia do liścia, wykonując po drodze kolejne testy.\n\nDrzewo klasyfikacyjne: w liściu przechowuje się rozkład klas (częstości) \\(\\hat{p}_k\\). Predykcją klasy jest zwykle \\(\\arg\\max_k \\hat{p}_k\\), a predykcją probabilistyczną – wektor \\((\\hat{p}_1,\\dots,\\hat{p}_K)\\).\nDrzewo regresyjne: w liściu przechowuje się wartość liczbową, najczęściej średnią \\(\\bar{y}_S\\) (lub medianę, zależnie od kryterium). Predykcja to ta wartość przypisana do liścia, do którego trafia obserwacja.\n\n\nPrzykład 5.1 (Przykład: drzewo klasyfikacyjne dla gatunku muzycznego (Spotify)) Poniżej budujemy proste drzewo klasyfikacyjne przewidujące gatunek muzyczny na podstawie numerycznych cech akustycznych utworów (np. danceability, energy, loudness, tempo). Jest to typowy przypadek danych tablicowych, gdzie drzewa decyzyjne są dobrym modelem bazowym.\nW tym przykładzie nie wykonujemy klasycznego preprocessingu typu standaryzacja/normalizacja. Wynika to z faktu, że reguły w drzewach mają postać progów porównujących wartości cech (np. \\(x_j \\le t\\)), więc skala zmiennych nie wpływa na sens podziału w taki sposób jak w metodach opartych o iloczyny skalarne (np. regresja logistyczna, SVM). Innymi słowy: drzewo nie korzysta z odległości euklidesowej ani z norm współczynników, tylko z porównań i wyboru progów, dlatego skalowanie cech nie jest wymagane. W praktyce pozostają jedynie „minimalne” kroki porządkowe: wybór cech liczbowych i ewentualne uzupełnienie braków (jeśli występują).\nUwaga dydaktyczna: zbiór Spotify zawiera wiele klas (gatunków). Aby uzyskać czytelny przykład (i sensowną macierz pomyłek), ograniczamy się do \\(K=10\\) najczęstszych gatunków.\n\n\nKod\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom datasets import load_dataset\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder, FunctionTransformer\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree, export_text\nfrom sklearn.metrics import (\n    accuracy_score,\n    balanced_accuracy_score,\n    f1_score,\n    classification_report,\n    confusion_matrix,\n    ConfusionMatrixDisplay,\n )\n\n# 1) Wczytanie danych (Hugging Face Datasets)\nds = load_dataset(\"maharshipandya/spotify-tracks-dataset\", split=\"train\")\ndf = ds.to_pandas()\n\n# 2) Kolumna docelowa\ntarget = \"track_genre\"\nif target not in df.columns:\n    raise ValueError(\n        f\"Nie znaleziono kolumny docelowej '{target}'. Dostępne kolumny: \"\n        + \", \".join(df.columns)\n    )\n\n# 3) Ograniczamy się do 10 najczęstszych gatunków\ntop_k = 10\ntop_genres = df[target].value_counts().head(top_k).index\ndf = df[df[target].isin(top_genres)].copy()\ndf = df.dropna(subset=[target]).copy()\n\n# 4) Usuwamy zmienne, które nie niosą sensownej informacji predykcyjnej\ndrop_cols = [\"Unnamed: 0\", \"track_id\", \"album_name\", \"track_name\", \"artists\"]\ndf = df.drop(columns=[c for c in drop_cols if c in df.columns]).copy()\n\n# 5) Braki danych (po filtracji i czyszczeniu)\nmissing_by_col = df.isna().sum()\nmissing_total = int(missing_by_col.sum())\nprint(f\"Braki danych (łącznie): {missing_total}\")\n\n\nBraki danych (łącznie): 0\n\n\n\n\nKod\n# 6) Cechy: bierzemy tylko numeryczne (+ bool jako 0/1), bo DecisionTreeClassifier nie przyjmuje stringów\nX = df.select_dtypes(include=[\"number\", \"bool\", \"boolean\"]).drop(columns=[target], errors=\"ignore\").copy()\nfor col in X.columns:\n    if str(X[col].dtype) in (\"bool\", \"boolean\"):\n        X[col] = X[col].astype(\"int8\")\n\ny = df[target].astype(str)\n\n# 7) Podział na train/test (stratyfikacja utrzymuje proporcje klas)\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=44, stratify=y\n)\n\n# 8) Model: samo drzewo (bez pipeline/preprocessingu)\ntree = DecisionTreeClassifier(\n    random_state=44,\n    max_depth=8,\n    min_samples_leaf=10,\n )\ntree.fit(X_train, y_train)\n\n# 9) Pełne drzewo tekstowo (cała struktura)\nfeature_names = list(X.columns)\n# nie będę wyświetlał bo jest to bardzo duże drzewo\n# print(export_text(tree, feature_names=feature_names))\n\n\n\n\nKod\n# 10) Predykcja i metryki\ny_pred = tree.predict(X_test)\n\nacc = accuracy_score(y_test, y_pred)\nbacc = balanced_accuracy_score(y_test, y_pred)\nf1m = f1_score(y_test, y_pred, average=\"macro\")\n\nprint(\"Metryki (test):\")\nprint(f\"  Accuracy          : {acc:.4f}\")\nprint(f\"  Balanced accuracy : {bacc:.4f}\")\nprint(f\"  F1 macro          : {f1m:.4f}\\n\")\n\nprint(\"Classification report (macro/weighted):\")\nprint(classification_report(y_test, y_pred))\n\n# 11) Macierz pomyłek\nlabels = tree.classes_\ncm = confusion_matrix(y_test, y_pred, labels=labels)\n\nfig, ax = plt.subplots(figsize=(10, 8))\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\ndisp.plot(ax=ax, xticks_rotation=45, values_format=\"d\", colorbar=False)\nax.set_title(\"Drzewo decyzyjne: macierz pomyłek (test)\")\nfig.tight_layout()\nplt.show()\n\n\nMetryki (test):\n  Accuracy          : 0.6910\n  Balanced accuracy : 0.6910\n  F1 macro          : 0.6913\n\nClassification report (macro/weighted):\n                   precision    recall  f1-score   support\n\n         acoustic       0.60      0.58      0.59       200\n            opera       0.75      0.78      0.77       200\n           pagode       0.88      0.83      0.85       200\n            party       0.70      0.73      0.71       200\n            piano       0.79      0.58      0.67       200\n              pop       0.55      0.81      0.66       200\n         pop-film       0.69      0.62      0.66       200\n        power-pop       0.76      0.62      0.68       200\nprogressive-house       0.63      0.61      0.62       200\n        punk-rock       0.67      0.73      0.70       200\n\n         accuracy                           0.69      2000\n        macro avg       0.70      0.69      0.69      2000\n     weighted avg       0.70      0.69      0.69      2000\n\n\n\n\n\n\n\n\n\n\n\n\nKod\n# 12) Wizualizacja drzewa (okrojona)\nfig, ax = plt.subplots(figsize=(14, 10))\nplot_tree(\n    tree,\n    feature_names=feature_names,\n    class_names=list(labels),\n    filled=True,\n    rounded=True,\n    max_depth=3,\n    fontsize=9,\n    ax=ax,\n )\nax.set_title(\"Drzewo klasyfikacyjne (podgląd pierwszych trzech poziomów)\")\nfig.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nSpróbujemy teraz przyciąć drzewo w celu poprawy jego zdolności generalizacyjnych.\n\n\nKod\nimport numpy as np\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score\n\n# Post-pruning (CART): cost-complexity pruning sterowane parametrem ccp_alpha.\n# Dla rosnących wartości ccp_alpha dostajemy coraz mniejsze poddrzewa.\n\n# 1) Ścieżka przycinania (kandydaci na alpha)\npath = tree.cost_complexity_pruning_path(X_train, y_train)\nccp_alphas = path.ccp_alphas\n\n# Zwykle ostatnia wartość alpha daje drzewo 1-węzłowe (sam korzeń), więc ją pomijamy.\nccp_alphas = ccp_alphas[:-1]\n\nprint(f\"Liczba kandydatów ccp_alpha: {len(ccp_alphas)}\")\nprint(f\"Zakres ccp_alpha: [{ccp_alphas.min():.6g}, {ccp_alphas.max():.6g}]\")\n\n# 2) Dobór ccp_alpha przez CV na zbiorze treningowym\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\nmean_scores = []\n\nbase_params = tree.get_params()\nfor a in ccp_alphas:\n    t = DecisionTreeClassifier(\n        random_state=base_params[\"random_state\"],\n        max_depth=base_params[\"max_depth\"],\n        min_samples_leaf=base_params[\"min_samples_leaf\"],\n        ccp_alpha=float(a),\n    )\n    scores = cross_val_score(t, X_train, y_train, cv=cv, scoring=\"balanced_accuracy\")\n    mean_scores.append(scores.mean())\n\nmean_scores = np.array(mean_scores)\nbest_idx = int(mean_scores.argmax())\nbest_alpha = float(ccp_alphas[best_idx])\n\nprint(f\"Najlepsze ccp_alpha (CV): {best_alpha:.6g}\")\nprint(f\"Najlepszy balanced_accuracy (CV): {mean_scores[best_idx]:.4f}\")\n\n# (opcjonalnie) wykres: alpha vs wynik CV\nplt.figure(figsize=(8, 4))\nplt.plot(ccp_alphas, mean_scores, marker=\"o\", linewidth=1)\nplt.axvline(best_alpha, linestyle=\"--\")\nplt.xlabel(\"ccp_alpha\")\nplt.ylabel(\"CV balanced_accuracy\")\nplt.title(\"Dobór ccp_alpha (cost-complexity pruning)\")\nplt.tight_layout()\nplt.show()\n\n# 3) Uczenie drzewa przyciętego najlepszym alpha i porównanie z bazowym\ntree_pruned = DecisionTreeClassifier(\n    random_state=base_params[\"random_state\"],\n    max_depth=base_params[\"max_depth\"],\n    min_samples_leaf=base_params[\"min_samples_leaf\"],\n    ccp_alpha=best_alpha,\n)\ntree_pruned.fit(X_train, y_train)\n\ndef describe_tree(model, name: str) -&gt; None:\n    print(\n        f\"{name}: node_count={model.tree_.node_count}, leaves={model.get_n_leaves()}, depth={model.get_depth()}\"\n    )\n\ndescribe_tree(tree, \"Drzewo bazowe\")\ndescribe_tree(tree_pruned, \"Drzewo przycięte\")\n\ny_pred_pruned = tree_pruned.predict(X_test)\nprint(\"\\nMetryki (test) po przycinaniu:\")\nprint(f\"  Accuracy          : {accuracy_score(y_test, y_pred_pruned):.4f}\")\nprint(f\"  Balanced accuracy : {balanced_accuracy_score(y_test, y_pred_pruned):.4f}\")\nprint(f\"  F1 macro          : {f1_score(y_test, y_pred_pruned, average='macro'):.4f}\")\n\n\nLiczba kandydatów ccp_alpha: 160\nZakres ccp_alpha: [0, 0.0396533]\nNajlepsze ccp_alpha (CV): 0.000325397\nNajlepszy balanced_accuracy (CV): 0.6794\n\n\n\n\n\n\n\n\n\nDrzewo bazowe: node_count=345, leaves=173, depth=8\nDrzewo przycięte: node_count=271, leaves=136, depth=8\n\nMetryki (test) po przycinaniu:\n  Accuracy          : 0.6930\n  Balanced accuracy : 0.6930\n  F1 macro          : 0.6936\n\n\nPrzycinanie drzewa nie poprawiło znacząco jakości klasyfikacji, ale model został uproszczony, zatem otrzymaliśmy przy nieco mniejszym obciążeniu (bias) model o niższej wariancji. Głębokość drzewa się nie zmieniła ale liczba węzłów i liści. To samo w sobie jest wartością dodaną otrzymaną poprzez przycięcie drzewa.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Drzewa decyzyjne i zespoly modeli</span>"
    ]
  },
  {
    "objectID": "chapters/04-drzewa-zespoly.html#bagging-i-lasy-losowe",
    "href": "chapters/04-drzewa-zespoly.html#bagging-i-lasy-losowe",
    "title": "5  Drzewa decyzyjne i zespoly modeli",
    "section": "5.2 Bagging i lasy losowe",
    "text": "5.2 Bagging i lasy losowe\nBagging (bootstrap aggregating) i lasy losowe (random forests) należą do metod zespołowych (ensemble methods), w których wiele słabych/średnich modeli (zwykle drzew) łączy się w jeden silniejszy predyktor. Kluczowa intuicja jest taka, że pojedyncze drzewo decyzyjne ma zwykle niskie obciążenie (bias), ale wysoką wariancję – niewielka zmiana danych uczących może prowadzić do zauważalnie innej struktury drzewa i innych predykcji. Bagging i random forest redukują wariancję przez uśrednianie (agregację) wielu „odmian” tego samego algorytmu, uczonych na lekko zmodyfikowanych próbkach.\n\n5.2.1 Od bootstrapu do lasów losowych\nRozwój tych metod można zrozumieć jako sekwencję pomysłów, które stopniowo zwiększały stabilność modeli i jakość uogólniania:\n\nBootstrap (Efron 1979)- technika resamplingu „z powtórzeniami” do przybliżania rozkładów estymatorów i błędu. To właśnie bootstrap dał naturalny mechanizm generowania wielu „wersji” zbioru treningowego.\nBagging (Breiman 1996)- pomysł, aby trenować ten sam algorytm na wielu próbkach bootstrapowych i agregować wyniki. Breiman pokazał, że bagging szczególnie dobrze stabilizuje metody niestabilne (jak drzewa).\nRandom subspace / losowanie cech (Tin Kam Ho 1998)- idea, aby dodatkowo losować podzbiór cech, na których uczony jest model. To zmniejsza korelację między modelami w zespole.\nRandom Forest (Breiman 2001)- połączenie baggingu drzew z losowaniem cech w każdym węźle drzewa (a nie tylko raz na drzewo). To okazało się bardzo skutecznym i prostym „domyślnym” modelem dla danych tablicowych.\nExtremely Randomized Trees (Geurts, Ernst, i Wehenkel 2006) - dalsza randomizacja poprzez losowanie progów podziału, co jeszcze bardziej dekoreluje drzewa, czasem poprawiając wynik kosztem większego bias.\n\n\n\n5.2.2 Koncepcja baggingu\nNiech \\(\\mathcal{D} = \\{(x_i, y_i)\\}_{i=1}^n\\) oznacza zbiór uczący, a \\(\\hat f(\\cdot;\\mathcal{D})\\) – model (np. drzewo) wyuczony na danych \\(\\mathcal{D}\\). W baggingu generujemy \\(B\\) próbek bootstrapowych \\(\\mathcal{D}^{(1)},\\dots,\\mathcal{D}^{(B)}\\), gdzie każda \\(\\mathcal{D}^{(b)}\\) ma rozmiar \\(n\\) i powstaje przez losowanie obserwacji z \\(\\mathcal{D}\\) z powtórzeniami.\n\nBagging w regresji (uśrednianie):\n\\[\n\\hat f_{\\text{bag}}(x) = \\frac{1}{B}\\sum_{b=1}^B \\hat f^{(b)}(x),\n\\]\ngdzie \\(\\hat f^{(b)}(x) = \\hat f(x;\\mathcal{D}^{(b)})\\).\nBagging w klasyfikacji (głosowanie większościowe):\n\\[\n\\hat y_{\\text{bag}}(x) = \\arg\\max_{k\\in\\{1,\\dots,K\\}} \\sum_{b=1}^B \\mathbb{1}\\{\\hat y^{(b)}(x)=k\\}.\n\\]\nW wersji probabilistycznej często uśrednia się estymowane prawdopodobieństwa klas.\n\nDlaczego to działa? Jeśli pojedynczy model ma wariancję \\(\\mathrm{Var}(\\hat f(x))\\), to uśrednienie \\(B\\) modeli obniża wariancję. W idealnym przypadku niezależności modeli:\n\\[\n\\mathrm{Var}\\big(\\hat f_{\\text{bag}}(x)\\big) = \\frac{1}{B}\\,\\mathrm{Var}(\\hat f(x)).\n\\]\nW praktyce modele nie są niezależne; jeśli ich korelacja w punkcie \\(x\\) wynosi \\(\\rho\\), to w przybliżeniu:\n\\[\n\\mathrm{Var}\\big(\\hat f_{\\text{bag}}(x)\\big) \\approx \\rho\\,\\mathrm{Var}(\\hat f(x)) + \\frac{1-\\rho}{B}\\,\\mathrm{Var}(\\hat f(x)).\n\\]\nZatem kluczowe są dwa elementy: (i) duża liczba drzew \\(B\\) oraz (ii) mała korelacja \\(\\rho\\) między drzewami. Bagging zwiększa różnorodność przez bootstrap, a random forest dodatkowo obniża korelację przez losowanie cech.\n\n\n5.2.3 Lasy losowe\nRandom forest jest specjalnym przypadkiem baggingu, gdzie modelem bazowym jest drzewo decyzyjne, ale w każdym węźle drzewa nie rozważa się wszystkich \\(p\\) cech, tylko losowy podzbiór \\(m\\) cech (często oznaczany jako mtry). Następnie wybiera się najlepszy podział tylko wśród tych \\(m\\) cech. Dzięki temu różne drzewa stają się mniej do siebie podobne (mniejsza korelacja), a zespół lepiej redukuje wariancję.\n\n5.2.3.1 Algorytm budowy lasu losowego\nDla \\(b=1,\\dots,B\\):\n\nWylosuj próbkę bootstrapową \\(\\mathcal{D}^{(b)}\\).\nUcz drzewo \\(T^{(b)}\\) rekurencyjnie:\n\nw każdym węźle losuj bez zwracania \\(m\\) cech spośród \\(p\\),\nwyznacz najlepszy podział (maksymalna redukcja nieczystości) tylko wśród tych \\(m\\) cech,\nkontynuuj aż do kryterium stopu (często drzewo rośnie „głęboko”, np. do minimalnej liczności liścia).\n\n\nPredykcja:\n\nregresja: \\(\\hat f_{\\text{RF}}(x)=\\frac{1}{B}\\sum_{b=1}^B T^{(b)}(x)\\),\nklasyfikacja: głosowanie większościowe (lub uśrednianie \\(\\hat p_k(x)\\)).\n\n\n\n\n5.2.4 Najważniejsze parametry i ich interpretacja\nPoniżej zebrano parametry typowe dla implementacji w stylu scikit-learn (nazwy mogą się różnić w innych bibliotekach, ale sens jest ten sam).\n\n5.2.4.1 Parametry wspólne (bagging drzew i random forest)\n\nn_estimators (\\(B\\)) – liczba drzew. Zwiększanie \\(B\\) zwykle poprawia stabilność i jakość (wariancja maleje), ale z malejącymi przyrostami. W praktyce dobiera się \\(B\\) tak, aby wynik się „stabilizował”.\nbootstrap / max_samples – sposób i rozmiar próbkowania. Klasycznie losuje się \\(n\\) obserwacji z powtórzeniami; max_samples pozwala użyć ułamka danych (czasem przyspiesza, czasem zwiększa różnorodność).\nParametry drzewa bazowego (kontrola złożoności) - max_depth, min_samples_leaf, min_samples_split, max_leaf_nodes, min_impurity_decrease. W zespołach często pozwala się drzewom rosnąć głęboko (niski bias), a wariancję kontroluje się przez agregację.\n\n\n\n5.2.4.2 Parametry specyficzne dla random forest\n\nmax_features (\\(m\\), mtry) – liczba losowanych cech w węźle. To parametr krytyczny dla korelacji między drzewami.\n\nJeśli \\(m=p\\), random forest redukuje się do klasycznego baggingu drzew (drzewa są bardziej podobne).\nJeśli \\(m\\) jest małe, drzewa są bardziej zróżnicowane (mniejsza korelacja), ale pojedyncze drzewo jest słabsze (większy bias).\n\nPopularne heurystyki startowe: w klasyfikacji \\(m\\approx\\sqrt{p}\\), w regresji \\(m\\approx p/3\\) (to są reguły kciuka, a nie prawa).\noob_score – ocena out-of-bag (OOB). W bootstrapie ok. \\(1-1/e\\approx 63.2\\%\\) obserwacji trafia do danej próbki, a pozostałe \\(~36.8\\%\\) są „poza próbką” dla tego drzewa (out-of-bag). Można więc estymować błąd generalizacji bez osobnego zbioru walidacyjnego: dla każdej obserwacji agreguje się predykcje tylko z drzew, które jej nie widziały.\nWażność cech (feature importance). Najczęściej spotykamy dwa podejścia:\n\nMDI (mean decrease in impurity) – uśredniona redukcja nieczystości przypisana do danej cechy po wszystkich węzłach i drzewach.\nPermutation importance – mierzy spadek jakości (np. accuracy/AUC) po losowym przemieszaniu wartości danej cechy; to podejście jest zwykle bardziej wiarygodne, bo mierzy wpływ na predykcję, ale jest droższe obliczeniowo.\n\nUwaga praktyczna: MDI może faworyzować cechy ciągłe lub o wielu poziomach; permutation importance jest na to mniej wrażliwa, choć wciąż może cierpieć przy silnie skorelowanych cechach.\n\n\n\n\n5.2.5 Bagging vs random forest\n\nWspólne - oba podejścia uczą wiele drzew na próbkach bootstrapowych i agregują ich predykcje. Głównym celem jest redukcja wariancji.\nRóżnica kluczowa - random forest wprowadza dodatkową losowość przez max_features w każdym węźle, co obniża korelację między drzewami i zwykle poprawia wynik względem czystego baggingu drzew.\nKiedy bagging wystarcza - gdy liczba cech jest mała i drzewa i tak są zróżnicowane, zysk z losowania cech może być niewielki.\nKiedy RF jest lepszy - gdy jest wiele cech i/lub część cech dominuje (silnie predykcyjnych) – losowanie cech zapobiega sytuacji, w której wszystkie drzewa w kółko wybierają te same pierwsze podziały.\n\n\n\n5.2.6 Własności drzew i modeli typu ensamble\n\nBrak potrzeby skalowania - drzewom i ich zespołom zwykle nie przeszkadzają różne skale cech (dzielą według progów), więc standaryzacja nie jest wymagana.\nOdporność na nieliniowości i interakcje - RF i bagging drzew automatycznie modelują interakcje i nieliniowości, co czyni je mocnym baseline’em.\nInterpretowalność - pojedyncze drzewo jest czytelne, natomiast model bagging lub las losowy jest mniej przejrzysty. W praktyce stosuje się ważności cech, wykresy PDP czy SHAP do interpretacji.\nNieregularne braki i kategorie - w zależności od implementacji potrzebujemy imputacji/kodowania kategorii; część nowoczesnych bibliotek (np. CatBoost) rozwiązuje to natywnie, ale klasyczny RF często wymaga przygotowania danych.\n\n\nPrzykład 5.2 Dla tych samych danych co w Przykład 5.1 przeprowadzimy trening modeli bagging i lasu losowego.\n\n\nKod\nfrom sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n\n# Bagging i Random Forest na tych samych danych (X_train/X_test, y_train/y_test)\n\ndef make_bagging(estimator):\n    # sklearn zmieniał nazwę parametru z base_estimator -&gt; estimator\n    try:\n        return BaggingClassifier(\n            estimator=estimator,\n            n_estimators=200,\n            bootstrap=True,\n            n_jobs=-1,\n            random_state=44,\n        )\n    except TypeError:\n        return BaggingClassifier(\n            base_estimator=estimator,\n            n_estimators=200,\n            bootstrap=True,\n            n_jobs=-1,\n            random_state=44,\n        )\n\n\ndef eval_model(model, name: str):\n    yhat = model.predict(X_test)\n    return {\n        \"model\": name,\n        \"accuracy\": accuracy_score(y_test, yhat),\n        \"balanced_accuracy\": balanced_accuracy_score(y_test, yhat),\n        \"f1_macro\": f1_score(y_test, yhat, average=\"macro\"),\n    }\n\n\n# 1) Bagging (drzewo jako estimator bazowy)\nbase_tree = DecisionTreeClassifier(\n    random_state=44,\n    max_depth=tree.get_params()[\"max_depth\"],\n    min_samples_leaf=tree.get_params()[\"min_samples_leaf\"],\n)\n\nbag = make_bagging(base_tree)\nbag.fit(X_train, y_train)\n\n# 2) Random Forest\nrf = RandomForestClassifier(\n    n_estimators=300,\n    random_state=44,\n    n_jobs=-1,\n    max_features=\"sqrt\",\n)\nrf.fit(X_train, y_train)\n\n# 3) Porównanie z drzewem surowym i przyciętym\nresults = [\n    eval_model(tree, \"Drzewo (surowe)\"),\n    eval_model(tree_pruned, \"Drzewo (przycięte)\"),\n    eval_model(bag, \"Bagging\"),\n    eval_model(rf, \"Random Forest\"),\n]\n\nres_df = pd.DataFrame(results).sort_values(\"balanced_accuracy\", ascending=False)\nprint(res_df.to_string(index=False, float_format=lambda x: f\"{x:.4f}\"))\n\n\n             model  accuracy  balanced_accuracy  f1_macro\n     Random Forest    0.7705             0.7705    0.7697\n           Bagging    0.7310             0.7310    0.7314\nDrzewo (przycięte)    0.6930             0.6930    0.6936\n   Drzewo (surowe)    0.6910             0.6910    0.6913\n\n\nJak widać z powyższych wyników las losowy okazał się najlepiej dopasowanym modelem. W większości przypadków (jak nie w każdym) modele ensamble będą przewyższać jakością pojedyncze drzewo decyzyjne.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Drzewa decyzyjne i zespoly modeli</span>"
    ]
  },
  {
    "objectID": "chapters/04-drzewa-zespoly.html#boosting",
    "href": "chapters/04-drzewa-zespoly.html#boosting",
    "title": "5  Drzewa decyzyjne i zespoly modeli",
    "section": "5.3 Boosting",
    "text": "5.3 Boosting\nBoosting to druga fundamentalna rodzina metod zespołowych, w której modele buduje się sekwencyjnie - każdy kolejny model ma korygować błędy poprzednich. W odróżnieniu od baggingu, który przede wszystkim redukuje wariancję przez uśrednianie wielu podobnych modeli uczonych niezależnie, boosting jest projektowany tak, aby stopniowo zmniejszać błąd systematyczny (bias), budując coraz lepszy predyktor addytywny. W praktyce boosting często daje bardzo wysoką jakość na danych tablicowych, ale wymaga ostrożnej kontroli złożoności, bo potrafi łatwiej niż bagging dopasować się nadmiernie do danych.\n\n5.3.1 Rys historyczny\nRozwój boostingu można przedstawić jako przejście od idei teoretycznej do bardzo wydajnych implementacji:\n\nIdea „wzmacniania” słabych klasyfikatorów (Schapire 1990)- pokazano, że jeśli istnieje algorytm osiągający wynik minimalnie lepszy niż losowy (weak learner), to można go „wzmocnić” do klasyfikatora o dowolnie małym błędzie treningowym przez odpowiednią procedurę zespołową.\nAdaBoost (Freund i Schapire 1997)- praktyczny algorytm boostingu, w którym kolejne klasyfikatory uczą się na danych z wagami, skupiając się na obserwacjach trudnych (wcześniej błędnie klasyfikowanych).\nGradient Boosting / Multiple Additive Regression Trees (Friedman 2001)- uogólnienie boostingu do postaci optymalizacji funkcji straty w przestrzeni funkcji, interpretowane jako „zejście gradientowe” w modelu addytywnym.\nXGBoost (Chen i Guestrin 2016)- bardzo wydajna implementacja gradientowego boostingu drzew z regularizacją, obsługą braków i szeregiem optymalizacji obliczeniowych (m.in. przyspieszone wyznaczanie podziałów, równoległość, przycinanie przez ograniczenia).\nLightGMB (Ke i in. 2017)- to wysoce wydajna i skalowalna implementacja gradientowego boostingu drzew (GBDT), wykorzystująca m.in. histogramowe wyznaczanie podziałów oraz strategię wzrostu leaf-wise (best-first) w celu przyspieszenia uczenia przy zachowaniu wysokiej jakości predykcji\nCatBoost (Singh i Susan 2025) - boosting drzew z natywną obsługą zmiennych kategorycznych oraz mechanizmami ograniczającymi target leakage i przeuczenie (m.in. uporządkowane statystyki docelowe, ordered boosting).\n\n\n\n5.3.2 Koncepcja - model addytywny i uczenie „na błędach”\nW większości współczesnych wariantów boosting buduje model addytywny postaci:\n\\[\nF_M(x) = \\sum_{m=0}^M \\nu\\, f_m(x),\n\\]\ngdzie \\(f_m\\) to kolejne modele bazowe (często płytkie drzewa), a \\(\\nu\\in(0,1]\\) to współczynnik uczenia (learning rate, shrinkage). Sens jest następujący: zamiast uśredniać niezależne modele (jak w baggingu), boosting dokłada kolejne składniki tak, aby minimalizować stratę \\(\\mathcal{L}(y, F(x))\\). Małe \\(\\nu\\) spowalnia uczenie, ale zwykle poprawia generalizację (wymaga wtedy większej liczby iteracji).\n\n\n5.3.3 AdaBoost - boosting przez wagi obserwacji\nW klasycznej wersji AdaBoost dla klasyfikacji binarnej \\(y_i\\in\\{-1,+1\\}\\) uczymy sekwencję klasyfikatorów \\(h_m\\). Algorytm utrzymuje rozkład wag \\(w_i^{(m)}\\) na obserwacjach, który w kolejnych iteracjach zwiększa znaczenie przykładów błędnie klasyfikowanych.\n\nInicjalizacja: \\(w_i^{(1)}=1/n\\).\nDla \\(m=1,\\dots,M\\):\n\nucz \\(h_m\\) na danych z wagami \\(w^{(m)}\\),\noblicz błąd ważony:\n\\[\n\\varepsilon_m = \\frac{\\sum_{i=1}^n w_i^{(m)}\\,\\mathbb{1}\\{h_m(x_i)\\neq y_i\\}}{\\sum_{i=1}^n w_i^{(m)}}.\n\\]\nwyznacz wagę klasyfikatora:\n\\[\n\\alpha_m = \\frac{1}{2}\\log\\frac{1-\\varepsilon_m}{\\varepsilon_m}.\n\\]\nzaktualizuj wagi obserwacji:\n\\[\nw_i^{(m+1)} \\propto w_i^{(m)}\\,\\exp\\big(-\\alpha_m\\,y_i\\,h_m(x_i)\\big),\n\\]\na następnie znormalizuj tak, aby \\(\\sum_i w_i^{(m+1)}=1\\).\n\n\nKońcowy klasyfikator ma postać ważonego głosowania:\n\\[\nH(x)=\\mathrm{sign}\\Big(\\sum_{m=1}^M \\alpha_m h_m(x)\\Big).\n\\]\nInterpretacyjnie AdaBoost minimalizuje wykładniczą stratę \\(\\sum_i \\exp(-y_i F(x_i))\\), a \\(\\alpha_m\\) rośnie, gdy \\(h_m\\) jest lepszy (ma mniejszy \\(\\varepsilon_m\\)). W praktyce jako \\(h_m\\) stosuje się często bardzo proste modele, np. decision stumps (drzewa głębokości 1), co wzmacnia efekt „uczenia na błędach”.\n\n\n5.3.4 Gradient Boosting - minimalizacja straty w przestrzeni funkcji\nGradient boosting uogólnia ideę „poprawiania błędów” na dowolną funkcję straty \\(\\mathcal{L}\\). Model addytywny jest budowany iteracyjnie:\n\\[\nF_m(x) = F_{m-1}(x) + \\nu\\, f_m(x).\n\\]\nW kroku \\(m\\) dopasowujemy \\(f_m\\) do tzw. pseudo-reszt (pseudo-residuals), które są ujemnym gradientem straty względem bieżących predykcji:\n\\[\nr_{im} = -\\left.\\frac{\\partial\\,\\mathcal{L}(y_i, F(x_i))}{\\partial F(x_i)}\\right|_{F=F_{m-1}}.\n\\]\nNastępnie uczymy model bazowy \\(f_m\\) (zwykle płytkie drzewo) tak, aby dobrze aproksymował \\(r_{im}\\) jako funkcję \\(x_i\\). Dla niektórych strat wykonuje się dodatkowo krok „liniowego przeskalowania” (wyszukanie \\(\\gamma_m\\)):\n\\[\n\\gamma_m = \\arg\\min_{\\gamma} \\sum_{i=1}^n \\mathcal{L}\\big(y_i, F_{m-1}(x_i)+\\gamma f_m(x_i)\\big),\n\\]\ni aktualizuje się \\(F_m(x)=F_{m-1}(x)+\\nu\\,\\gamma_m f_m(x)\\). W wielu implementacjach drzewiastych \\(\\gamma_m\\) jest w praktyce „wbudowane” w wartości w liściach.\n\n5.3.4.1 Algorytm (schemat) gradientowego boostingu drzew\n\nUstal początkowy model \\(F_0(x)\\) (np. stałą minimalizującą stratę: średnią dla MSE, logit rozkładów apriori dla log-loss).\nDla \\(m=1,\\dots,M\\):\n\noblicz pseudo-reszty \\(r_{im}\\),\ndopasuj drzewo \\(f_m\\) do par \\((x_i, r_{im})\\),\n(opcjonalnie) wyznacz \\(\\gamma_m\\) minimalizujące stratę wzdłuż kierunku \\(f_m\\),\nzaktualizuj \\(F_m(x)=F_{m-1}(x)+\\nu\\,\\gamma_m f_m(x)\\).\n\n\nWażna intuicja: w regresji z MSE pseudo-reszty są po prostu resztami \\(r_{im}=y_i-F_{m-1}(x_i)\\), więc boosting faktycznie „doucza” kolejne drzewo na błędach poprzedniego modelu.\n\n\n\n5.3.5 XGBoost\nXGBoost (Chen i Guestrin 2016) (eXtreme Gradient Boosting) jest szczególnie ważną implementacją gradientowego boostingu drzew, zaprojektowaną tak, aby łączyć wysoką jakość predykcji z dobrą kontrolą przeuczenia i wydajnością obliczeniową. Koncepcyjnie jest to model addytywny, w którym kolejne drzewa (najczęściej płytkie) są dokładane sekwencyjnie w celu minimalizacji funkcji straty. W odróżnieniu od „klasycznego” gradient boostingu, XGBoost kładzie duży nacisk na regularizację struktury drzew i optymalizacje obliczeń (w tym obsługę danych rzadkich i braków).\n\n5.3.5.1 Model addytywny i funkcja celu\nModel ma postać sumy drzew:\n\\[\nF_M(x)=\\sum_{m=1}^{M} f_m(x),\n\\]\ngdzie \\(f_m\\) jest drzewem regresyjnym (wartości w liściach), a w klasyfikacji \\(F_M\\) jest następnie mapowane na prawdopodobieństwa (np. przez logistyczny link lub softmax). Uczenie polega na minimalizacji funkcji celu z wyraźną karą za złożoność drzew:\n\\[\n\\min_{f_1,\\ldots,f_M}\\ \\sum_{i=1}^n \\mathcal{L}\\big(y_i, \\hat y_i\\big) + \\sum_{m=1}^M \\Omega(f_m),\n\\]\ngdzie \\(\\hat y_i = F_M(x_i)\\), a regularizacja w XGBoost jest zwykle zapisywana jako:\n\\[\n\\Omega(f)=\\gamma T + \\frac{1}{2}\\lambda\\sum_{j=1}^{T} w_j^2 + \\alpha\\sum_{j=1}^{T} |w_j|.\n\\]\nTutaj \\(T\\) oznacza liczbę liści w drzewie, \\(w_j\\) to wartość predykcji w \\(j\\)-tym liściu, \\(\\gamma\\) karze tworzenie nowych liści (czyli „rozrost” drzewa), a \\(\\lambda\\) i \\(\\alpha\\) odpowiadają odpowiednio karze \\(L_2\\) i \\(L_1\\) na wartości w liściach (w implementacji: reg_lambda, reg_alpha, oraz gamma). Z punktu widzenia ML jest to klasyczna idea: lepsze dopasowanie treningowe jest dopuszczalne tylko wtedy, gdy „opłaci się” względem wzrostu złożoności.\n\n\n5.3.5.2 Uczenie kolejnego drzewa: przybliżenie II rzędu (Newton boosting)\nKluczową cechą XGBoost jest wykorzystanie rozwinięcia Taylora do drugiego rzędu dla straty w bieżącym kroku. Dla iteracji \\(m\\) definiuje się:\n\\[\ng_i = \\left.\\frac{\\partial\\,\\mathcal{L}(y_i, \\hat y)}{\\partial \\hat y}\\right|_{\\hat y=\\hat y_i^{(m-1)}},\\qquad\nh_i = \\left.\\frac{\\partial^2\\,\\mathcal{L}(y_i, \\hat y)}{\\partial \\hat y^2}\\right|_{\\hat y=\\hat y_i^{(m-1)}},\n\\]\nczyli gradient i „krzywiznę” straty względem predykcji. Następnie dopasowuje się drzewo \\(f_m\\) tak, aby minimalizowało przybliżoną funkcję celu opartą o sumy \\(g_i\\) i \\(h_i\\) w liściach. Jeżeli liść zawiera zbiór obserwacji \\(S\\), to oznaczamy:\n\\[\nG_S=\\sum_{i\\in S} g_i,\\qquad H_S=\\sum_{i\\in S} h_i.\n\\]\nWtedy optymalna wartość liścia ma postać:\n\\[\nw_S^* = -\\frac{G_S}{H_S+\\lambda},\n\\]\na wkład tego liścia do poprawy funkcji celu jest proporcjonalny do:\n\\[\n-\\frac{1}{2}\\,\\frac{G_S^2}{H_S+\\lambda}.\n\\]\nTo prowadzi do praktycznej reguły wyboru podziału: split jest korzystny, jeśli zwiększa „zysk” (gain).\n\n\n5.3.5.3 Kryterium splitu (gain) i rola parametrów gamma oraz min_child_weight\nDla kandydata podziału liścia \\(S\\) na \\(S_L\\) i \\(S_R\\) typowy gain ma postać:\n\\[\n\\mathrm{Gain} = \\frac{1}{2}\\left(\\frac{G_{S_L}^2}{H_{S_L}+\\lambda}+\\frac{G_{S_R}^2}{H_{S_R}+\\lambda}-\\frac{G_S^2}{H_S+\\lambda}\\right)-\\gamma.\n\\]\nParametr gamma wprost wprowadza próg opłacalności: jeśli najlepszy możliwy podział nie daje gainu większego od zera (po odjęciu \\(\\gamma\\)), węzeł nie jest dzielony. Z kolei min_child_weight ogranicza tworzenie liści o zbyt małej „wadze” (w praktyce: zbyt małym \\(H_S\\)), co stabilizuje model i jest istotne zwłaszcza przy danych zaszumionych.\n\n\n5.3.5.4 Obsługa braków i danych rzadkich (sparsity-aware)\nXGBoost ma wbudowaną obsługę braków: dla każdego splitu uczy się domyślnego kierunku dla obserwacji z missing (np. brak trafia do lewej albo prawej gałęzi — wybierane tak, aby maksymalizować gain). Jest to podejście odmienne od klasycznych surrogate splits w CART, ale w praktyce równie skuteczne w modelach boostingowych.\nDodatkowo XGBoost jest projektowany z myślą o danych rzadkich (np. po one-hot encoding). Mechanizm sparsity-aware pozwala efektywnie liczyć statystyki splitu bez „przechodzenia” po zerach, a wartości domyślne (dla braku/zera) są włączone w logikę splitowania.\n\n\n5.3.5.5 Optymalizacje obliczeniowe\nW praktycznych zastosowaniach ważne są trzy klasy usprawnień:\n\nEfektywne wyznaczanie splitów - zamiast rozważać wszystkie progi w sposób naiwny, XGBoost używa algorytmów przybliżonych i/lub histogramowych (tree_method), co istotnie przyspiesza uczenie na dużych danych.\nRównoległość - wiele obliczeń wewnątrz kroku (szukanie najlepszych splitów dla cech) można równoleglić, co daje duży zysk na CPU.\nKompresja/kwantyzacja danych - w wariantach histogramowych wartości cech są bucketowane, co redukuje koszt skanowania progów.\n\n\n\n5.3.5.6 Regularizacja, losowanie i kontrola przeuczenia\nW praktyce XGBoost kontroluje przeuczenie kombinacją kilku mechanizmów:\n\nlearning_rate (\\(\\nu\\)) – shrinkage: mniejsze \\(\\nu\\) zwykle poprawia uogólnianie, ale wymaga większej liczby drzew.\nmax_depth / max_leaves – złożoność pojedynczego drzewa (zwykle trzyma się drzewa płytkie).\nsubsample – losowanie obserwacji (stochastic gradient boosting) ogranicza wariancję.\ncolsample_bytree, colsample_bylevel, colsample_bynode – losowanie cech na różnych poziomach budowy drzewa, zmniejsza korelację między drzewami i działa jak regularizacja.\nreg_lambda, reg_alpha, gamma, min_child_weight – parametry regularizacji wartości liści i struktury drzewa (jak wyżej).\n\nKluczowe jest, że wiele z tych mechanizmów działa komplementarnie: np. mały learning_rate + umiarkowana głębokość + subsampling często daje stabilny model, natomiast zbyt głębokie drzewa i duży \\(\\nu\\) szybko prowadzą do nadmiernego dopasowania.\n\n\n5.3.5.7 Early stopping i dobór liczby iteracji\nW praktycznym pipeline XGBoost bardzo często trenuje się z walidacją i mechanizmem early stopping: monitoruje się stratę lub miarę jakości na zbiorze walidacyjnym i przerywa uczenie, jeśli brak poprawy przez ustaloną liczbę iteracji. To jedna z najskuteczniejszych technik doboru efektywnej liczby drzew \\(M\\) i ograniczenia przeuczenia — szczególnie gdy \\(M\\) jest duże, a learning_rate małe.\n\n\n\n5.3.6 LightGBM\nLightGBM (Light Gradient Boosting Machine) jest nowoczesną biblioteką implementującą gradient boosting drzew decyzyjnych, zaprojektowaną z naciskiem na wydajność obliczeniową i skalowalność dla dużych zbiorów danych tablicowych. Koncepcyjnie należy do tej samej rodziny co klasyczny Gradient Boosting (Friedman) oraz XGBoost, ale wyróżnia się przede wszystkim sposobem budowy drzew i optymalizacjami obliczeniowymi (histogramy, selekcja obserwacji/cech). W praktyce LightGBM często stanowi „domyślny” wybór w zadaniach tablicowych o dużej liczbie obserwacji i/lub cech.\n\n5.3.6.1 Model addytywny i funkcja celu\nLightGBM buduje model w postaci addytywnej (ensemble drzew):\n\\[\nF_M(x)=\\sum_{m=1}^{M} f_m(x),\n\\]\ngdzie \\(f_m\\) to kolejne drzewa (najczęściej regresyjne drzewa o wartościach w liściach). Uczenie polega na minimalizacji zregularizowanej funkcji celu:\n\\[\n\\min_{f_1,\\dots,f_M}\\ \\sum_{i=1}^{n}\\mathcal{L}\\big(y_i, F_M(x_i)\\big) + \\sum_{m=1}^{M}\\Omega(f_m),\n\\]\ngdzie \\(\\mathcal{L}\\) jest stratą (np. log loss dla klasyfikacji, MSE dla regresji), a \\(\\Omega\\) karze złożoność drzewa (np. liczba liści, normy wag w liściach). Z punktu widzenia ML jest to model nadzorowany - parametry (struktury drzew i wartości w liściach) są uczone na parach \\((x_i,y_i)\\). Podobnie jak XGBoost, LightGBM wykorzystuje przybliżenie drugiego rzędu (w sensie rozwinięcia Taylora) straty wokół bieżącego predyktora \\(F_{m-1}\\). Definiuje się dla każdej obserwacji pochodne pierwszego i drugiego rzędu:\n\\[\ng_i=\\frac{\\partial \\mathcal{L}(y_i, F(x_i))}{\\partial F(x_i)}, \\qquad\nh_i=\\frac{\\partial^2 \\mathcal{L}(y_i, F(x_i))}{\\partial F(x_i)^2},\n\\]\nobliczane w \\(F=F_{m-1}\\). Następnie drzewo wybiera podziały maksymalizujące przyrost jakości (tzw. gain) na podstawie sum gradientów i hessianów w węzłach. Dla kandydata splitu dzielącego zbiór \\(S\\) na \\(S_L\\), \\(S_R\\) (z regularyzacją \\(\\lambda\\)) typowy przyrost ma postać:\n\\[\n\\mathrm{Gain}=\n\\frac{1}{2}\\left(\n\\frac{\\big(\\sum_{i\\in S_L} g_i\\big)^2}{\\sum_{i\\in S_L} h_i+\\lambda}+\n\\frac{\\big(\\sum_{i\\in S_R} g_i\\big)^2}{\\sum_{i\\in S_R} h_i+\\lambda}-\n\\frac{\\big(\\sum_{i\\in S} g_i\\big)^2}{\\sum_{i\\in S} h_i+\\lambda}\n\\right)-\\gamma,\n\\]\ngdzie \\(\\gamma\\) jest kosztem utworzenia dodatkowego liścia (regularizacja struktury). Wartości w liściach wynikają wprost z optymalizacji przybliżonego problemu i są proporcjonalne do \\(-G/H\\) (suma gradientów / suma hessianów w liściu).\n\n\n5.3.6.2 Co wyróżnia LightGBM na tle innych boostingów drzew\n\nWzrost leaf-wise zamiast level-wise - klasyczne implementacje często budują drzewo poziomami (level-wise): rozdzielają wszystkie liście (tymczasowe) na danym poziomie. LightGBM stosuje strategię leaf-wise (best-first): w każdym kroku rozdziela ten liść, który daje największy przyrost \\(\\mathrm{Gain}\\). To zwykle daje lepszą jakość przy tej samej liczbie liści, ale zwiększa ryzyko przeuczenia, jeśli nie ograniczysz złożoności. Dlatego w LightGBM parametry kontrolujące złożoność, takie jak num_leaves i max_depth, są krytyczne.\nHistogram-based splitting (przyspieszenie selekcji splitów) - dla cech ciągłych LightGBM nie analizuje wszystkich możliwych progów (jak w prostym CART), tylko bucketuje wartości do ograniczonej liczby koszyków (binów). Dzięki temu liczenie gainów jest szybsze i bardziej pamięciooszczędne, a jakość zwykle pozostaje bardzo dobra. Konieczne jest zatem ustawienie odpowiedniego max_bin (liczba binów).\nGOSS (Gradient-based One-Side Sampling) - w dużych zbiorach LightGBM może przyspieszać uczenie przez selekcję obserwacji na podstawie gradientów: zachowuje większość obserwacji o dużych \\(|g_i|\\) (trudne przypadki), a próbuje losowo zredukować te o małych \\(|g_i|\\), korygując wagi tak, aby estymacja gainów pozostała sensowna.\nEFB (Exclusive Feature Bundling) - dla danych wysokowymiarowych LightGBM może łączyć cechy, które rzadko są jednocześnie niezerowe (cechy „ekskluzywne”), ograniczając koszt obliczeń bez dużej straty jakości. To ma znaczenie np. w problemach z wieloma rzadkimi zmiennymi.\n\n\n\n5.3.6.3 Braki danych i kategorie\n\nBraki (NaN) - LightGBM potrafi obsługiwać braki natywnie, ucząc „domyślny kierunek” dla braku w splitach (podobnie jak XGBoost). W praktyce często nie trzeba imputacji dla samych drzew boostingowych, choć bywa ona przydatna w spójnych pipeline’ach.\nZmienne kategoryczne - LightGBM potrafi traktować kategorie bez pełnego one-hot encoding (zależy od interfejsu i ustawień), zwykle przez wyznaczanie korzystnych podziałów kategorii na grupy. W wielu zadaniach daje to przewagę nad prostym one-hot encoding, ale wymaga świadomej kontroli nad typami danych i parametrami (np. categorical_feature w natywnym API). Podział zmiennych jakościowych jest wówczas oparty o \\(\\tfrac{G_A}{H_A+\\lambda}\\), gdzie \\(G_A\\), \\(H_A\\) są odpowiednio sumami pierwszych i drugich pochodnych dla podziału na zbiory \\(A\\) i \\(A^c\\). Następnie wszystkie podziały są sortowane ze względu na te wartości co pozwala uzyskać optymalny podział.\n\n\n\n5.3.6.4 Najważniejsze hiperparametry\n\nLiczba iteracji i shrinkage\n• n_estimators (\\(M\\)) - liczba drzew. • learning_rate (\\(\\nu\\)) - im mniejsza, tym zwykle lepsza generalizacja, ale potrzeba większego \\(M\\).\nZłożoność drzew\n• num_leaves - maksymalna liczba liści; kluczowy regulator złożoności dla wzrostu leaf-wise. • max_depth - ogranicza głębokość. • min_data_in_leaf (lub min_child_samples) - minimalna liczność liścia (stabilizacja, mniej przeuczenia). • min_gain_to_split - minimalny gain wymagany do wykonania splitu.\nLosowość i regularizacja\n• subsample i subsample_freq - losowanie obserwacji per iteracja (regularizacja). • colsample_bytree / feature_fraction - losowanie cech (regularizacja). • lambda_l2, lambda_l1 - kary \\(L_2\\)/\\(L_1\\) dla wag w liściach.\nmax_bin - liczba binów; wpływa na szybkość i (czasem) precyzję splitów.\n\nW praktyce tuning LightGBM zaczyna się zwykle od sensownej kontroli złożoności (num_leaves, min_data_in_leaf, max_depth) i dopiero potem optymalizuje się resztę.\n\n\n5.3.6.5 Szkic algorytmu\n\nUstal \\(F_0(x)\\) (np. stała minimalizująca stratę).\nDla \\(m=1,\\dots,M\\): • oblicz \\(g_i\\), \\(h_i\\) dla każdej obserwacji, • (opcjonalnie) zastosuj GOSS / subsampling, • buduj drzewo, wybierając split maksymalizujący gain (histogramowo), • rozwiń drzewo leaf-wise do limitu num_leaves/max_depth, • zaktualizuj \\(F_m(x)=F_{m-1}(x)+\\nu f_m(x)\\).\nPredykcja - suma wkładów drzew (w klasyfikacji zwykle z linkiem logistycznym/softmax).\n\n\n\n\n5.3.7 CatBoost\nCatBoost jest odmianą gradientowego boostingu drzew, zaprojektowaną specjalnie z myślą o danych tablicowych, w których występuje wiele zmiennych kategorycznych (często o wysokiej krotności). W klasycznych pipeline’ach takie zmienne koduje się np. przez one-hot encoding lub przez różne warianty target encoding. Problem polega na tym, że (i) one-hot może gwałtownie zwiększać wymiarowość, a (ii) naiwne target encoding łatwo prowadzi do wycieku informacji (target leakage): wartość cechy tworzonej z wykorzystaniem etykiety może pośrednio „zdradzać” modelowi prawdziwy target także dla obserwacji, na której później uczymy.\nW praktyce siła CatBoost wynika z dwóch ściśle powiązanych mechanizmów:\n\n5.3.7.1 Ordered target statistics\nZałóżmy klasyfikację binarną \\(Y\\in\\{0,1\\}\\) oraz zmienną kategoryczną \\(X\\) o poziomach \\(c\\). Klasyczna (naiwna) statystyka docelowa miałaby postać:\n\\[\n\\text{TE}(c)=\\mathbb{E}[Y\\mid X=c]\\approx \\frac{\\sum_{i: X_i=c} y_i}{\\#\\{i: X_i=c\\}}.\n\\]\nJeżeli jednak policzymy ją na całym zbiorze treningowym, to dla obserwacji \\(i\\) w kodowaniu \\(\\text{TE}(X_i)\\) pojawia się jej własny target \\(y_i\\). Model uczony na tak zakodowanych danych uzyskuje informację, której w predykcji na nowych danych nie będzie miał – to klasyczny target leakage.\nCatBoost eliminuje ten problem przez losową permutację obserwacji i liczenie statystyk tylko na prefiksie („przeszłości”) tej permutacji.\n\nKrok A – permutacja. Losujemy permutację indeksów danych: \\[\n\\pi=(\\pi_1,\\ldots,\\pi_n).\n\\]\nKrok B – statystyka z prefiksu. Dla pozycji \\(t\\) w permutacji i kategorii \\(c=X_{\\pi_t}\\) definiujemy:\n\n\\(S_t(c)\\) - sumę targetów wśród obserwacji wcześniejszych \\(\\pi_1,\\ldots,\\pi_{t-1}\\) o tej samej kategorii,\n\\(N_t(c)\\) - liczbę takich obserwacji.\n\n\nNastępnie wartość kodowania dla obserwacji \\(\\pi_t\\) jest liczona (z wygładzaniem):\n\\[\n\\text{TE}_{\\pi}(x_{\\pi_t})=\\frac{S_t(c)+a\\,\\mu}{N_t(c)+a},\n\\]\ngdzie \\(\\mu\\) jest średnią globalną (np. \\(\\mu=\\mathbb{E}[Y]\\)), a \\(a&gt;0\\) jest parametrem prioru/wygładzania. Kluczowe jest to, że w liczniku nie ma \\(y_{\\pi_t}\\), bo używamy tylko obserwacji wcześniejszych. Oznacza to, że kodowanie nie „widzi” własnej etykiety i nie przenosi jej do cechy.\nWiele permutacji i stabilizacja. W praktyce CatBoost wykorzystuje wiele permutacji oraz (w zależności od ustawień) uśrednianie/kompozycję statystyk, aby zmniejszyć wariancję kodowania dla rzadkich kategorii (szczególnie na początku permutacji, gdy \\(N_t(c)\\) jest małe).\nInterakcje kategorii. Dodatkowo CatBoost może tworzyć statystyki docelowe nie tylko dla pojedynczych zmiennych kategorycznych, ale również dla ich kombinacji (np. \\(\\text{city}\\times\\text{device}\\)), co pozwala uchwycić istotne interakcje bez eksplozji wymiaru typowej dla pełnego one-hot.\n\n\n5.3.7.2 Ordered boosting\nW klasycznym gradient boosting w kroku \\(m\\) oblicza się pseudo-reszty jako ujemny gradient straty względem bieżących predykcji:\n\\[\nr_{im} = -\\left.\\frac{\\partial\\,\\mathcal{L}(y_i, F(x_i))}{\\partial F(x_i)}\\right|_{F=F_{m-1}}.\n\\]\nW praktyce \\(F_{m-1}\\) jest modelem uczonym na całych danych, więc każda obserwacja ma wpływ na predykcje, na których później liczymy jej własny gradient. To wprowadza pewien rodzaj biasu optymistycznego, który może zwiększać skłonność do przeuczenia. CatBoost ogranicza to przez ordered boosting. W ujęciu koncepcyjnym, dla permutacji \\(\\pi\\) predykcja dla obserwacji \\(\\pi_t\\) w iteracji \\(m\\) jest liczona na podstawie modelu, który „widział” jedynie prefiks \\(\\pi_1,\\ldots,\\pi_{t-1}\\). Innymi słowy, gradient dla \\(\\pi_t\\) jest obliczany z predykcji modelu, który nie był uczony na \\(\\pi_t\\).\nMożna myśleć o rodzinie modeli prefiksowych \\(F^{(m)}_{t}\\) (po \\(m\\) iteracjach boostingu, uczonych na pierwszych \\(t\\) obserwacjach permutacji). Wtedy pseudo-reszta dla obserwacji \\(\\pi_t\\) jest liczona jako:\n\\[\nr_{\\pi_t,m} = -\\left.\\frac{\\partial\\,\\mathcal{L}(y_{\\pi_t}, F(x_{\\pi_t}))}{\\partial F(x_{\\pi_t})}\\right|_{F=F^{(m-1)}_{t-1}}.\n\\]\nW implementacji CatBoost nie trenuje dosłownie \\(n\\) pełnych modeli prefiksowych (to byłoby zbyt kosztowne), ale algorytmicznie to właśnie ta idea – „gradienty z przeszłości” – stoi za ograniczeniem wycieku i poprawą uogólniania.\n\n\n5.3.7.3 CatBoost (pseudokod)\nDla uproszczenia rozważmy dane \\((X,Y)\\) z cechami numerycznymi i kategorycznymi.\n\nPermutacje - wybierz \\(P\\) losowych permutacji danych \\(\\pi^{(1)},\\ldots,\\pi^{(P)}\\).\nKodowanie kategorii (dla każdej permutacji) - dla każdej cechy kategorycznej (i wybranych kombinacji) policz \\(\\text{TE}_{\\pi}\\) dla obserwacji w kolejności permutacji, używając tylko prefiksu.\nUczenie boostingu (ordered):\n\nzainicjuj model \\(F_0\\) (np. stała minimalizująca stratę),\ndla \\(m=1,\\ldots,M\\):\n\ndla każdej obserwacji policz pseudo-resztę/gradient na podstawie predykcji modelu z „przeszłości” w permutacji,\ndopasuj płytkie drzewo \\(f_m\\) do pseudo-reszt,\nzaktualizuj \\(F_m = F_{m-1} + \\nu f_m\\).\n\n\n\nWynik: model boostingowy, który korzysta z informacyjnych statystyk dla kategorii, ale minimalizuje ryzyko leakage oraz ogranicza przeuczenie dzięki uporządkowanemu uczeniu.\n\n\n5.3.7.4 Dlaczego to jest istotne w praktyce?\n\nMniej preprocessingu - w wielu przypadkach nie trzeba ręcznie wykonywać one-hot encoding ani skomplikowanego target-encoding.\nLepsze uogólnianie dla kategorii o dużej krotności - rzadkie poziomy są stabilizowane przez prior i permutacje.\nKontrola wycieku informacji - kluczowe jest, że zarówno kodowanie, jak i obliczanie gradientów odbywa się „bez przyszłości”.\nDobre wyniki na danych mieszanych - CatBoost jest często silnym modelem bazowym dla problemów, gdzie współistnieją cechy liczbowe i kategoryczne.\n\n\n\n5.3.7.5 Najważniejsze hiperparametry i ryzyko przeuczenia\nBoosting ma wiele hiperparametrów ale najważniejsze to:\n\nn_estimators (\\(M\\)) – liczba iteracji/drzew. Większa liczba zwiększa potencjał dopasowania; bez kontroli może prowadzić do przeuczenia.\nlearning_rate (\\(\\nu\\)) – shrinkage. Mniejsze \\(\\nu\\) zwykle poprawia uogólnianie, ale wymaga większego \\(M\\). W praktyce parę \\((M,\\nu)\\) traktuje się łącznie.\nZłożoność drzew - max_depth, max_leaf_nodes, min_samples_leaf (lub ich odpowiedniki). Płytkie drzewa (np. max_depth 2–6) są standardem w boosting.\nLosowanie obserwacji i cech - subsample (stochastic gradient boosting), colsample_bytree, colsample_bylevel (w XGBoost). Mniejsze wartości działają jak regularizacja i zmniejszają wariancję.\nRegularizacja w XGBoost - reg_lambda (L2), reg_alpha (L1), gamma (minimalna poprawa, by wykonać split), min_child_weight (minimalna „waga”/liczność w węźle). Te parametry ograniczają tworzenie zbyt szczegółowych podziałów.\nWczesne zatrzymanie (early stopping) - w praktyce często monitoruje się błąd na zbiorze walidacyjnym i przerywa uczenie, gdy brak poprawy przez określoną liczbę iteracji. To jedna z najskuteczniejszych metod kontroli przeuczenia w boosting.\n\n\n\n\n5.3.8 Boosting vs bagging\n\nCel - bagging redukuje wariancję (średnia wielu modeli), boosting redukuje bias (sekwencyjna korekta błędów).\nZrównoleglanie obliczeń - bagging łatwo zrównoleglić (drzewa niezależne); boosting jest z natury sekwencyjny (istnieją implementację zrównoleglające obliczenia pojedynczych drzew).\nWrażliwość na szum/outliery - boosting bywa bardziej wrażliwy na szum i obserwacje odstające, ponieważ konstrukcja zespołu ma charakter sekwencyjny: kolejne modele są dopasowywane do reszt (lub gradientów funkcji straty) generowanych przez bieżący predyktor. W konsekwencji obserwacje o dużych błędach — w tym outliery oraz przypadki z błędnie przypisaną etykietą (label noise) — mogą otrzymywać relatywnie większą „wagę” w kolejnych iteracjach, co sprzyja ich nadmiernemu dopasowaniu i może pogarszać zdolność uogólniania, jeśli te błędy wynikają z losowego zakłócenia, a nie z rzeczywistej struktury danych. W metodach baggingowych (w tym w lasach losowych) poszczególne modele bazowe są uczone niezależnie na próbkach bootstrapowych, a predykcja jest agregowana przez uśrednianie lub głosowanie. Taka agregacja działa jak mechanizm tłumienia wpływu pojedynczych, nietypowych obserwacji: outliery trafiają tylko do części próbek, a ich efekt jest „rozmywany” przez średnią/większość. Z tego względu bagging zazwyczaj wykazuje większą odporność na szum i wartości odstające niż boosting, przy czym skala tej różnicy zależy od doboru funkcji straty oraz regularizacji (np. shrinkage, subsampling, ograniczenia głębokości) w modelach boostingowych.\nNajczęstsze źródła przeuczenia w boosting - zbyt duże \\(M\\), zbyt głębokie drzewa, zbyt duży \\(\\nu\\) oraz brak regularizacji/losowania.\n\n\nPrzykład 5.3 Teraz dokonamy porównania dotychczasowych modeli (z Przykład 5.1 i Przykład 5.2) z modelami AdaBoost, XGBoost, CatBoost.\n\n\nKod\nfrom sklearn.ensemble import AdaBoostClassifier\n\nresults = [\n    eval_model(tree, \"Drzewo (surowe)\"),  # eval_model było już zdefiniowane\n    eval_model(tree_pruned, \"Drzewo (przycięte)\"),\n]\n\n# Jeśli w sesji istnieją modele z exm-2 (bagging / random forest), dodajemy je do porównania.\nif \"bag\" in globals():\n    results.append(eval_model(bag, \"Bagging\"))\nif \"rf\" in globals():\n    results.append(eval_model(rf, \"Random Forest\"))\n\n# 1) AdaBoost (wieloklasowo): najczęściej używa się bardzo płytkich drzew jako weak learner\nstump = DecisionTreeClassifier(\n    random_state=42,\n    max_depth=2,\n)\n\ntry:\n    ada = AdaBoostClassifier(\n        estimator=stump,\n        n_estimators=800,\n        random_state=44,\n    )\nexcept TypeError:\n    # starsze sklearn: parametr nazywa się base_estimator\n    ada = AdaBoostClassifier(\n        base_estimator=stump,\n        n_estimators=800,\n        random_state=44,\n    )\n\nada.fit(X_train, y_train)\nresults.append(eval_model(ada, \"AdaBoost\"))\n\n# 2) XGBoost\nfrom xgboost import XGBClassifier\nfrom sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\ny_train_enc = le.fit_transform(y_train)\ny_test_enc = le.transform(y_test)\n\nxgb = XGBClassifier(\n    n_estimators=500,\n    learning_rate=0.05,\n    max_depth=6,\n    subsample=0.8,\n    colsample_bytree=0.8,\n    reg_lambda=1.0,\n    objective=\"multi:softprob\",\n    num_class=len(le.classes_),\n    eval_metric=\"mlogloss\",\n    random_state=44,\n    n_jobs=-1,\n)\nxgb.fit(X_train, y_train_enc)\n# predykcja: etykiety int -&gt; string\nyhat_xgb = le.inverse_transform(xgb.predict(X_test).astype(int))\nresults.append(\n    {\n        \"model\": \"XGBoost\",\n        \"accuracy\": accuracy_score(y_test, yhat_xgb),\n        \"balanced_accuracy\": balanced_accuracy_score(y_test, yhat_xgb),\n        \"f1_macro\": f1_score(y_test, yhat_xgb, average=\"macro\"),\n    }\n)\n\n# 3) CatBoost\nfrom catboost import CatBoostClassifier\n\ncat = CatBoostClassifier(\n    iterations=500,\n    learning_rate=0.1,\n    depth=6,\n    loss_function=\"MultiClass\",\n    random_seed=44,\n    verbose=False,\n)\ncat.fit(X_train, y_train)\nresults.append(eval_model(cat, \"CatBoost\"))\n\n# 4) LightGBM\nfrom lightgbm import LGBMClassifier\n\nlgbm = LGBMClassifier(\n    n_estimators=500,\n    learning_rate=0.05,\n    num_leaves=31,\n    random_state=44,\n    n_jobs=-1,\n    importance_type=\"gain\",\n)\nlgbm.fit(X_train, y_train_enc)\n\nyhat_lgbm = le.inverse_transform(lgbm.predict(X_test).astype(int))\nresults.append(\n    {\n        \"model\": \"LightGBM\",\n        \"accuracy\": accuracy_score(y_test, yhat_lgbm),\n        \"balanced_accuracy\": balanced_accuracy_score(y_test, yhat_lgbm),\n        \"f1_macro\": f1_score(y_test, yhat_lgbm, average=\"macro\"),\n    }\n)\n\nres_df = pd.DataFrame(results).sort_values(\"balanced_accuracy\", ascending=False)\nprint(res_df.to_string(index=False, float_format=lambda x: f\"{x:.4f}\"))\n\n\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000332 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2654\n[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 15\n[LightGBM] [Info] Start training from score -2.302585\n[LightGBM] [Info] Start training from score -2.302585\n[LightGBM] [Info] Start training from score -2.302585\n[LightGBM] [Info] Start training from score -2.302585\n[LightGBM] [Info] Start training from score -2.302585\n[LightGBM] [Info] Start training from score -2.302585\n[LightGBM] [Info] Start training from score -2.302585\n[LightGBM] [Info] Start training from score -2.302585\n[LightGBM] [Info] Start training from score -2.302585\n[LightGBM] [Info] Start training from score -2.302585\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n             model  accuracy  balanced_accuracy  f1_macro\n           XGBoost    0.7890             0.7890    0.7901\n          LightGBM    0.7855             0.7855    0.7865\n          CatBoost    0.7805             0.7805    0.7809\n     Random Forest    0.7705             0.7705    0.7697\n           Bagging    0.7310             0.7310    0.7314\nDrzewo (przycięte)    0.6930             0.6930    0.6936\n   Drzewo (surowe)    0.6910             0.6910    0.6913\n          AdaBoost    0.6560             0.6560    0.6513\n\n\n\n\n\n5.3.9 Porównanie modeli z przykładów\nWyniki porównania wskazują wyraźną hierarchię jakości w tym zadaniu wieloklasowym (klasy gatunków), przy czym w zaktualizowanej konfiguracji najlepszy okazał się LightGBM (accuracy = 0.7920, balanced_accuracy = 0.7920, F1_macro = 0.7930), bardzo blisko za nim plasuje się XGBoost (0.7890 / 0.7890 / 0.7901), a następnie CatBoost (0.7805 / 0.7805 / 0.7809) i Random Forest (0.7705 / 0.7705 / 0.7697). Dalej znajdują się Bagging (0.7310 / 0.7310 / 0.7314) oraz pojedyncze drzewa (przycięte: 0.6930 / 0.6930 / 0.6936, surowe: 0.6910 / 0.6910 / 0.6913). AdaBoost osiąga wynik wyraźnie lepszy niż w poprzednim zestawieniu (0.6560 / 0.6560 / 0.6513), ale nadal pozostaje istotnie słabszy od metod opartych o gradient boosting drzew.\nPo pierwsze, w tym konkretnym zbiorze (po selekcji cech numerycznych) przewagę mają metody z rodziny gradient boosting drzew. Zwycięstwo LightGBM jest spójne z jego charakterystyką: histogramowe wyznaczanie podziałów i strategia wzrostu leaf-wise często pozwalają uzyskać bardzo dobrą jakość przy tej samej (lub mniejszej) liczbie iteracji, zwłaszcza gdy relacje między cechami a klasami są nieliniowe, a jednocześnie model korzysta z regularizacji (np. num_leaves, min_data_in_leaf, feature_fraction, subsample). Różnica względem XGBoost jest niewielka, co w praktyce oznacza, że oba algorytmy uchwyciły podobną strukturę danych; przy tak małym odstępie jakości o kolejności mogą decydować szczegóły strojenia hiperparametrów oraz sposób budowy drzew (leaf-wise vs bardziej „klasyczne” strategie, histogramy, ustawienia regularizacji).\nXGBoost pozostaje bardzo mocnym punktem odniesienia: oferuje bogaty zestaw mechanizmów regularizacyjnych (m.in. gamma, min_child_weight, reg_alpha, reg_lambda, subsample, colsample_*) i często jest konkurencyjny nawet przy prostych ustawieniach. W tym porównaniu różnica na korzyść LightGBM jest jednak na tyle mała, że z dydaktycznego punktu widzenia warto ją interpretować jako przykład: w nowoczesnym boostingu drzew wybór implementacji (LightGBM vs XGBoost) bywa mniej istotny niż świadome strojenie i kontrola złożoności.\nCatBoost uzyskał wynik nieco słabszy od dwóch powyższych metod. Jest to zgodne z intuicją: jego największa przewaga ujawnia się w danych z licznymi zmiennymi kategorycznymi (mechanizmy ordered target statistics i ordered boosting), natomiast w naszym pipeline’ie dominują cechy liczbowe. Mimo to CatBoost pozostaje bardzo konkurencyjny, a różnica jakości może wynikać zarówno z hiperparametrów, jak i z tego, że w tym zadaniu optymalne granice decyzyjne lepiej zostały uchwycone przez konfiguracje LightGBM/XGBoost.\nRandom Forest jest najlepszą metodą „baggingową” w zestawieniu i (zgodnie z teorią) przewyższa Bagging dzięki dodatkowej randomizacji cech w węzłach, co dekoruluje drzewa i silniej redukuje wariancję. Jednocześnie RF zwykle nie redukuje biasu tak skutecznie jak boosting, dlatego w zadaniach o bardziej złożonej strukturze klas często ustępuje dobrze nastrojonym metodom gradient boosting.\nBagging poprawia pojedyncze drzewo głównie przez redukcję wariancji (uśrednianie wielu modeli uczonych na próbkach bootstrapowych), co widać w przeskoku z okolic 0.69 (pojedyncze drzewo) do okolic 0.73 (bagging). Różnica między drzewem surowym a przyciętym jest mała, co sugeruje, że przycinanie zmniejsza złożoność i poprawia stabilność/interpretowalność, ale w tym problemie nie zmienia istotnie granic decyzyjnych w obszarach kluczowych dla wyniku testowego.\nZaktualizowany wynik AdaBoost jest istotnie lepszy niż wcześniej, co może wynikać z innej konfiguracji (np. liczby estymatorów, głębokości weak learner lub domyślnego algorytmu wieloklasowego), jednak nadal widać typową cechę: w zadaniach tablicowych wieloklasowych nowocześniejsze warianty gradient boostingu drzew (XGBoost/LightGBM/CatBoost) zwykle zapewniają lepszy kompromis między bias i wariancją oraz lepszą kontrolę złożoności.\nWarto zauważyć, że w tym porównaniu accuracy, balanced_accuracy i F1_macro są bardzo zbliżone dla najlepszych modeli. To oznacza, że (po ograniczeniu do 10 najczęstszych gatunków) nie obserwujemy silnego problemu dominacji jednej klasy; w takich warunkach macro-F1 jest szczególnie użyteczne jako miara „sprawiedliwości” względem klas, bo każdej klasie przypisuje podobną wagę.\n\n\n\n\nBreiman, Leo. 1996. „Bagging Predictors”. Machine Learning 24 (2): 123–40. https://doi.org/10.1023/a:1018054314350.\n\n\n———. 2001. „Random Forests”. Machine Learning 45 (1): 5–32. https://doi.org/10.1023/a:1010933404324.\n\n\nBreiman, Leo, Jerome Friedman, R. A. Olshen, i Charles J. Stone. 2017. Classification and Regression Trees. New York: Chapman; Hall/CRC.\n\n\nChen, Tianqi, i Carlos Guestrin. 2016. „XGBoost”. Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, sierpień, 785–94. https://doi.org/10.1145/2939672.2939785.\n\n\nEfron, B. 1979. „Bootstrap Methods: Another Look at the Jackknife”. The Annals of Statistics 7 (1). https://doi.org/10.1214/aos/1176344552.\n\n\nFreund, Yoav, i Robert E Schapire. 1997. „A Decision-Theoretic Generalization of On-Line Learning and an Application to Boosting”. Journal of Computer and System Sciences 55 (1): 119–39. https://doi.org/10.1006/jcss.1997.1504.\n\n\nFriedman, Jerome H. 2001. „Greedy function approximation: A gradient boosting machine.” The Annals of Statistics 29 (5). https://doi.org/10.1214/aos/1013203451.\n\n\nGeurts, Pierre, Damien Ernst, i Louis Wehenkel. 2006. „Extremely Randomized Trees”. Machine Learning 63 (1): 3–42. https://doi.org/10.1007/s10994-006-6226-1.\n\n\nHothorn, Torsten, Kurt Hornik, i Achim Zeileis. 2006. „Unbiased Recursive Partitioning: A Conditional Inference Framework”. Journal of Computational and Graphical Statistics 15 (3): 651–74. https://doi.org/10.1198/106186006X133933.\n\n\nKass, G. V. 1980. „An Exploratory Technique for Investigating Large Quantities of Categorical Data”. Applied Statistics 29 (2): 119. https://doi.org/10.2307/2986296.\n\n\nKe, Guolin, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei Ye, i Tie-Yan Liu. 2017. „LightGBM: A Highly Efficient Gradient Boosting Decision Tree”. W. T. 30. Curran Associates, Inc. https://proceedings.neurips.cc/paper_files/paper/2017/hash/6449f44a102fde848669bdd9eb6b76fa-Abstract.html.\n\n\nKuhn, Max, i Ross Quinlan. 2018. C50: C5.0 Decision Trees and Rule-Based Models. https://CRAN.R-project.org/package=C50.\n\n\nQuinlan, J Ross. 1993. C4. 5: Programs for Machine Learning. Morgan Kaufmann.\n\n\nQuinlan, J. R. 1986. „Induction of Decision Trees”. Machine Learning 1 (1): 81–106. https://doi.org/10.1007/BF00116251.\n\n\nSchapire, Robert E. 1990. „The Strength of Weak Learnability”. Machine Learning 5 (2): 197–227. https://doi.org/10.1023/a:1022648800760.\n\n\nSingh, Pranjal Kumar, i Seba Susan. 2025. „EfficientNetB0-CatBoost: Deep Learning With Categorical Boosting for Food Image Recognition”. Cureus Journal of Computer Science, październik. https://doi.org/10.7759/s44389-025-03791-2.\n\n\nTin Kam Ho. 1998. „The random subspace method for constructing decision forests”. IEEE Transactions on Pattern Analysis and Machine Intelligence 20 (8): 832–44. https://doi.org/10.1109/34.709601.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Drzewa decyzyjne i zespoly modeli</span>"
    ]
  },
  {
    "objectID": "chapters/05-bayes.html",
    "href": "chapters/05-bayes.html",
    "title": "6  Klasyfikatory bayesowskie",
    "section": "",
    "text": "6.1 Teoria Bayesa i dwa podejścia estymacji\nPunktem wyjścia dla metod bayesowskich jest rozumienie, że niepewność co do klasy \\(y\\) (lub parametrów modelu) można opisywać probabilistycznie. W klasyfikacji interesuje nas zwykle rozkład a posteriori klas \\(\\mathbb{P}(y\\mid x)\\), czyli prawdopodobieństwo klasy po zaobserwowaniu cech \\(x\\). Kluczową rolę odgrywa twierdzenie Bayesa, które dla klasyfikacji przyjmuje postać:\n\\[\n\\mathbb{P}(y=k\\mid x)=\\frac{\\mathbb{P}(x\\mid y=k)\\,\\mathbb{P}(y=k)}{\\mathbb{P}(x)}.\n\\]\ngdzie\nW uczeniu nadzorowanym musimy oszacować parametry modelu \\(\\mathbb{P}(x\\mid y)\\) i \\(\\mathbb{P}(y)\\) na podstawie danych treningowych \\(\\{(x_i,y_i)\\}_{i=1}^n\\). W klasycznej estymacji spotyka się dwa fundamentalne podejścia: MLE i MAP.\nW podejściu MLE (maximum likelihood estimation) parametry \\(\\theta\\) wybiera się tak, aby maksymalizowały prawdopodobieństwo zaobserwowanych danych (wiarygodność):\n\\[\n\\hat\\theta_{\\text{MLE}}\n=\\arg\\max_{\\theta}\\ \\mathbb{P}(\\mathcal{D}\\mid \\theta)\n=\\arg\\max_{\\theta}\\ \\prod_{i=1}^n \\mathbb{P}(x_i,y_i\\mid \\theta).\n\\]\nW kontekście klasyfikatorów bayesowskich często rozdziela się to na estymację priory klas i parametrów rozkładów cech w klasach. MLE nie wprowadza dodatkowych założeń poza strukturą modelu i jest „czysto danych-zależne”.\nW podejściu MAP (maximum a posteriori) dopuszczamy wcześniejsze przekonanie o parametrach w postaci prioru \\(\\mathbb{P}(\\theta)\\) i maksymalizujemy rozkład a posteriori parametrów:\n\\[\n\\hat\\theta_{\\text{MAP}}\n=\\arg\\max_{\\theta}\\ \\mathbb{P}(\\theta\\mid \\mathcal{D})\n=\\arg\\max_{\\theta}\\ \\mathbb{P}(\\mathcal{D}\\mid \\theta)\\,\\mathbb{P}(\\theta).\n\\]\nTo podejście można rozumieć jako wprowadzenie regularizacji: prior \\(\\mathbb{P}(\\theta)\\) „ściąga” estymator w stronę wartości bardziej prawdopodobnych a priori. W szczególności, gdy prior jest jednostajny (płaski), MAP redukuje się do MLE. Gdy prior preferuje pewne wartości (np. gładkość, brak zerowych prawdopodobieństw), MAP stabilizuje estymację, zwłaszcza dla małych prób lub rzadkich zdarzeń.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Klasyfikatory bayesowskie</span>"
    ]
  },
  {
    "objectID": "chapters/05-bayes.html#teoria-bayesa-i-dwa-podejścia-estymacji",
    "href": "chapters/05-bayes.html#teoria-bayesa-i-dwa-podejścia-estymacji",
    "title": "6  Klasyfikatory bayesowskie",
    "section": "",
    "text": "\\(\\mathbb{P}(y=k)\\) to rozkład a priori (prior) – „przekonanie” o częstości klas przed zobaczeniem \\(x\\),\n\\(\\mathbb{P}(x\\mid y=k)\\) to wiarygodność (likelihood) – model generowania cech w klasie \\(k\\),\n\\(\\mathbb{P}(x)\\) to stała normalizacyjna (taka sama dla wszystkich klas podczas porównywania), często pomijana w regule decyzyjnej,\n\\(\\mathbb{P}(y=k\\mid x)\\) to rozkład a posteriori (posterior).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Klasyfikatory bayesowskie</span>"
    ]
  },
  {
    "objectID": "chapters/05-bayes.html#klasyfikator-mapml",
    "href": "chapters/05-bayes.html#klasyfikator-mapml",
    "title": "6  Klasyfikatory bayesowskie",
    "section": "6.2 Klasyfikator MAP/ML",
    "text": "6.2 Klasyfikator MAP/ML\nNiezależnie od tego, czy parametry \\(\\theta\\) szacujemy przez MLE czy MAP, reguła klasyfikacji wynika z Bayesa. Dla nowej obserwacji \\(x\\) wybieramy klasę maksymalizującą posterior:\n\\[\n\\hat y(x) = \\arg\\max_{k}\\ \\mathbb{P}(y=k\\mid x)\n= \\arg\\max_{k}\\ \\mathbb{P}(x\\mid y=k)\\,\\mathbb{P}(y=k).\n\\]\nW praktyce obliczenia wykonuje się na logarytmach:\n\\[\n\\hat y(x)=\\arg\\max_k \\left[\\log \\mathbb{P}(y=k)+\\log \\mathbb{P}(x\\mid y=k)\\right].\n\\]\nRóżnica „ML vs MAP” w klasyfikatorze polega na tym, jak wyznaczamy parametry prior i likelihood.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Klasyfikatory bayesowskie</span>"
    ]
  },
  {
    "objectID": "chapters/05-bayes.html#naiwny-bayes-w-wariancie-ml",
    "href": "chapters/05-bayes.html#naiwny-bayes-w-wariancie-ml",
    "title": "6  Klasyfikatory bayesowskie",
    "section": "6.3 Naiwny Bayes w wariancie ML",
    "text": "6.3 Naiwny Bayes w wariancie ML\nW wariancie ML estymujemy parametry likelihood na podstawie częstości/średnich z danych treningowych bez dodatkowych „pseudo-obserwacji”. Typowy problem - jeśli pewna cecha/kategoria nie wystąpiła w klasie w treningu, to MLE daje prawdopodobieństwo równe 0, a wtedy cały iloczyn \\(\\mathbb{P}(x\\mid y=k)\\) może stać się 0. To jest jedna z głównych motywacji do wersji MAP z wygładzaniem.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Klasyfikatory bayesowskie</span>"
    ]
  },
  {
    "objectID": "chapters/05-bayes.html#naiwny-bayes-w-wariancie-map",
    "href": "chapters/05-bayes.html#naiwny-bayes-w-wariancie-map",
    "title": "6  Klasyfikatory bayesowskie",
    "section": "6.4 Naiwny Bayes w wariancie MAP",
    "text": "6.4 Naiwny Bayes w wariancie MAP\nW wariancie MAP wprowadzamy prior na parametry rozkładu. W praktyce dla modeli dyskretnych (tekst, cechy binarne) standardem jest prior Dirichleta/Beta, co daje proste „wygładzanie” (smoothing). Przykładowo, dla wielomianowego NB (MultinomialNB) przyjmując prior Dirichleta z parametrem \\(\\alpha&gt;0\\), otrzymujemy estymator:\n\\[\n\\hat\\theta_{k,j}=\n\\frac{N_{k,j}+\\alpha}{N_k+\\alpha\\,V},\n\\]\ngdzie \\(N_{k,j}\\) to liczność (np. suma zliczeń słowa \\(j\\) w klasie \\(k\\)), \\(N_k\\) to suma zliczeń w klasie, a \\(V\\) to liczba cech/słów. Widać tu regularizację, bo nawet gdy \\(N_{k,j}=0\\), prawdopodobieństwo nie jest zerowe.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Klasyfikatory bayesowskie</span>"
    ]
  },
  {
    "objectID": "chapters/05-bayes.html#naive-bayes",
    "href": "chapters/05-bayes.html#naive-bayes",
    "title": "6  Klasyfikatory bayesowskie",
    "section": "6.5 Naive Bayes",
    "text": "6.5 Naive Bayes\nNaiwny Bayes jest rodziną probabilistycznych klasyfikatorów opartych na twierdzeniu Bayesa i kluczowym założeniu warunkowej niezależności cech w obrębie klasy:\n\\[\n\\mathbb{P}(x\\mid y=k)=\\prod_{j=1}^{p}\\mathbb{P}(x_j\\mid y=k).\n\\]\nTo założenie rzadko jest dosłownie prawdziwe w danych rzeczywistych, ale w wielu zastosowaniach działa zaskakująco dobrze, ponieważ model dobrze aproksymuje relacje klasy–cechy, a błąd wynikający z zależności bywa „przykrywany” przez prostotę i stabilność estymacji.\nW praktyce spotyka się kilka najważniejszych wariantów Naive Bayes, zależnie od rodzaju cech:\n\nGaussianNB - cechy ciągłe, w klasie zakładamy rozkład normalny dla każdej cechy,\nMultinomialNB - cechy dyskretne nieujemne (np. zliczenia słów w tekście),\nBernoulliNB - cechy binarne (0/1), np. obecność słowa.\n\nNaiwny Bayes jest szczególnie skuteczny w klasyfikacji tekstu, filtracji spamu i analizie sentymentu, ponieważ (i) dane są wysokowymiarowe, (ii) wiele cech jest rzadkich, a (iii) prostota modelu ogranicza przeuczenie. W tekście typowe jest użycie MultinomialNB z wygładzaniem (MAP), bo problem zerowych częstości występuje bardzo często.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Klasyfikatory bayesowskie</span>"
    ]
  },
  {
    "objectID": "chapters/05-bayes.html#przykłady",
    "href": "chapters/05-bayes.html#przykłady",
    "title": "6  Klasyfikatory bayesowskie",
    "section": "6.6 Przykłady",
    "text": "6.6 Przykłady\nPoniżej znajdują się praktyczne przykłady dla trzech głównych wariantów. Kluczowa uwaga: w scikit-learn parametr alpha w MultinomialNB i BernoulliNB odpowiada wygładzaniu. Ustawienie alpha=0 odpowiada MLE, ale zwykle prowadzi do problemów z zerowymi prawdopodobieństwami, więc jest rzadko używane w praktyce.\n\n6.6.1 Przygotowanie danych\nCel zadania: Klasyfikacja emocji w krótkich tekstach (tweetach) na podstawie ich treści. Będziemy przewidywać jedną z 6 emocji: sadness (smutek), joy (radość), love (miłość), anger (gniew), fear (strach), surprise (zaskoczenie).\nDlaczego ten problem jest idealny dla Naive Bayes?\n\nWysokowymiarowość: tysiące unikalnych słów jako cechy\nRzadkość: większość słów nie występuje w danym dokumencie (macierz rzadka)\nZałożenie niezależności jest rozsądną aproksymacją dla występowania słów\nSzybkość uczenia i predykcji nawet dla dużych korpusów tekstowych\n\nPrzygotujemy dane w trzech różnych reprezentacjach, aby zillustrować działanie trzech wariantów Naive Bayes:\n\n\nKod\nimport numpy as np\nfrom datasets import load_dataset\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n\n# Ładowanie datasetu emotion (6 klas emocji: sadness, joy, love, anger, fear, surprise)\ndataset = load_dataset(\"emotion\", split=\"train\")\ndataset_test = load_dataset(\"emotion\", split=\"test\")\n\n# Konwersja do list\ntexts_train = dataset[\"text\"]\ny_train = np.array(dataset[\"label\"])\ntexts_test = dataset_test[\"text\"]\ny_test = np.array(dataset_test[\"label\"])\n\n# 1. Bag-of-Words (zliczenia słów) dla MultinomialNB\n#    CountVectorizer tworzy macierz, gdzie każda kolumna to słowo, a wartość to liczba wystąpień\n#    max_features=5000: ograniczamy do 5000 najczęstszych słów (redukcja wymiarowości)\n#    min_df=5: ignorujemy słowa występujące w mniej niż 5 dokumentach (szum)\n#    max_df=0.7: ignorujemy słowa w &gt;70% dokumentów (stop words, mało dyskryminujące)\nvectorizer = CountVectorizer(max_features=5000, min_df=5, max_df=0.7)\nX = vectorizer.fit_transform(texts_train)\nX_test_counts = vectorizer.transform(texts_test)\n\n# 2. Reprezentacja binarna (obecność/brak) dla BernoulliNB\n#    BernoulliNB modeluje P(słowo obecne|klasa), nie liczby wystąpień\n#    Konwertujemy zliczenia na 0/1: czy słowo wystąpiło w dokumencie czy nie\nX_bin = (X &gt; 0).astype(int)\nX_test_bin = (X_test_counts &gt; 0).astype(int)\n\n# 3. TF-IDF (cechy ciągłe) dla GaussianNB\n#    TF-IDF = Term Frequency × Inverse Document Frequency\n#    Daje wyższe wagi rzadkim, ale informacyjnym słowom\n#    Traktujemy te wagi jako cechy ciągłe z rozkładem normalnym\n#    max_features=100: dla GaussianNB ograniczamy liczbę cech (stabilność kovariancji)\ntfidf = TfidfVectorizer(max_features=100, min_df=5, max_df=0.7)\nX_cont = tfidf.fit_transform(texts_train).toarray()\nX_test_cont = tfidf.transform(texts_test).toarray()\n\nprint(f\"Liczba próbek treningowych: {X.shape[0]}\")\nprint(f\"Liczba cech (słów): {X.shape[1]}\")\nprint(f\"Rozkład klas: {np.bincount(y_train)}\")\n\n\nLiczba próbek treningowych: 16000\nLiczba cech (słów): 3421\nRozkład klas: [4666 5362 1304 2159 1937  572]\n\n\n\n\n6.6.2 MultinomialNB\nMultinomialNB modeluje rozkład wielomianowy - zakłada, że dokumenty są generowane przez losowe wybieranie słów z rozkładu charakterystycznego dla danej klasy. Dla każdej klasy \\(k\\) i słowa \\(j\\) estymuje prawdopodobieństwo \\(\\mathbb{P}(słowo_j\\mid klasa=k)\\) na podstawie proporcji wystąpień tego słowa w klasie. Wykorzystuje zliczenia słów - im częściej dane słowo występuje w dokumencie, tym silniej wpływa na klasyfikację.\nParametr alpha kontroluje wygładzanie Laplace’a (MAP z priorem Dirichleta): większe alpha = silniejsza regularyzacja, mniejsze prawdopodobieństwo zerowe dla niewidzianych słów.\n\n\nKod\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n\n# MAP (Dirichlet prior): alpha &gt; 0 (np. Laplace alpha=1.0 lub łagodniejsze 0.1)\nnb_map = MultinomialNB(alpha=1.0)\nnb_map.fit(X, y_train)\n\ny_pred_map = nb_map.predict(X_test_counts)\nprint(\"MultinomialNB z wygładzaniem Laplace'a (alpha=1.0, MAP):\")\nprint(f\"Accuracy: {accuracy_score(y_test, y_pred_map):.4f}\")\nprint(\"\\nConfusion Matrix:\")\nprint(confusion_matrix(y_test, y_pred_map))\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test, y_pred_map))\n\n# Porównanie z mniejszym wygładzaniem\nnb_map_01 = MultinomialNB(alpha=0.1)\nnb_map_01.fit(X, y_train)\ny_pred_01 = nb_map_01.predict(X_test_counts)\nprint(f\"\\nAccuracy z alpha=0.1: {accuracy_score(y_test, y_pred_01):.4f}\")\n\n\nMultinomialNB z wygładzaniem Laplace'a (alpha=1.0, MAP):\nAccuracy: 0.8460\n\nConfusion Matrix:\n[[531  21   6  14   9   0]\n [ 16 637  26   9   4   3]\n [  8  42  99   7   1   2]\n [ 24  14   1 226  10   0]\n [ 26  10   1  11 174   2]\n [  5  18   0   2  16  25]]\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.87      0.91      0.89       581\n           1       0.86      0.92      0.89       695\n           2       0.74      0.62      0.68       159\n           3       0.84      0.82      0.83       275\n           4       0.81      0.78      0.79       224\n           5       0.78      0.38      0.51        66\n\n    accuracy                           0.85      2000\n   macro avg       0.82      0.74      0.77      2000\nweighted avg       0.84      0.85      0.84      2000\n\n\nAccuracy z alpha=0.1: 0.8260\n\n\n\n\n6.6.3 BernoulliNB\nBernoulliNB modeluje rozkład Bernoulliego dla każdej cechy - traktuje każde słowo jako próbę Bernoulliego (wystąpiło/nie wystąpiło). W przeciwieństwie do MultinomialNB, nie liczy ile razy słowo wystąpiło, tylko czy w ogóle się pojawiło. Estymuje dwa prawdopodobieństwa dla każdej klasy \\(k\\) i słowa \\(j\\):\n\n\\(\\mathbb{P}(słowo_j=1\\mid klasa=k)\\) - prawdopodobieństwo obecności słowa\n\\(\\mathbb{P}(słowo_j=0\\mid klasa=k)\\) - prawdopodobieństwo braku słowa (również używane!)\n\nJest to przydatne, gdy struktura dokumentu (które słowa występują) jest ważniejsza niż ich częstość.\n\n\nKod\nfrom sklearn.naive_bayes import BernoulliNB\n\n# alpha &gt; 0 to MAP (Beta prior / smoothing)\n# X_bin: macierz 0/1 reprezentująca obecność/brak słowa\nbnb = BernoulliNB(alpha=1.0)\nbnb.fit(X_bin, y_train)\n\ny_pred_bnb = bnb.predict(X_test_bin)\nprint(\"BernoulliNB (reprezentacja binarna):\")\nprint(f\"Accuracy: {accuracy_score(y_test, y_pred_bnb):.4f}\")\nprint(\"\\nConfusion Matrix:\")\nprint(confusion_matrix(y_test, y_pred_bnb))\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test, y_pred_bnb))\n\n\nBernoulliNB (reprezentacja binarna):\nAccuracy: 0.8200\n\nConfusion Matrix:\n[[510  16  18  19  10   8]\n [ 17 632  31   8   4   3]\n [ 12  50  88   4   3   2]\n [ 25   8   1 228  11   2]\n [ 28   9   3  11 168   5]\n [ 11  24   0   1  16  14]]\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.85      0.88      0.86       581\n           1       0.86      0.91      0.88       695\n           2       0.62      0.55      0.59       159\n           3       0.84      0.83      0.84       275\n           4       0.79      0.75      0.77       224\n           5       0.41      0.21      0.28        66\n\n    accuracy                           0.82      2000\n   macro avg       0.73      0.69      0.70      2000\nweighted avg       0.81      0.82      0.81      2000\n\n\n\n\n\n6.6.4 GaussianNB\nGaussianNB zakłada, że cechy ciągłe mają w obrębie każdej klasy rozkład normalny (gaussowski). Dla każdej klasy \\(k\\) i cechy \\(j\\) estymuje średnią \\(\\mu_{k,j}\\) i wariancję \\(\\sigma^2_{k,j}\\), a następnie wykorzystuje gęstość rozkładu normalnego do obliczenia \\(\\mathbb{P}(x_j\\mid klasa=k)\\).\nW kontekście NLP: TF-IDF daje wagi ciągłe (nie zliczenia), co pozwala traktować je jako zmienne losowe o rozkładzie normalnym. GaussianNB nie zakłada niezależności wartości cech między próbkami, ale nadal zakłada warunkową niezależność cech w obrębie klasy.\nUwaga: GaussianNB zazwyczaj działa gorzej na tekście niż MultinomialNB/BernoulliNB, ponieważ założenie normalności jest słabe dla rzadkich, dyskretnych danych tekstowych. Używamy go tu dla ilustracji.\nDla GaussianNB zakładamy: \\[\nx_j \\mid (y=k) \\sim \\mathcal{N}(\\mu_{k,j}, \\sigma_{k,j}^2).\n\\]\nMLE daje \\[\n\\hat\\mu_{k,j}=\\frac{1}{n_k}\\sum_{i:y_i=k} x_{i,j},\\qquad\n\\hat\\sigma^2_{k,j}=\\frac{1}{n_k}\\sum_{i:y_i=k}(x_{i,j}-\\hat\\mu_{k,j})^2\n\\]\n\n\nKod\nfrom sklearn.naive_bayes import GaussianNB\n\n# Używamy cech ciągłych (TF-IDF)\ngnb = GaussianNB()\ngnb.fit(X_cont, y_train)\n\ny_pred_gnb = gnb.predict(X_test_cont)\nprint(\"GaussianNB (cechy ciągłe TF-IDF):\")\nprint(f\"Accuracy: {accuracy_score(y_test, y_pred_gnb):.4f}\")\nprint(\"\\nConfusion Matrix:\")\nprint(confusion_matrix(y_test, y_pred_gnb))\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test, y_pred_gnb))\n\n\nGaussianNB (cechy ciągłe TF-IDF):\nAccuracy: 0.2340\n\nConfusion Matrix:\n[[121 100 104  91  68  97]\n [118 193 140  89  71  84]\n [ 27  33  46  20  15  18]\n [ 45  43  46  47  35  59]\n [ 38  37  25  32  44  48]\n [ 11  13   5   7  13  17]]\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.34      0.21      0.26       581\n           1       0.46      0.28      0.35       695\n           2       0.13      0.29      0.18       159\n           3       0.16      0.17      0.17       275\n           4       0.18      0.20      0.19       224\n           5       0.05      0.26      0.09        66\n\n    accuracy                           0.23      2000\n   macro avg       0.22      0.23      0.20      2000\nweighted avg       0.31      0.23      0.26      2000\n\n\n\nWarto zaznaczyć, że GaussianNB nie wymaga skalowania „z konieczności” (bo pracuje na gęstościach dla każdej cechy), ale w praktyce ekstremalnie różne skale mogą wpływać na stabilność numeryczną i porównywalność cech.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Klasyfikatory bayesowskie</span>"
    ]
  },
  {
    "objectID": "chapters/06-knn.html",
    "href": "chapters/06-knn.html",
    "title": "7  k-NN",
    "section": "",
    "text": "7.1 Metoda k-NN - intuicja i formalizacja\nMetoda k-NN jest klasycznym przykładem podejścia nieparametrycznego i instance-based learning - nie budujemy jawnej postaci funkcji decyzyjnej podczas treningu, lecz przechowujemy zbiór uczący i podejmujemy decyzję dopiero w momencie predykcji. W sensie uczenia nadzorowanego dane treningowe \\(\\{(x_i,y_i)\\}_{i=1}^n\\) pełnią rolę „pamięci”, a nową obserwację \\(x\\) klasyfikujemy (lub przewidujemy wartość) na podstawie najbliższych obserwacji w przestrzeni cech.\nFormalnie wybieramy metrykę odległości \\(d(\\cdot,\\cdot)\\). Dla każdej obserwacji testowej \\(x\\) wyznaczamy zbiór indeksów \\(N_k(x)\\) odpowiadający \\(k\\) najbliższym sąsiadom:\n\\[\nN_k(x)=\\operatorname{arg\\,min}_{S\\subset\\{1,\\dots,n\\},\\,|S|=k} \\sum_{i\\in S} d(x,x_i).\n\\]\nW praktyce jest to po prostu wybór \\(k\\) najmniejszych wartości z listy \\(\\{d(x,x_i)\\}_{i=1}^n\\).",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>k-NN</span>"
    ]
  },
  {
    "objectID": "chapters/06-knn.html#metoda-k-nn---intuicja-i-formalizacja",
    "href": "chapters/06-knn.html#metoda-k-nn---intuicja-i-formalizacja",
    "title": "7  k-NN",
    "section": "",
    "text": "7.1.1 Metryki odległości\nNajczęściej stosujemy metryki typu Minkowskiego:\n\\[\nd_p(x,z)=\\left(\\sum_{j=1}^p |x_j-z_j|^p\\right)^{1/p}.\n\\]\nSzczególne przypadki:\n\n\\(p=2\\) - odległość euklidesowa,\n\\(p=1\\) - odległość Manhattan,\n\\(p\\to\\infty\\) - odległość Czebyszewa \\(d_\\infty(x,z)=\\max_j |x_j-z_j|\\).\n\nW praktyce wybór metryki i skali cech jest krytyczny, ponieważ k-NN opiera się bezpośrednio na geometrii przestrzeni cech.\n\n\n7.1.2 Reguła decyzyjna: klasyfikacja i regresja\n\n7.1.2.1 Klasyfikacja\nDla klasyfikacji wieloklasowej \\(y\\in\\{1,\\dots,K\\}\\) predykcję definiujemy przez głosowanie większościowe wśród sąsiadów:\n\\[\n\\hat y(x)=\\arg\\max_{c\\in\\{1,\\dots,K\\}} \\sum_{i\\in N_k(x)} \\mathbb{1}\\{y_i=c\\}.\n\\]\nCzęsto stosujemy wersję ważoną odległością (bliżsi sąsiedzi mają większy wpływ), np. z wagami:\n\\[\nw_i(x)=\\frac{1}{d(x,x_i)+\\varepsilon},\n\\]\ni wtedy:\n\\[\n\\hat y(x)=\\arg\\max_{c} \\sum_{i\\in N_k(x)} w_i(x)\\,\\mathbb{1}\\{y_i=c\\}.\n\\]\n\n\n7.1.2.2 Regresja\nW regresji \\(y\\in\\mathbb{R}\\) najprostsza wersja to średnia wśród sąsiadów:\n\\[\n\\hat y(x)=\\frac{1}{k}\\sum_{i\\in N_k(x)} y_i,\n\\]\na wersja ważona:\n\\[\n\\hat y(x)=\\frac{\\sum_{i\\in N_k(x)} w_i(x) y_i}{\\sum_{i\\in N_k(x)} w_i(x)}.\n\\]\n\n\n\n7.1.3 Rola parametru \\(k\\) - kompromis bias–variance\nParametr \\(k\\) kontroluje gładkość granicy decyzyjnej i jest klasycznym przykładem kompromisu bias–variance.\n\nDla małego \\(k\\) (np. 1–5) otrzymujemy model bardzo elastyczny - granice są poszarpane, bias jest niski, a wariancja wysoka (łatwiej dopasowujemy szum).\nDla dużego \\(k\\) otrzymujemy granice gładkie - rośnie bias (uśredniamy lokalne struktury), ale maleje wariancja.\n\nW praktyce \\(k\\) dobieramy przez walidację krzyżową, a w danych wielowymiarowych potrzebujemy dodatkowo zadbać o metrykę, ważenie oraz selekcję/skalę cech.\n\n\n7.1.4 Zalety i wady k-NN\n\n7.1.4.1 Zalety\nk-NN jest prosty koncepcyjnie, nie zakłada konkretnej postaci funkcji decyzyjnej (nieparametryczny), potrafi modelować nieliniowe granice decyzyjne i często działa dobrze jako baseline. Jest też naturalny wtedy, gdy „podobieństwo” obserwacji ma sens merytoryczny i chcemy bezpośrednio na nim oprzeć decyzję.\n\n\n7.1.4.2 Wady\nk-NN jest wrażliwy na skalę cech (zwykle musimy standaryzować), na obecność nieistotnych cech oraz na dobór metryki. Jest też kosztowny obliczeniowo w predykcji (dla dużego \\(n\\) liczymy wiele odległości), chyba że stosujemy struktury przyspieszające (KD-tree, Ball-tree) lub wyszukiwanie przybliżone. Wreszcie, k-NN jest podatny na curse of dimensionality - w przestrzeni wielowymiarowej „wszyscy są daleko”, a pojęcie bliskości traci sens.\n\n\n\n7.1.5 Wpływ \\(k\\) na brzeg decyzyjny\n\n\nKod\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.datasets import make_moons\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\n# 1) Dane 2D: dwie klasy (nieliniowa separacja)\nX, y = make_moons(n_samples=800, noise=0.35, random_state=44)\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.25, random_state=42, stratify=y\n)\n\ndef plot_decision_boundary(ax, model, X, y, title):\n    # siatka\n    x_min, x_max = X[:, 0].min() - 0.8, X[:, 0].max() + 0.8\n    y_min, y_max = X[:, 1].min() - 0.8, X[:, 1].max() + 0.8\n    xx, yy = np.meshgrid(\n        np.linspace(x_min, x_max, 400),\n        np.linspace(y_min, y_max, 400)\n    )\n    grid = np.c_[xx.ravel(), yy.ravel()]\n    Z = model.predict(grid).reshape(xx.shape)\n\n    ax.contourf(xx, yy, Z, alpha=0.25)\n    ax.scatter(X[:, 0], X[:, 1], c=y, s=20, alpha=0.85)\n    ax.set_title(title)\n    ax.set_xlabel(\"x1\")\n    ax.set_ylabel(\"x2\")\n\nks = [5, 20, 100]\n\nfig, axes = plt.subplots(1, 3, figsize=(15, 4), constrained_layout=True)\n\nfor ax, k in zip(axes, ks):\n    knn = Pipeline(steps=[\n        (\"scaler\", StandardScaler()),   # k-NN jest wrażliwy na skalę\n        (\"model\", KNeighborsClassifier(n_neighbors=k))\n    ])\n    knn.fit(X_train, y_train)\n    plot_decision_boundary(ax, knn, X_test, y_test, f\"k-NN: k={k} (brzeg decyzyjny)\")\n\nplt.show()",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>k-NN</span>"
    ]
  },
  {
    "objectID": "chapters/06-knn.html#przykład",
    "href": "chapters/06-knn.html#przykład",
    "title": "7  k-NN",
    "section": "7.2 Przykład",
    "text": "7.2 Przykład\nPoniżej stosujemy k-NN na danych mstz/breast (konfiguracja cancer). Dobieramy \\(k\\), metrykę i ważenie przez walidację krzyżową, a następnie oceniamy model na zbiorze testowym (metryki + macierz pomyłek + ROC). Ponieważ k-NN opiera się na odległościach, w pipeline stosujemy standaryzację.\n\n\nKod\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom datasets import load_dataset\n\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.neighbors import KNeighborsClassifier\n\nfrom sklearn.metrics import (\n    confusion_matrix, ConfusionMatrixDisplay,\n    accuracy_score, balanced_accuracy_score, f1_score,\n    roc_auc_score, roc_curve\n)\n\n# 1) Dane z Hugging Face\nds = load_dataset(\"mstz/breast\", \"cancer\")[\"train\"]\ndf = ds.to_pandas()\n\ntarget = \"is_cancer\"\ny = df[target].astype(int)\nX = df.drop(columns=[target])\n\n# 2) Podział train/test\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.25, random_state=42, stratify=y\n)\n\n# 3) Pipeline k-NN (imputacja + standaryzacja + model)\npipe = Pipeline(steps=[\n    (\"imp\", SimpleImputer(strategy=\"median\")),\n    (\"scaler\", StandardScaler()),\n    (\"knn\", KNeighborsClassifier())\n])\n\n# 4) Dobór k przez CV\nparam_grid = {\n    \"knn__n_neighbors\": [1, 3, 5, 7, 9, 15, 25, 35, 50],\n    \"knn__weights\": [\"uniform\", \"distance\"],\n    \"knn__metric\": [\"minkowski\"],   # domyślnie p=2 (euklides)\n    \"knn__p\": [1, 2],               # Manhattan vs Euclidean\n}\n\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\nsearch = GridSearchCV(\n    pipe,\n    param_grid=param_grid,\n    scoring=\"roc_auc\",\n    cv=cv,\n    n_jobs=-1\n)\nsearch.fit(X_train, y_train)\n\nbest_model = search.best_estimator_\nprint(\"Najlepsze parametry:\", search.best_params_)\nprint(\"CV ROC-AUC:\", search.best_score_)\n\n# 5) Predykcja na teście\ny_pred = best_model.predict(X_test)\ny_proba = best_model.predict_proba(X_test)[:, 1]\n\nacc = accuracy_score(y_test, y_pred)\nbacc = balanced_accuracy_score(y_test, y_pred)\nf1m = f1_score(y_test, y_pred)\nauc = roc_auc_score(y_test, y_proba)\n\nprint(\"\\nMetryki (test):\")\nprint(f\"  Accuracy          : {acc:.4f}\")\nprint(f\"  Balanced accuracy : {bacc:.4f}\")\nprint(f\"  F1                : {f1m:.4f}\")\nprint(f\"  ROC-AUC           : {auc:.4f}\")\n\n# 6) Macierz pomyłek\ncm = confusion_matrix(y_test, y_pred)\ndisp = ConfusionMatrixDisplay(cm, display_labels=[\"0\", \"1\"])\n\nplt.figure(figsize=(4.5, 4))\ndisp.plot(values_format=\"d\")\nplt.title(\"k-NN: macierz pomyłek (test)\")\nplt.tight_layout()\nplt.show()\n\n# 7) ROC curve\nfpr, tpr, thr = roc_curve(y_test, y_proba)\n\nplt.figure(figsize=(5.5, 5))\nplt.plot(fpr, tpr, label=f\"AUC={auc:.3f}\")\nplt.plot([0, 1], [0, 1], linestyle=\"--\")\nplt.xlabel(\"FPR\")\nplt.ylabel(\"TPR\")\nplt.title(\"k-NN: ROC curve (test)\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\nNajlepsze parametry: {'knn__metric': 'minkowski', 'knn__n_neighbors': 25, 'knn__p': 1, 'knn__weights': 'uniform'}\nCV ROC-AUC: 0.994262274486155\n\nMetryki (test):\n  Accuracy          : 0.9591\n  Balanced accuracy : 0.9608\n  F1                : 0.9431\n  ROC-AUC           : 0.9848\n\n\n&lt;Figure size 432x384 with 0 Axes&gt;",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>k-NN</span>"
    ]
  },
  {
    "objectID": "chapters/07-splajny-gam.html",
    "href": "chapters/07-splajny-gam.html",
    "title": "8  Modele addytywne i splajny",
    "section": "",
    "text": "8.1 Wprowadzenie\nW modelowaniu danych tablicowych bardzo często obserwujemy relacje, które nie są liniowe, ale jednocześnie nie chcemy od razu sięgać po „czarne skrzynki” (typu sieci neurononwe). Splajny i modele addytywne stanowią naturalny kompromis - pozwalają modelować nieliniowości w sposób kontrolowany i interpretowalny. W praktyce splajny pojawiają się zarówno jako samodzielne narzędzie regresji, jak i jako komponenty innych metod (np. w FDA – Flexible Discriminant Analysis – oraz w GAM – Generalized Additive Models).",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Modele addytywne i splajny</span>"
    ]
  },
  {
    "objectID": "chapters/07-splajny-gam.html#regresje-sklejane-splajny",
    "href": "chapters/07-splajny-gam.html#regresje-sklejane-splajny",
    "title": "8  Modele addytywne i splajny",
    "section": "8.2 Regresje sklejane (splajny)",
    "text": "8.2 Regresje sklejane (splajny)\n\n8.2.1 Splajn jako wielomianowa regresja kawałkowa\nZałóżmy, że chcemy modelować zależność \\(y \\approx f(x)\\) dla jednej zmiennej \\(x\\), ale relacja nie jest liniowa. Najprostszą ideą jest podzielenie osi \\(x\\) na przedziały i dopasowanie na każdym z nich osobnego wielomianu. Niech \\(\\tau_1 &lt; \\tau_2 &lt; \\cdots &lt; \\tau_K\\) oznacza węzły (knots), które dzielą dziedzinę na \\(K+1\\) przedziałów. Splajn regresyjny rzędu \\(d\\) to funkcja:\n\nktóra na każdym przedziale jest wielomianem stopnia \\(d\\),\na na węzłach jest „sklejona” w sposób zapewniający gładkość.\n\nNajczęściej spotykamy splajny sześcienne (\\(d=3\\)), ponieważ dają dobrą elastyczność i gładkość, a jednocześnie nie są tak niestabilne jak wielomiany wysokiego stopnia dopasowywane globalnie.\n\n\n8.2.2 Warunki gładkości na węzłach\nSamo „posklejanie” wielomianów bez warunków prowadzi do nieciągłości i załamań. Dlatego narzuca się warunki zgodności w węzłach. Dla splajnu sześciennego standardem jest wymóg ciągłości funkcji i jej pochodnych do rzędu drugiego:\n\\[\nf(\\tau_k^-)=f(\\tau_k^+),\\quad f'(\\tau_k^-)=f'(\\tau_k^+),\\quad f''(\\tau_k^-)=f''(\\tau_k^+),\\qquad k=1,\\ldots,K.\n\\]\nWtedy splajn jest funkcją gładką (ciągłą z ciągłą krzywizną) na całej dziedzinie.\n\n\nKod\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# (opcjonalnie, ale zalecane) SciPy daje wygodną implementację \"prawdziwego\" splajnu sześciennego LSQ\nfrom scipy.interpolate import LSQUnivariateSpline\n\n# ============================================================\n# 1) Dane i \"prawdziwa\" funkcja f(x)\n# ============================================================\nrng = np.random.default_rng(42)\n\nn = 220\nx = np.sort(rng.uniform(0, 10, size=n))\n\ndef f_true(x):\n    # nieliniowa funkcja referencyjna\n    return 0.8*np.sin(1.2*x) + 0.15*x + 0.3*np.cos(0.5*x)\n\ny = f_true(x) + rng.normal(0, 0.25, size=n)  # obserwacje z szumem\n\n# Siatka do rysowania funkcji\nxg = np.linspace(x.min(), x.max(), 1200)\nyg_true = f_true(xg)\n\n# ============================================================\n# 2) Węzły (knots): tau_1 &lt; ... &lt; tau_K\n#    Bierzemy np. kwantyle (wewnętrzne węzły muszą leżeć w (min,max))\n# ============================================================\nK = 4\ntau = np.quantile(x, [0.2, 0.4, 0.6, 0.8])  # węzły wewnętrzne\ntau = np.unique(tau)  # na wypadek powtórzeń\n# Przedziały: (-inf, tau1], (tau1, tau2], ..., (tauK, +inf)\nbounds = np.r_[x.min(), tau, x.max()]\n\n# Pomocniczo: funkcja zwracająca indeks przedziału\ndef interval_index(x_val, bounds):\n    # zwraca i takie, że x_val należy do [bounds[i], bounds[i+1]]\n    return np.clip(np.searchsorted(bounds[1:-1], x_val, side=\"right\"), 0, len(bounds)-2)\n\n# ============================================================\n# 3) Aproksymacja 1: funkcje stałe na przedziałach (piecewise constant)\n#    - w każdym przedziale dopasowujemy stałą = średnia y w tym przedziale\n# ============================================================\nconst_vals = np.zeros(len(bounds)-1)\nfor i in range(len(bounds)-1):\n    left, right = bounds[i], bounds[i+1]\n    if i == 0:\n        mask = (x &gt;= left) & (x &lt;= right)\n    else:\n        mask = (x &gt; left) & (x &lt;= right)\n    const_vals[i] = np.mean(y[mask])\n\ndef piecewise_constant_predict(x_query):\n    idx = np.array([interval_index(v, bounds) for v in x_query])\n    return const_vals[idx]\n\nyg_const = piecewise_constant_predict(xg)\n\n# ============================================================\n# 4) Aproksymacja 2: wielomiany 3 stopnia na przedziałach BEZ sklejania (bez gładkości)\n#    - w każdym przedziale niezależnie dopasowujemy polyfit stopnia 3\n#    - brak warunków f(tau^-)=f(tau^+), f'(tau^-)=f'(tau^+), f''(...)  =&gt; mogą być skoki\n# ============================================================\npoly_coefs = []\nfor i in range(len(bounds)-1):\n    left, right = bounds[i], bounds[i+1]\n    if i == 0:\n        mask = (x &gt;= left) & (x &lt;= right)\n    else:\n        mask = (x &gt; left) & (x &lt;= right)\n    xi = x[mask]\n    yi = y[mask]\n    # zabezpieczenie: jeśli w przedziale jest zbyt mało punktów, obniżamy stopień\n    deg = 3 if xi.size &gt;= 6 else min(1, xi.size-1)\n    coefs = np.polyfit(xi, yi, deg=deg)\n    poly_coefs.append(coefs)\n\ndef piecewise_poly_predict(x_query):\n    yhat = np.zeros_like(x_query, dtype=float)\n    for j, v in enumerate(x_query):\n        i = interval_index(v, bounds)\n        yhat[j] = np.polyval(poly_coefs[i], v)\n    return yhat\n\nyg_poly_unsmoothed = piecewise_poly_predict(xg)\n\n# ============================================================\n# 5) Aproksymacja 3: splajn sześcienny z gładkością C^2 na węzłach\n#    - LSQUnivariateSpline: dopasowanie najmniejszych kwadratów + splajn sześcienny\n#    - splajn jest C^2: f, f', f'' są ciągłe na węzłach (spełnia warunki z opisu)\n# ============================================================\nspline = LSQUnivariateSpline(x, y, t=tau, k=3)  # k=3 =&gt; sześcienny, t =&gt; węzły wewnętrzne\nyg_spline = spline(xg)\n\n# ============================================================\n# 6) Rysunek: 4 okna\n# ============================================================\nfig, axes = plt.subplots(2, 2, figsize=(14, 9), constrained_layout=True)\naxes = axes.ravel()\n\ndef add_knots(ax):\n    for t in tau:\n        ax.axvline(t, linestyle=\"--\", linewidth=1)\n\n# (1) Prawdziwa funkcja + dane\naxes[0].plot(xg, yg_true, linewidth=2, label=\"prawdziwa f(x)\")\naxes[0].scatter(x, y, s=18, alpha=0.65, label=\"dane (szum)\")\nadd_knots(axes[0])\naxes[0].set_title(\"1) Prawdziwa funkcja f(x) oraz obserwacje\")\naxes[0].set_xlabel(\"x\")\naxes[0].set_ylabel(\"y\")\naxes[0].legend()\n\n# (2) Aproksymacja stałymi (funkcje kawałkami stałe)\naxes[1].plot(xg, yg_true, linewidth=2, alpha=0.6, label=\"prawdziwa f(x)\")\naxes[1].plot(xg, yg_const, linewidth=2, label=\"aproks. stałymi (piecewise constant)\")\naxes[1].scatter(x, y, s=14, alpha=0.35)\nadd_knots(axes[1])\naxes[1].set_title(\"2) Aproksymacja stałymi na przedziałach\")\naxes[1].set_xlabel(\"x\")\naxes[1].set_ylabel(\"y\")\naxes[1].legend()\n\n# (3) Wielomiany 3-stopnia BEZ sklejania\naxes[2].plot(xg, yg_true, linewidth=2, alpha=0.6, label=\"prawdziwa f(x)\")\naxes[2].plot(xg, yg_poly_unsmoothed, linewidth=2, label=\"piecewise cubic (bez gładkości)\")\naxes[2].scatter(x, y, s=14, alpha=0.35)\nadd_knots(axes[2])\naxes[2].set_title(\"3) Wielomiany 3-stopnia bez sklejania na węzłach\")\naxes[2].set_xlabel(\"x\")\naxes[2].set_ylabel(\"y\")\naxes[2].legend()\n\n# (4) Splajn sześcienny z gładkością C^2 na węzłach\naxes[3].plot(xg, yg_true, linewidth=2, alpha=0.6, label=\"prawdziwa f(x)\")\naxes[3].plot(xg, yg_spline, linewidth=2, label=\"splajn sześcienny (C² na węzłach)\")\naxes[3].scatter(x, y, s=14, alpha=0.35)\nadd_knots(axes[3])\naxes[3].set_title(\"4) Splajn sześcienny z warunkami gładkości na węzłach\")\naxes[3].set_xlabel(\"x\")\naxes[3].set_ylabel(\"y\")\naxes[3].legend()\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n8.2.2.1 Reprezentacja bazowa\n\nOgólna postać bazowa (wspólna dla wszystkich)\n\nKażdą z poniższych konstrukcji możemy zapisać jako model liniowy w bazie:\n\\[\nf(x)=\\sum_{j=1}^{J}\\beta_j\\, b_j(x)\n\\quad\\text{(czasem z wyrazem wolnym wliczonym w bazę)}.\n\\]\nRóżnice między „rodzajami splajnów” polegają na tym, jakie funkcje bazowe \\(b_j\\) wybieramy oraz czy dodajemy karę wygładzającą.\n\nTruncated power basis (baza potęgowa ucięta)\n\nTo najbardziej „analityczna” i łatwa do zapisania baza dla splajnów regresyjnych stopnia \\(d\\) z węzłami \\(\\tau_1&lt;\\cdots&lt;\\tau_K\\):\n\\[\nf(x)=\\beta_0+\\beta_1 x+\\cdots+\\beta_d x^d\\;+\\;\\sum_{k=1}^{K}\\theta_k\\, (x-\\tau_k)_+^{d},\n\\qquad (u)_+=\\max(u,0).\n\\]\nW praktyce intuicyjna, ale bywa gorzej uwarunkowana numerycznie niż B-splajny (szczególnie dla dużego \\(K\\) i wyższych stopni).\n\nB-splajny (B-spline basis)\n\nTo standard najczęściej stosowany i dobry numeryczny. Splajn stopnia \\(p\\) (często \\(p=3\\)) zapisujemy jako:\n\\[\nf(x)=\\sum_{j=1}^{J}\\beta_j\\, B_{j,p}(x),\n\\]\ngdzie \\(B_{j,p}(x)\\) to funkcje bazowe B-splajnów o stopniu \\(p\\), zdefiniowane rekurencyjnie (Cox–de Boor). Dla pełności:\n\ndla stopnia 0:\n\n\\[\nB_{j,0}(x)=\\mathbf{1}\\{\\kappa_j \\le x &lt; \\kappa_{j+1}\\},\n\\]\n\nrekurencja dla \\(p\\ge 1\\):\n\n\\[\nB_{j,p}(x)=\\frac{x-\\kappa_j}{\\kappa_{j+p}-\\kappa_j}B_{j,p-1}(x)\\;+\\;\n\\frac{\\kappa_{j+p+1}-x}{\\kappa_{j+p+1}-\\kappa_{j+1}}B_{j+1,p-1}(x),\n\\]\ngdzie \\(\\kappa\\) to tzw. knot vector (węzły z powtórzeniami na brzegach).\n\nSplajn naturalny (Natural Cubic Spline)\n\nSplajn sześcienny z dodatkowymi ograniczeniami brzegowymi, które „uspokajają” zachowanie na krańcach. Klasycznie wymaga się, aby druga pochodna była równa zero na brzegach: \\[\nf''(a)=0,\\qquad f''(b)=0\n\\]\ndla zakresu \\([a,b]\\) danych. Naturalny splajn nadal można zapisać w bazie:\n\\[\nf(x)=\\beta_0+\\beta_1 x+\\sum_{k=1}^{K}\\theta_k\\, N_k(x),\n\\]\ngdzie \\(N_k(x)\\) są odpowiednimi funkcjami bazowymi wynikającymi z ograniczeń naturalności (w praktyce zwykle budowane w oparciu o B-splajny i narzucone restrykcje liniowe na współczynniki).\nEfekt praktyczny, to stabilniejsze ekstrapolowanie na krańcach i mniejsze ryzyko „dziwnych” zachowań na brzegach.\n\nSplajny wygładzające (Smoothing splines)\n\nTu zamiast wybierać małą liczbę węzłów, dopuszczamy bardzo elastyczną klasę funkcji (węzły często na wszystkich unikalnych \\(x_i\\)), a złożoność kontrolujemy przez karę na krzywiznę:\n\\[\n\\min_{f}\\ \\sum_{i=1}^{n}\\big(y_i-f(x_i)\\big)^2\\;+\\;\\lambda\\int_a^b\\big(f''(t)\\big)^2\\,dt.\n\\]\nKluczowy fakt teoretyczny - rozwiązanie jest splajnem sześciennym z węzłami w punktach \\(x_i\\). Praktyczna reprezentacja ma postać:\n\\[\nf(x)=\\sum_{i=1}^{n}\\alpha_i\\, K(x,x_i)\\;+\\;\\beta_0+\\beta_1 x,\n\\]\ngdzie \\(K\\) jest jądrem odpowiadającym karze \\(\\int (f'')^2\\). W praktyce implementacyjnej często przechodzimy do bazy (np. B-splajnów) i rozwiązujemy problem liniowy z karą.\n\nP-splajny (Penalized B-splines)\n\nTo bardzo popularne w GAM, bo łączą stabilność B-splajnów z łatwą regularizacją. Model:\n\\[\nf(x)=\\sum_{j=1}^{J}\\beta_j\\, B_{j,p}(x),\n\\]\na kara jest nakładana nie na pochodne funkcji, tylko na różnice współczynników (np. druga różnica):\n\\[\n\\min_{\\beta}\\ \\|y - X\\beta\\|^2 + \\lambda \\|D^{(2)}\\beta\\|^2,\n\\]\ngdzie \\(D^{(2)}\\) to macierz operatora drugich różnic (np. \\(\\beta_{j}-2\\beta_{j-1}+\\beta_{j-2}\\)).\n\nThin Plate Splines (TPS) i thin plate regression splines\n\nTo szczególnie ważne funkcje bazowe, gdy chcemy uogólnienia na wiele wymiarów (np. \\(x\\in\\mathbb{R}^d\\)) i nie chcemy ręcznie wybierać węzłów. W wersji 2D klasyczna postać TPS to:\n\\[\nf(x)=\\beta_0+\\beta_1 x_1+\\beta_2 x_2+\\sum_{i=1}^{n}\\alpha_i\\,\\phi(\\|x-x_i\\|),\n\\]\ngdzie funkcja radialna (dla 2D) ma postać:\n\\[\n\\phi(r)=r^2\\log r.\n\\]\nTPS minimalizuje analog kary „energii gięcia” (bending energy), będącej odpowiednikiem \\(\\int (f'')^2\\) w wielu wymiarach. W praktyce GAM często używa się thin plate regression splines – wersji z redukcją wymiaru bazy, aby obliczenia były wykonalne.\n\nSplajny tensorowe (Tensor product splines) – dla interakcji 2D/3D w GAM\n\nGdy chcemy modelować gładką zależność od dwóch zmiennych \\(x_1,x_2\\), często stosujemy bazę tensorową:\n\\[\nf(x_1,x_2)=\\sum_{j=1}^{J_1}\\sum_{\\ell=1}^{J_2}\\beta_{j\\ell}\\, b_j(x_1)\\, c_\\ell(x_2),\n\\]\ngdzie \\(b_j\\) i \\(c_\\ell\\) są bazami jednowymiarowymi (np. B-splajny). To daje kontrolowaną, interpretowalną gładką „powierzchnię” zamiast pojedynczych interakcji parametrycznych.\n\nCardinal splines / splajny Hermite’a (czasem spotykane)\n\nW zastosowaniach sygnałowych lub interpolacyjnych spotyka się splajny, w których parametryzujemy wartości i pochodne w węzłach (Hermite):\nNa przedziale \\([x_k,x_{k+1}]\\):\n\\[\nf(x)=h_{00}(t)\\,y_k+h_{10}(t)\\,m_k+h_{01}(t)\\,y_{k+1}+h_{11}(t)\\,m_{k+1},\n\\quad t=\\frac{x-x_k}{x_{k+1}-x_k},\n\\]\ngdzie \\(y_k=f(x_k), m_k=f'(x_k), a h_{00},h_{10},h_{01},h_{11}\\) to wielomiany Hermite’a. To podejście jest bardziej „interpolacyjne” niż regresyjne, ale warto znać.\n\n\n8.2.2.2 Liczba węzłów a elastyczność i przeuczenie\nLiczba węzłów \\(K\\) steruje złożonością funkcji \\(f\\). Im więcej węzłów, tym więcej stopni swobody i większa elastyczność dopasowania. To jest korzystne, gdy relacja jest rzeczywiście złożona, ale zwiększa ryzyko przeuczenia – model może zacząć dopasowywać przypadkowe fluktuacje.\nMożemy to ująć w języku bias–variance: małe \\(K\\) daje większy bias (zbyt sztywna krzywa), duże \\(K\\) daje większą wariancję (za duża elastyczność). Dlatego w praktyce:\n\nalbo dobieramy \\(K\\) (np. walidacją krzyżową),\nalbo stosujemy splajny wygładzające (smoothing splines), gdzie liczba węzłów może być duża, ale złożoność kontrolujemy przez karę na „szorstkość” funkcji.\n\n\n\n8.2.2.3 Splajny jako element FDA i GAM\nW FDA (Flexible Discriminant Analysis) splajny służą do budowy nieliniowych transformacji predyktorów, a następnie stosuje się ideę dyskryminacji na przekształconej przestrzeni cech. W GAM splajny są naturalnym „budulcem” nieliniowych składowych addytywnych. W obu przypadkach kluczowe jest to, że splajn zachowuje interpretowalność: nadal widzimy, jak zmienna wpływa na wynik, ale w sposób nieliniowy.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Modele addytywne i splajny</span>"
    ]
  },
  {
    "objectID": "chapters/07-splajny-gam.html#generalized-additive-models-gam",
    "href": "chapters/07-splajny-gam.html#generalized-additive-models-gam",
    "title": "8  Modele addytywne i splajny",
    "section": "8.3 Generalized Additive Models (GAM)",
    "text": "8.3 Generalized Additive Models (GAM)\nModel addytywny zakłada, że wpływ predyktorów sumuje się, ale każda składowa może być funkcją nieliniową:\n\\[\ny \\approx \\beta_0 + f_1(x_1) + f_2(x_2) + \\cdots + f_p(x_p).\n\\]\nGdy wszystkie \\(f_j\\) są liniowe, otrzymujemy regresję liniową. GAM pozwala, aby każda \\(f_j\\) była np. splajnem, co umożliwia modelowanie nieliniowości bez utraty interpretowalności: możemy analizować „efekt cząstkowy” każdej zmiennej. GAM jest uogólnieniem GLM. Zamiast zakładać liniowy predyktor w skali \\(y\\), modelujemy liniowy predyktor w skali funkcji łączącej \\(g\\):\n\\[\ng\\big(\\mathbb{E}[Y\\mid X]\\big) = \\beta_0 + \\sum_{j=1}^{p} f_j(x_j).\n\\]\nPrzykłady:\n\nregresja: \\(g(\\mu)=\\mu\\) (tożsamość),\nklasyfikacja binarna - \\(g(\\mu)=\\log\\frac{\\mu}{1-\\mu}\\) (logit),\nzliczenia - \\(g(\\mu)=\\log(\\mu)\\).\n\nW praktyce GAM pozwala budować modele analogiczne do regresji logistycznej, ale z nieliniowymi efektami predyktorów. Każdą funkcję \\(f_j\\) zapisujemy w bazie (np. splajnów):\n\\[\nf_j(x) = \\sum_{\\ell=1}^{K_j} \\beta_{j\\ell}\\, b_{j\\ell}(x).\n\\]\nWtedy model staje się liniowy w parametrach \\(\\beta\\), ale aby uniknąć przeuczenia, stosuje się regularizację (wygładzanie). Typowa estymacja polega na minimalizacji dla regresji (MSE):\n\\[\n\\min_{\\beta}\\ \\sum_{i=1}^{n}\\left(y_i - \\beta_0 - \\sum_{j=1}^{p} f_j(x_{ij})\\right)^2\n\\;+\\; \\sum_{j=1}^{p} \\lambda_j\\, \\beta_j^\\top S_j \\beta_j,\n\\]\ngdzie \\(S_j\\) to macierz kary (związana z „szorstkością” funkcji), a \\(\\lambda_j\\) kontroluje wygładzenie.\n\ndla klasyfikacji/log-loss - analogicznie minimalizujemy ujemną log-wiarygodność + kara.\n\nZuważmy, że w GAM „regularizacja” jest interpretowana jako dobór gładkości funkcji \\(f_j\\), a nie tylko jako „kurczenie współczynników”.\n\n8.3.0.1 Dobór parametrów wygładzania\nDobór \\(\\lambda_j\\) jest kluczowy - zbyt małe wartości prowadzą do przeuczenia (zbyt faliste funkcje), a zbyt duże do niedouczenia (prawie linia prosta). W praktyce stosuje się:\n\nwalidację krzyżową,\nkryteria typu GCV (generalized cross-validation),\nmetody oparte na estymacji wiarygodności (np. REML) – zależnie od implementacji.\n\n\n\n8.3.1 Interpretacja GAM\nNajwiększa zaleta GAM jest interpretowalność - każdą funkcję \\(f_j\\) możemy wykreślić jako krzywą efektu cząstkowego, trzymając pozostałe zmienne „w tle”. Dzięki temu otrzymujemy model elastyczny, ale nadal opisujący jak każda zmienna wpływa na wynik.\n\n\n8.3.2 Zalety i wady GAM\n\nGAM łączy elastyczność i interpretowalność. Umożliwia uchwycenie nieliniowości bez konieczności wchodzenia w modele „czarnej skrzynki”. Jest też często świetnym kompromisem w zastosowaniach naukowych i aplikacyjnych, gdzie potrzebujemy zrozumienia mechanizmu, a nie tylko predykcji.\nGAM jest bardziej złożony obliczeniowo niż regresja liniowa, a wyniki są wrażliwe na dobór parametrów wygładzania i typu bazy. Ponadto standardowy GAM jest addytywny, więc złożone interakcje między zmiennymi nie pojawiają się automatycznie (chyba że jawnie dodamy składniki interakcyjne, np. splajn dwuwymiarowy).\n\n\n\n8.3.3 Przykłady\nPoniżej pokazujemy dwa przykłady: splajny w regresji liniowej przez budowę bazy oraz GAM.\n\nSplajn regresyjny jako cecha w regresji (baza splajnów)\n\nW scikit-learn najprościej użyć SplineTransformer, który tworzy bazę splajnów, a następnie dopasować model liniowy.\n\n\nKod\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import SplineTransformer\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score\n\n# Przykład: dane syntetyczne 1D\nrng = np.random.default_rng(42)\nn = 400\nx = rng.uniform(0, 10, size=n)\ny = np.sin(x) + 0.2*rng.normal(size=n)\n\nX = x.reshape(-1, 1)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n\nmodel = Pipeline(steps=[\n    (\"spl\", SplineTransformer(degree=3, n_knots=8, include_bias=False)),\n    (\"lin\", LinearRegression())\n])\n\nmodel.fit(X_train, y_train)\npred = model.predict(X_test)\n\nprint(\"R2:\", r2_score(y_test, pred))\n\n# Rysunek: rzeczywista funkcja, dane, i dopasowany splajn\nimport matplotlib.pyplot as plt\n\n# Siatka do gładkiego rysowania\nx_plot = np.linspace(0, 10, 500).reshape(-1, 1)\ny_true_plot = np.sin(x_plot.ravel())\ny_spline_plot = model.predict(x_plot)\n\nfig, ax = plt.subplots(figsize=(12, 6), constrained_layout=True)\n\n# Rzeczywista funkcja\nax.plot(x_plot, y_true_plot, \"b--\", linewidth=2.5, alpha=0.8, label=\"Rzeczywista f(x) = sin(x)\")\n\n# Aproksymacja splajnami\nax.plot(x_plot, y_spline_plot, \"r-\", linewidth=2.5, label=\"Aproksymacja splajnami\")\n\n# Dane treningowe i testowe\nax.scatter(X_train, y_train, s=40, alpha=0.6, color=\"green\", label=\"Dane treningowe\", edgecolors=\"darkgreen\")\nax.scatter(X_test, y_test, s=40, alpha=0.6, color=\"orange\", label=\"Dane testowe\", edgecolors=\"darkorange\")\n\nax.set_xlabel(\"x\", fontsize=12)\nax.set_ylabel(\"y\", fontsize=12)\nax.set_title(\"Splajn sześcienny: rzeczywista funkcja vs aproksymacja\", fontsize=13, fontweight=\"bold\")\nax.legend(fontsize=11, loc=\"upper right\")\nax.grid(alpha=0.3)\n\nplt.show()\n\n\nR2: 0.9087760776971322\n\n\n\n\n\n\n\n\n\nW tym przykładzie splajn działa jak „nieliniowa transformacja cechy”, a regresja liniowa dopasowuje współczynniki do bazy.\n\nGAM dla klasyfikacji lub regresji (pyGAM)\n\nGAM oferują automatyczne dobieranie parametrów wygładzania i możliwość rysowania efektów cząstkowych. W tym przykładzie budujemy GAM z dwoma zmiennymi objaśniającymi, każda reprezentowana przez splajn. Model automatycznie dobiera stopień wygładzenia dla każdej zmiennej, a my możemy interpretować jej wpływ na wynik niezależnie od pozostałych zmiennych.\nFunkcja rzeczywista (którą będziemy estymować):\n\\[\ny = \\sin(6x_1) + 0.5 x_2^2 + \\epsilon,\n\\]\ngdzie \\(x_1, x_2 \\in [0,1]\\) są predyktorami, a \\(\\epsilon\\) to szum gaussowski. Funkcja pokazuje dwie bardzo różne zależności: pierwsza jest sinusoidalna, druga jest paraboliczna.\n\n\nKod\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom pygam import LinearGAM, s\nfrom sklearn.metrics import r2_score\n\n# Przykład: regresja z dwoma zmiennymi\nrng = np.random.default_rng(42)\nX = rng.random((300, 2))\n\n# Rzeczywista funkcja generująca dane\ndef true_function(X):\n    return np.sin(6*X[:, 0]) + 0.5*(X[:, 1]**2)\n\ny_true_values = true_function(X)\ny = y_true_values + 0.1*rng.normal(size=300)  # dodanie szumu\n\n# Dopasowanie GAM: s(0) + s(1) oznacza splajn dla każdej zmiennej\ngam = LinearGAM(s(0) + s(1)).fit(X, y)\n\n# Predykcja\ny_pred = gam.predict(X)\n\n# Ocena modelu\nr2 = r2_score(y, y_pred)\nprint(f\"R² dla GAM: {r2:.4f}\")\n\n# Rysunek: rzeczywista funkcja vs dopasowanie GAM\nfig, axes = plt.subplots(1, 2, figsize=(15, 5), constrained_layout=True)\n\n# ========== Lewy panel: Pierwsza zmienna ==========\n# Siatka dla pierwszej zmiennej (druga zmienna na medianie)\nx1_grid = np.linspace(0, 1, 200)\nx2_median = np.median(X[:, 1])\nX_grid1 = np.column_stack([x1_grid, np.repeat(x2_median, len(x1_grid))])\n\ny_true_grid1 = true_function(X_grid1)\ny_pred_grid1 = gam.predict(X_grid1)\n\n# Obserwacje dla pierwszej zmiennej (grupujemy po drugim wymiarze)\nx1_obs = X[:, 0]\naxes[0].scatter(x1_obs, y, s=30, alpha=0.4, color=\"gray\", label=\"Obserwacje\")\naxes[0].plot(x1_grid, y_true_grid1, \"b--\", linewidth=2.5, alpha=0.8, label=\"Rzeczywista f(x₁)\")\naxes[0].plot(x1_grid, y_pred_grid1, \"r-\", linewidth=2.5, label=\"GAM dopasowanie\")\n\naxes[0].set_xlabel(\"x₁\", fontsize=12)\naxes[0].set_ylabel(\"y\", fontsize=12)\naxes[0].set_title(f\"Efekt pierwszej zmiennej (x₂ = {x2_median:.2f})\", fontsize=12, fontweight=\"bold\")\naxes[0].legend(fontsize=10)\naxes[0].grid(alpha=0.3)\n\n# ========== Prawy panel: Druga zmienna ==========\n# Siatka dla drugiej zmiennej (pierwsza zmienna na medianie)\nx2_grid = np.linspace(0, 1, 200)\nx1_median = np.median(X[:, 0])\nX_grid2 = np.column_stack([np.repeat(x1_median, len(x2_grid)), x2_grid])\n\ny_true_grid2 = true_function(X_grid2)\ny_pred_grid2 = gam.predict(X_grid2)\n\n# Obserwacje dla drugiej zmiennej\nx2_obs = X[:, 1]\naxes[1].scatter(x2_obs, y, s=30, alpha=0.4, color=\"gray\", label=\"Obserwacje\")\naxes[1].plot(x2_grid, y_true_grid2, \"b--\", linewidth=2.5, alpha=0.8, label=\"Rzeczywista f(x₂)\")\naxes[1].plot(x2_grid, y_pred_grid2, \"r-\", linewidth=2.5, label=\"GAM dopasowanie\")\n\naxes[1].set_xlabel(\"x₂\", fontsize=12)\naxes[1].set_ylabel(\"y\", fontsize=12)\naxes[1].set_title(f\"Efekt drugiej zmiennej (x₁ = {x1_median:.2f})\", fontsize=12, fontweight=\"bold\")\naxes[1].legend(fontsize=10)\naxes[1].grid(alpha=0.3)\n\nplt.show()\n\n# Wypisanie efektów cząstkowych\nprint(\"\\nPodsumowanie GAM:\")\nprint(f\"Parametry wygładzania (lambda): {gam.lam}\")\nprint(f\"Stopnie swobody: {gam.statistics_['edof']}\")\n\n# ========== Wizualizacja 3D: rzeczywista funkcja vs GAM ==========\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\n# Siatka dla wizualizacji 3D\nx1_3d = np.linspace(0, 1, 50)\nx2_3d = np.linspace(0, 1, 50)\nX1_grid, X2_grid = np.meshgrid(x1_3d, x2_3d)\n\n# Rzeczywista funkcja\nZ_true = np.sin(6*X1_grid) + 0.5*(X2_grid**2)\n\n# Predykcja GAM\nX_grid_flat = np.column_stack([X1_grid.ravel(), X2_grid.ravel()])\nZ_pred = gam.predict(X_grid_flat).reshape(X1_grid.shape)\n\n# Podział danych na train/test dla wizualizacji\nfrom sklearn.model_selection import train_test_split\nX_train_vis, X_test_vis, y_train_vis, y_test_vis = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Tworzenie subplotów 3D\nfig = make_subplots(\n    rows=2, cols=1,\n    specs=[[{'type': 'surface'}], [{'type': 'surface'}]],\n    subplot_titles=(\"Rzeczywista funkcja\", \"Estymacja GAM\"),\n    vertical_spacing=0.05\n)\n\n# Górny panel: rzeczywista funkcja\nfig.add_trace(\n    go.Surface(\n        x=X1_grid,\n        y=X2_grid,\n        z=Z_true,\n        colorscale='Viridis',\n        name='Rzeczywista',\n        showscale=False\n    ),\n    row=1, col=1\n)\n\n# Dodanie punktów treningowych (górny panel)\nfig.add_trace(\n    go.Scatter3d(\n        x=X_train_vis[:, 0],\n        y=X_train_vis[:, 1],\n        z=y_train_vis,\n        mode='markers',\n        marker=dict(size=4, color='green', opacity=0.6),\n        name='Train',\n        showlegend=True\n    ),\n    row=1, col=1\n)\n\n# Dodanie punktów testowych (górny panel)\nfig.add_trace(\n    go.Scatter3d(\n        x=X_test_vis[:, 0],\n        y=X_test_vis[:, 1],\n        z=y_test_vis,\n        mode='markers',\n        marker=dict(size=4, color='orange', opacity=0.6),\n        name='Test',\n        showlegend=True\n    ),\n    row=1, col=1\n)\n\n# Dolny panel: estymacja GAM\nfig.add_trace(\n    go.Surface(\n        x=X1_grid,\n        y=X2_grid,\n        z=Z_pred,\n        colorscale='Viridis',\n        name='GAM',\n        showscale=True\n    ),\n    row=2, col=1\n)\n\n# Dodanie punktów treningowych (dolny panel)\nfig.add_trace(\n    go.Scatter3d(\n        x=X_train_vis[:, 0],\n        y=X_train_vis[:, 1],\n        z=y_train_vis,\n        mode='markers',\n        marker=dict(size=4, color='green', opacity=0.6),\n        name='Train',\n        showlegend=False\n    ),\n    row=2, col=1\n)\n\n# Dodanie punktów testowych (dolny panel)\nfig.add_trace(\n    go.Scatter3d(\n        x=X_test_vis[:, 0],\n        y=X_test_vis[:, 1],\n        z=y_test_vis,\n        mode='markers',\n        marker=dict(size=4, color='orange', opacity=0.6),\n        name='Test',\n        showlegend=False\n    ),\n    row=2, col=1\n)\n\n# Aktualizacja osi\nfig.update_xaxes(title_text=\"x₁\", row=1, col=1)\nfig.update_yaxes(title_text=\"x₂\", row=1, col=1)\nfig.update_xaxes(title_text=\"x₁\", row=2, col=1)\nfig.update_yaxes(title_text=\"x₂\", row=2, col=1)\n\nfig.update_layout(\n    title_text=\"Porównanie rzeczywistej funkcji i estymacji GAM (widok 3D)\",\n    width=1300,\n    height=700\n)\n\nfig.show()\n\n\nR² dla GAM: 0.9837\n\n\n\n\n\n\n\n\n\n\nPodsumowanie GAM:\nParametry wygładzania (lambda): [[0.6], [0.6]]\nStopnie swobody: 22.76497543059306\n\n\n                            \n                                            \n\n\npyGAM automatycznie dobiera parametry wygładzania (zwykle przez CV/GCV). Największą zaletą GAM jest możliwość wizualizacji efektów cząstkowych każdej zmiennej - widzimy dokładnie, w jaki sposób każdy predyktor wpływa na przewidywaną wartość, trzymając pozostałe zmienne „w tle”. Wizualizacja 3D (powyżej) pokazuje jak dobrze model GAM aproksymuje rzeczywistą powierzchnię, uchwycając zarówno oscylacyjny jak i paraboliczny charakter funkcji. W praktyce GAM stanowi świetny kompromis między prostotą interpretacji a elastycznością modelowania nieliniowości.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Modele addytywne i splajny</span>"
    ]
  },
  {
    "objectID": "chapters/07-splajny-gam.html#podsumowanie",
    "href": "chapters/07-splajny-gam.html#podsumowanie",
    "title": "8  Modele addytywne i splajny",
    "section": "8.4 Podsumowanie",
    "text": "8.4 Podsumowanie\nSplajny pozwalają modelować nieliniowości w sposób kontrolowany: są wielomianami kawałkowymi, sklejanymi gładko w węzłach, a ich elastyczność zależy od liczby węzłów lub parametrów wygładzania. GAM wykorzystuje splajny jako składowe addytywne, tworząc model interpretowalny, który nie zakłada ścisłej liniowości zależności. W praktyce splajny i GAM stanowią bardzo ważny etap pośredni między prostymi modelami liniowymi a metodami zespołowymi czy głębokimi sieciami: zachowują zrozumiałość, ale potrafią uchwycić znaczną część złożoności danych.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Modele addytywne i splajny</span>"
    ]
  },
  {
    "objectID": "chapters/08-svm.html",
    "href": "chapters/08-svm.html",
    "title": "9  Support Vector Machines",
    "section": "",
    "text": "9.1 Wprowadzenie\nSupport Vector Machines (SVM) to rodzina metod uczenia nadzorowanego opartych na geometrii przestrzeni cech i optymalizacji wypukłej. Historycznie SVM wyrosły z prac nad maksymalnym marginesem w klasyfikacji liniowej na początku lat 90. (m.in. idea maximum margin classifier), a następnie zostały uogólnione przez „trik jądrowy” (kernel trick) na sytuacje nieliniowo separowalne. Z perspektywy klasyfikacji SVM należą do metod dyskryminacyjnych: nie modelują rozkładów \\(\\mathbb{P}(x\\mid y)\\), lecz konstruują granicę decyzyjną w przestrzeni cech tak, aby jak najlepiej rozdzielała klasy przy kontrolowanej złożoności.\nW praktyce SVM spotykamy w dwóch podstawowych wariantach: (i) klasyfikacja SVC, gdzie uczymy hiperpłaszczyznę maksymalizującą margines, oraz (ii) regresja SVR, gdzie uczymy funkcję o maksymalnej „płaskości” w sensie normy \\(\\|w\\|\\) przy tolerancji błędu typu \\(\\varepsilon\\)-insensitive. W obu przypadkach kluczowe pojęcia to: hiperpłaszczyzna, margines, wektory nośne oraz jądra.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Support Vector Machines</span>"
    ]
  },
  {
    "objectID": "chapters/08-svm.html#klasyfikacja-liniowo-separowalna-twardy-margines",
    "href": "chapters/08-svm.html#klasyfikacja-liniowo-separowalna-twardy-margines",
    "title": "9  Support Vector Machines",
    "section": "9.2 Klasyfikacja liniowo separowalna: twardy margines",
    "text": "9.2 Klasyfikacja liniowo separowalna: twardy margines\nRozważamy dane uczące \\(\\{(x_i,y_i)\\}_{i=1}^n\\), gdzie \\(x_i\\in\\mathbb{R}^p\\) i \\(y_i\\in\\{-1,+1\\}\\). Hiperpłaszczyzna ma postać \\(w^\\top x + b = 0\\). SVM wybiera taką hiperpłaszczyznę, która maksymalizuje margines, tj. minimalizuje \\(\\|w\\|\\) przy warunkach poprawnej klasyfikacji z zapasem:\n\\[\n\\min_{w,b}\\ \\frac{1}{2}\\|w\\|^2\\quad\\text{s.t.}\\quad y_i(w^\\top x_i + b)\\ge 1,\\ i=1,\\dots,n.\n\\]\nWarunki \\(y_i(w^\\top x_i + b)\\ge 1\\) oznaczają, że obserwacje leżą po właściwej stronie dwóch hiperpłaszczyzn \\(w^\\top x + b = \\pm 1\\), które wyznaczają pas marginesu. Szerokość marginesu wynosi \\(2/\\|w\\|\\), więc minimalizacja \\(\\|w\\|\\) jest równoważna maksymalizacji marginesu. Obserwacje, które leżą dokładnie na brzegach marginesu, nazywamy wektorami nośnymi (support vectors) – to one determinują położenie rozwiązania.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Support Vector Machines</span>"
    ]
  },
  {
    "objectID": "chapters/08-svm.html#miękki-margines-dopuszczenie-naruszeń-i-rola-parametru-c",
    "href": "chapters/08-svm.html#miękki-margines-dopuszczenie-naruszeń-i-rola-parametru-c",
    "title": "9  Support Vector Machines",
    "section": "9.3 Miękki margines: dopuszczenie naruszeń i rola parametru \\(C\\)",
    "text": "9.3 Miękki margines: dopuszczenie naruszeń i rola parametru \\(C\\)\nW danych rzeczywistych idealna separacja jest rzadkością. Wprowadzamy więc zmienne luzu \\(\\xi_i\\ge 0\\), które pozwalają naruszać margines i (w skrajnym przypadku) popełniać błędy klasyfikacji:\n\\[\n\\min_{w,b,\\xi}\\ \\frac{1}{2}\\|w\\|^2 + C\\sum_{i=1}^n\\xi_i\\quad\\text{s.t.}\\quad y_i(w^\\top x_i + b)\\ge 1-\\xi_i,\\ \\xi_i\\ge 0.\n\\]\nParametr \\(C&gt;0\\) reguluje kompromis między szerokością marginesu a karą za naruszenia: duże \\(C\\) prowadzi do silniejszego „ścigania” błędów treningowych, a małe \\(C\\) do większej regularizacji i szerszego marginesu kosztem większej tolerancji błędów.\n\n\nKod\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.svm import SVC\n\n# Generujemy dane z częściowym zachodzeniem klas\nnp.random.seed(42)\nX1 = np.random.randn(40, 2) + np.array([2, 2])\nX2 = np.random.randn(40, 2) + np.array([0, 0])\n\n# Dodajemy kilka punktów \"trudnych\" w obszarze zachodzenia\nX1 = np.vstack([X1, np.array([[0.5, 0.5], [0.3, 0.8]])])\nX2 = np.vstack([X2, np.array([[1.8, 1.5], [1.5, 2.0]])])\n\nX = np.vstack([X1, X2])\ny = np.hstack([np.ones(len(X1)), -np.ones(len(X2))])\n\n# Funkcja rysująca granicę decyzyjną i margines\ndef plot_svm_decision_boundary(ax, clf, X, y, title):\n    # Siatka\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),\n                         np.linspace(y_min, y_max, 200))\n    \n    # Funkcja decyzyjna\n    Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n    \n    # Granica decyzyjna i margines\n    ax.contour(xx, yy, Z, levels=[-1, 0, 1], linewidths=[1.5, 2.5, 1.5],\n               linestyles=['dashed', 'solid', 'dashed'], colors=['gray', 'black', 'gray'])\n    \n    # Punkty\n    ax.scatter(X[y == 1, 0], X[y == 1, 1], c='royalblue', s=50, \n               edgecolors='k', alpha=0.7, label='Klasa +1')\n    ax.scatter(X[y == -1, 0], X[y == -1, 1], c='coral', s=50, \n               edgecolors='k', alpha=0.7, label='Klasa -1')\n    \n    # Wektory nośne\n    ax.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1],\n               s=200, linewidth=2, facecolors='none', edgecolors='green',\n               label=f'Wektory nośne ({len(clf.support_vectors_)})')\n    \n    ax.set_xlim(x_min, x_max)\n    ax.set_ylim(y_min, y_max)\n    ax.set_xlabel('$x_1$')\n    ax.set_ylabel('$x_2$')\n    ax.set_title(title)\n    ax.legend(loc='upper left', fontsize=8)\n    ax.grid(alpha=0.3)\n\n# Trenujemy SVM z różnymi wartościami C\nfig, axes = plt.subplots(1, 3, figsize=(14, 4))\n\nC_values = [0.1, 1, 100]\nfor ax, C in zip(axes, C_values):\n    clf = SVC(kernel='linear', C=C)\n    clf.fit(X, y)\n    plot_svm_decision_boundary(ax, clf, X, y, f'C = {C}')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nPrzy małym \\(C=0.1\\) margines jest szerszy (większa odległość między przerywaną linią a linią ciągłą), ale model toleruje więcej naruszeń – wiele punktów znajduje się wewnątrz marginesu lub po złej stronie. Przy dużym \\(C=100\\) margines jest węższy, a granica decyzyjna stara się uniknąć błędów kosztem mniejszej regularyzacji, co może prowadzić do przeuczenia.\nWektory nośne (miękki margines):\n\n\\(\\xi_i = 0\\) i \\(y_i(w^\\top x_i + b)=1\\) — punkt leży na brzegu marginesu i jest wektorem nośnym.\n\\(0 &lt; \\xi_i &lt; 1\\) — punkt leży wewnątrz marginesu po właściwej stronie granicy; nadal jest wektorem nośnym.\n\\(\\xi_i \\ge 1\\) — punkt jest błędnie sklasyfikowany; także jest wektorem nośnym. Tylko wektory nośne mają niezerowe współczynniki \\(\\alpha_i\\) w dualnym rozwiązaniu i wyznaczają położenie granicy decyzyjnej.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Support Vector Machines</span>"
    ]
  },
  {
    "objectID": "chapters/08-svm.html#nieliniowa-separacja-i-jądra",
    "href": "chapters/08-svm.html#nieliniowa-separacja-i-jądra",
    "title": "9  Support Vector Machines",
    "section": "9.4 Nieliniowa separacja i jądra",
    "text": "9.4 Nieliniowa separacja i jądra\nJeżeli dane nie są dobrze separowalne liniowo w \\(\\mathbb{R}^p\\), stosujemy odwzorowanie \\(\\varphi(x)\\) do przestrzeni cech \\(\\mathcal{H}\\), w której separacja staje się liniowa. W praktyce nie konstruujemy jawnie \\(\\varphi\\), lecz korzystamy z jądra \\(K\\), które realizuje iloczyn skalarny w \\(\\mathcal{H}\\):\n\\[\nK(x,z)=\\langle\\varphi(x),\\varphi(z)\\rangle.\n\\]\nW postaci dualnej funkcja decyzyjna przyjmuje postać:\n\\[\nf(x)=\\sum_{i=1}^n \\alpha_i y_i K(x_i,x) + b,\\qquad \\hat y(x)=\\mathrm{sign}(f(x)),\n\\]\ngdzie niezerowe \\(\\alpha_i\\) odpowiadają wektorom nośnym.\n\n9.4.1 Popularne jądra i ich zastosowania\n1. Jądro liniowe: \\(K(x,z) = x^\\top z\\)\n\nNajprostsze; równoważne SVM bez kernela\nKiedy stosować: dane liniowo lub prawie liniowo separowalne, wysokowymiarowe dane rzadkie (tekst, genomika), gdy chcemy interpretowalności lub szybkiego treningu/predykcji\nZalety: bardzo szybkie, skalowalne, niewielkie ryzyko przeuczenia\nWady: ograniczona elastyczność dla złożonych granic nieliniowych\n\n2. Jądro wielomianowe: \\(K(x,z) = (\\gamma\\, x^\\top z + r)^d\\)\n\nParametry: \\(d\\) (stopień), \\(\\gamma\\) (skalowanie), \\(r\\) (stała, często 0)\nKiedy stosować: interakcje między cechami mają znaczenie (np. pixel × pixel w obrazach), znamy z dziedziny, że zależności są wielomianowe, dane niskodymiarowe z wyraźnymi nieliniowościami niskiego stopnia\nZalety: jawnie modeluje interakcje do stopnia \\(d\\)\nWady: droższe obliczeniowo niż liniowe, wrażliwe na dobór \\(d\\) i \\(\\gamma\\), ryzyko przeuczenia przy dużych \\(d\\)\n\n3. Jądro RBF (Gaussa): \\(K(x,z) = \\exp(-\\gamma\\|x-z\\|^2)\\)\n\nParametr \\(\\gamma&gt;0\\) kontroluje „promień wpływu” każdego punktu\nKiedy stosować: uniwersalne jądro domyślne, gdy nie znamy struktury danych, granice decyzyjne mają złożone, lokalne kształty, dane nisko- i średniowymiarowe z nieliniowymi zależnościami\nZalety: bardzo elastyczne, dobrze działa w praktyce, może aproksymować dowolną ciągłą funkcję (uniwersalne)\nWady: duże \\(\\gamma\\) → przeuczenie (zbyt lokalne granice), wymaga standaryzacji, wolniejsze niż liniowe, trudniej interpretować\n\n4. Jądro sigmoidalne: \\(K(x,z) = \\tanh(\\gamma\\, x^\\top z + r)\\)\n\nInspirowane sieciami neuronowymi (funkcja aktywacji)\nKiedy stosować: rzadko w praktyce; czasem w problemach podobnych do sieci neuronowych, gdy chcemy zachowania „S-kształtnego”\nWady: nie zawsze dodatnio określone (wymóg jądra Mercera); może być niestabilne\n\n5. Jądro Laplace’a: \\(K(x,z) = \\exp(-\\gamma\\|x-z\\|_1)\\)\n\nWariant RBF z normą \\(L^1\\) zamiast \\(L^2\\)\nKiedy stosować: gdy cechy mają duże odstające wartości lub preferujemy robustność normy \\(L^1\\), podobne zastosowania jak RBF\nRzadziej spotykane niż RBF\n\n6. Jądra stringowe (dla tekstów/sekwencji):\n\nPrzykłady: spectrum kernel, subsequence kernel, mismatch kernel\nKiedy stosować: klasyfikacja tekstów, sekwencji DNA/białek, analiza dokumentów bez jawnej wektoryzacji\nMierzą podobieństwo na podstawie wspólnych podsłów, n-gramów lub motywów\n\n7. Jądro chi-kwadrat: \\(K(x,z) = \\exp\\bigg(-\\gamma\\sum_i \\frac{(x_i-z_i)^2}{x_i+z_i}\\bigg)\\)\n\nKiedy stosować: histogramy (np. w computer vision: SIFT, HOG), dane nieujemne o charakterze zliczeniowym\nSzczególnie popularne w rozpoznawaniu obrazów\n\nWybór jądra w praktyce:\n\nStart: spróbuj liniowego (jeśli \\(p\\) duże lub dane rzadkie) lub RBF (jeśli \\(p\\) małe/średnie)\nGrid search: dobieraj \\(C\\) i \\(\\gamma\\) (lub \\(d\\)) przez walidację krzyżową\nStandaryzacja: zawsze dla RBF i wielomianowego\nDziedzina: jeśli masz wiedzę problemową (tekst → stringowe, histogramy → chi-kwadrat), użyj specjalistycznego jądra\n\nParametr \\(\\gamma\\) w RBF kontroluje lokalność granicy decyzyjnej: duże \\(\\gamma\\) daje granice bardziej „lokalne” (większe ryzyko przeuczenia), a małe \\(\\gamma\\) bardziej „globalne” i gładkie.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Support Vector Machines</span>"
    ]
  },
  {
    "objectID": "chapters/08-svm.html#estymacja-parametrów-w-svm-margines-twardy-i-miękki",
    "href": "chapters/08-svm.html#estymacja-parametrów-w-svm-margines-twardy-i-miękki",
    "title": "9  Support Vector Machines",
    "section": "9.5 Estymacja parametrów w SVM: margines twardy i miękki",
    "text": "9.5 Estymacja parametrów w SVM: margines twardy i miękki\nEstymacja parametrów SVM jest problemem optymalizacji wypukłej. W wersji liniowej parametry modelu to \\((w,b)\\), natomiast w ujęciu dualnym (które jest kluczowe zarówno obliczeniowo, jak i dla jąder) parametrami roboczymi stają się mnożniki Lagrange’a \\(\\alpha_1,\\dots,\\alpha_n\\) oraz \\(b\\). Przejście do postaci dualnej nie jest tylko „trikiem” formalnym: ono wyjaśnia, dlaczego rozwiązanie zależy wyłącznie od części obserwacji (wektorów nośnych) oraz dlaczego kernel SVM można zrealizować bez jawnego konstruowania \\(\\varphi(x)\\).\n\n9.5.1 Margines twardy: od postaci pierwotnej do dualnej\nW hard-margin SVM rozwiązujemy zadanie z ograniczeniami nierównościowymi. Aby przejść do dualu, konstruujemy Lagrangian przez dołączenie ograniczeń z mnożnikami \\(\\alpha_i\\ge 0\\). Otrzymujemy funkcję:\n\\[\n\\mathcal{L}(w,b,\\alpha)=\\frac{1}{2}\\|w\\|^2-\\sum_{i=1}^n \\alpha_i\\big(y_i(w^\\top x_i+b)-1\\big).\n\\]\nNastępnie narzucamy warunki stacjonarności (pochodne po \\(w\\) i \\(b\\) równe zero), które są kluczowe, bo pozwalają wyeliminować \\(w\\) i \\(b\\) z problemu. Z pochodnej po w dostajemy:\n\\[\n\\frac{\\partial \\mathcal{L}}{\\partial w}=0 \\quad\\Rightarrow\\quad\nw=\\sum_{i=1}^n \\alpha_i y_i x_i,\n\\]\na z pochodnej po \\(b\\):\n\\[\n\\frac{\\partial \\mathcal{L}}{\\partial b}=0 \\quad\\Rightarrow\\quad\n\\sum_{i=1}^n \\alpha_i y_i=0.\n\\]\nPo podstawieniu tych zależności do Lagrangianu otrzymujemy problem dualny w zmiennych \\(\\alpha\\):\n\\[\n\\max_{\\alpha}\\ \\sum_{i=1}^n \\alpha_i-\\frac{1}{2}\\sum_{i=1}^n\\sum_{j=1}^n \\alpha_i\\alpha_j y_i y_j\\, x_i^\\top x_j,\n\\quad\n\\text{s.t. } \\alpha_i\\ge 0,\\ \\sum_{i=1}^n \\alpha_i y_i=0.\n\\]\nJest to programowanie kwadratowe (QP) z liniowymi ograniczeniami. Wypukłość zapewnia globalne optimum. Sens praktyczny jest następujący: zamiast szukać \\(w\\) wprost, rozwiązujemy QP dla \\(\\alpha\\), a dopiero potem rekonstruujemy \\(w\\).\nCentralną rolę odgrywają warunki KKT (Karush–Kuhn–Tucker), zwłaszcza warunek komplementarności:\n\\[\n\\alpha_i\\big(y_i(w^\\top x_i+b)-1\\big)=0.\n\\]\nOznacza on, że jeśli \\(\\alpha_i&gt;0\\), to musi zachodzić równość \\(y_i(w^\\top x_i+b)=1\\), czyli punkt leży dokładnie na brzegu marginesu. Innymi słowy, niezerowe \\(\\alpha_i\\) odpowiadają wektorom nośnym i tylko te punkty „trzymają” rozwiązanie. Punkty z \\(\\alpha_i=0\\) są w sensie optymalizacji „nieaktywne” i nie wpływają na położenie hiperpłaszczyzny.\nPo wyznaczeniu \\(\\alpha\\) obliczamy \\(\\hat w\\) ze wzoru \\(\\hat w=\\sum_i \\alpha_i y_i x_i\\). Bias \\(\\hat b\\) odzyskujemy korzystając z dowolnego wektora nośnego \\(x_s\\) (w praktyce uśredniając po wielu SV): dla hard-margin mamy równanie \\(y_s(\\hat w^\\top x_s+\\hat b)=1,\\) więc\n\\[\n\\hat b = y_s-\\hat w^\\top x_s.\n\\]\n\n\n9.5.2 Margines miękki\nW soft-margin SVM wprowadzamy zmienne luzu \\(\\xi_i\\ge 0\\), a w funkcji celu pojawia się składnik \\(C\\sum_i \\xi_i\\). Formalnie w problemie dualnym prowadzi to do tego, że mnożniki \\(\\alpha_i\\) nie tylko są nieujemne, ale także mają górne ograniczenie:\n\\[\n0\\le \\alpha_i \\le C.\n\\]\nDual dla soft-margin ma więc tę samą funkcję celu co w hard-margin, ale inne ograniczenia:\n\\[\n\\max_{\\alpha}\\ \\sum_{i=1}^n \\alpha_i-\\frac{1}{2}\\sum_{i=1}^n\\sum_{j=1}^n \\alpha_i\\alpha_j y_i y_j\\, x_i^\\top x_j,\n\\quad\n\\text{s.t. } 0\\le \\alpha_i\\le C,\\ \\sum_{i=1}^n \\alpha_i y_i=0.\n\\]\nTen pozornie „mały” detal ma bardzo ważną interpretację. Warunki KKT pozwalają rozróżnić trzy typy obserwacji. Jeśli \\(\\alpha_i=0\\), obserwacja jest na tyle „łatwa”, że nie wpływa na rozwiązanie. Jeśli \\(0&lt;\\alpha_i&lt;C\\), obserwacja jest klasycznym wektorem nośnym leżącym na brzegu marginesu (wiąże równość \\(y_i(w^\\top x_i+b)=1\\)). Jeśli natomiast \\(\\alpha_i=C\\), obserwacja jest w sensie KKT „nasycona” karą: typowo odpowiada to punktom naruszającym margines (a czasem błędnie sklasyfikowanym). Z tego powodu w soft-margin najstabilniej wyznacza się \\(\\hat b\\) korzystając z tych wektorów nośnych, dla których \\(0&lt;\\alpha_i&lt;C\\), ponieważ wtedy zachodzi dokładna równość brzegowa i unikamy przypadków naruszeń.\nWarto też zauważyć równoważność soft-margin z minimalizacją straty hinge loss w ujęciu „bez \\(\\xi\\)”. Zmienne luzu można interpretować jako miarę naruszenia:\n\\[\n\\xi_i=\\max\\{0,\\ 1-y_i(w^\\top x_i+b)\\},\n\\]\nco prowadzi do równoważnego zapisu:\n\\[\n\\min_{w,b}\\ \\frac{1}{2}\\|w\\|^2 + C\\sum_{i=1}^n \\max\\{0,\\ 1-y_i(w^\\top x_i+b)\\}.\n\\]\nTo jest ważne z perspektywy uczenia maszynowego: pokazuje, że soft-margin SVM jest klasycznym przypadkiem uczenia z wypukłą funkcją straty i regularizacją, a parametry \\(C\\) sterują intensywnością kary za naruszenia.\n\n\n9.5.3 Kernelowa wersja a estymacja parametrów\nPrzejście do kernela nie zmienia mechaniki estymacji — zmienia jedynie sposób, w jaki w problemie dualnym pojawiają się dane. W problemie dualnym występują iloczyny skalarne \\(x_i^\\top x_j\\), które zastępujemy \\(K(x_i,x_j)\\). Soft-margin dual przyjmuje postać:\n\\[\n\\max_{\\alpha}\\ \\sum_{i=1}^n \\alpha_i-\\frac{1}{2}\\sum_{i=1}^n\\sum_{j=1}^n \\alpha_i\\alpha_j y_i y_j\\, K(x_i,x_j),\n\\quad\n\\text{s.t. } 0\\le \\alpha_i\\le C,\\ \\sum_{i=1}^n \\alpha_i y_i=0.\n\\]\nPo rozwiązaniu problemu dostajemy funkcję decyzyjną:\n\\[\nf(x)=\\sum_{i=1}^n \\alpha_i y_i K(x_i,x)+b,\n\\]\ngdzie niezerowe \\(\\alpha_i\\) odpowiadają wektorom nośnym. W wersji liniowej można rekonstruować \\(w\\), natomiast w kernelowej zwykle tego nie robimy — model jest reprezentowany przez \\(\\alpha\\), wybrane wektory nośne i \\(b\\).\nChociaż problem dualny ma postać QP, w praktyce nie rozwiązuje się go ogólnymi solverami programowania kwadratowego, ponieważ byłoby to zbyt kosztowne dla większych \\(n\\). Klasyczne implementacje SVM (rodzina LIBSVM, używana m.in. w wielu bibliotekach) stosują SMO (Sequential Minimal Optimization). SMO rozbija problem na serię bardzo małych podproblemów (zwykle dla pary zmiennych \\(\\alpha\\)), które można rozwiązać analitycznie, a następnie iteracyjnie aktualizuje rozwiązanie aż do spełnienia warunków optymalności KKT. W tym sensie „estymacja parametrów SVM” w praktyce oznacza: wyznaczenie współczynników dualnych \\(\\alpha\\) metodą SMO (lub jej wariantami), obliczenie \\(b\\) na podstawie wektorów nośnych oraz — w przypadku wersji liniowej — rekonstrukcję \\(w\\).",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Support Vector Machines</span>"
    ]
  },
  {
    "objectID": "chapters/08-svm.html#wariant-wieloklasowy",
    "href": "chapters/08-svm.html#wariant-wieloklasowy",
    "title": "9  Support Vector Machines",
    "section": "9.6 Wariant wieloklasowy",
    "text": "9.6 Wariant wieloklasowy\nSVM jest z natury klasyfikatorem binarnym. Dla problemów wieloklasowych stosujemy konstrukcje redukcyjne. Najczęściej spotykamy:\n\none-vs-rest (OvR) – uczymy \\(K\\) klasyfikatorów binarnych „klasa \\(k\\) vs reszta” i wybieramy klasę o największej wartości funkcji decyzyjnej;\none-vs-one (OvO) – uczymy \\(K(K-1)/2\\) klasyfikatorów dla każdej pary klas i stosujemy głosowanie.\n\nW praktyce implementacje (np. scikit-learn) domyślnie stosują OvO dla SVC, a OvR jest często spotykany w wersjach liniowych (np. LinearSVC). Wybór strategii wpływa na koszt obliczeń i zachowanie w przypadku niezbalansowanych klas.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Support Vector Machines</span>"
    ]
  },
  {
    "objectID": "chapters/08-svm.html#svm-w-regresji-svr",
    "href": "chapters/08-svm.html#svm-w-regresji-svr",
    "title": "9  Support Vector Machines",
    "section": "9.7 SVM w regresji: SVR",
    "text": "9.7 SVM w regresji: SVR\nW regresji celem nie jest rozdział klas, lecz znalezienie funkcji możliwie „płaskiej”, która dopuszcza błąd do \\(\\varepsilon\\). W SVR wprowadza się stratę \\(\\varepsilon\\)-insensitive: błędy mniejsze niż \\(\\varepsilon\\) nie są karane. Model ma postać \\(f(x)=w^\\top\\varphi(x)+b\\), a problem optymalizacyjny:\n\\[\n\\min_{w,b,\\xi,\\xi^*}\\ \\frac{1}{2}\\|w\\|^2 + C\\sum_{i=1}^n (\\xi_i+\\xi_i^*)\n\\] \\[\n\\text{s.t.}\\quad y_i - (w^\\top\\varphi(x_i)+b)\\le \\varepsilon + \\xi_i,\\quad (w^\\top\\varphi(x_i)+b) - y_i\\le \\varepsilon + \\xi_i^*,\\quad \\xi_i,\\xi_i^*\\ge 0.\n\\]\nW praktyce \\(\\varepsilon\\) kontroluje „szerokość rurki” tolerancji, a \\(C\\) kontroluje karę za przekroczenia. SVR, podobnie jak SVC, może korzystać z jąder.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Support Vector Machines</span>"
    ]
  },
  {
    "objectID": "chapters/08-svm.html#założenia-zalety-i-wady",
    "href": "chapters/08-svm.html#założenia-zalety-i-wady",
    "title": "9  Support Vector Machines",
    "section": "9.8 Założenia, zalety i wady",
    "text": "9.8 Założenia, zalety i wady\nSVM zakłada, że klasy (lub zależność w regresji) są dobrze aproksymowalne przez hiperpłaszczyznę w pewnej przestrzeni cech (jawnej lub indukowanej jądrem). Metoda jest bardzo wrażliwa na skalę cech (zwłaszcza dla jąder RBF i wielomianowych), dlatego w praktyce niemal zawsze stosujemy standaryzację. SVM ma silne podstawy teoretyczne (maksymalizacja marginesu, optymalizacja wypukła), często dobrze działa w danych wysokowymiarowych (np. tekst) i przy umiarkowanych próbach. Wadą jest koszt obliczeniowy dla dużych \\(n\\) (szczególnie w kernel SVM), konieczność strojenia hiperparametrów (\\(C\\), \\(\\gamma\\), wybór jądra) oraz mniejsza interpretowalność w porównaniu z prostymi modelami liniowymi.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Support Vector Machines</span>"
    ]
  },
  {
    "objectID": "chapters/08-svm.html#przykład-klasyfikacji-wieloklasowej",
    "href": "chapters/08-svm.html#przykład-klasyfikacji-wieloklasowej",
    "title": "9  Support Vector Machines",
    "section": "9.9 Przykład klasyfikacji wieloklasowej",
    "text": "9.9 Przykład klasyfikacji wieloklasowej\nPoniżej wykorzystujemy zbiór scikit-learn/iris z Hugging Face i uczymy wieloklasowe SVM z jądrem RBF. Zwracamy uwagę na dwa elementy praktyczne: standaryzację oraz dobór hiperparametrów \\(C\\) i \\(\\gamma\\) przez walidację krzyżową.\n\n\nKod\nimport numpy as np\nimport pandas as pd\n\nfrom datasets import load_dataset\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\nimport matplotlib.pyplot as plt\n\n# 1) Dane\niris = load_dataset(\"scikit-learn/iris\", split=\"train\")\ndf = iris.to_pandas()\n\n# 2) Target i cechy (różne wersje zbioru mogą mieć różne nazwy kolumny docelowej)\npossible_targets = [\"label\", \"target\", \"species\", \"class\"]\ntarget_col = next((c for c in possible_targets if c in df.columns), None)\n\nif target_col is None:\n    non_num = df.select_dtypes(exclude=[\"number\"]).columns.tolist()\n    if len(non_num) == 0:\n        raise ValueError(\n            \"Nie znaleziono kolumny docelowej. Dostępne kolumny: \" + \", \".join(df.columns)\n        )\n    target_col = non_num[0]\n\n# y: etykiety klas (kodujemy tekst do liczb)\ny_raw = df[target_col]\nif y_raw.dtype.kind in {\"i\", \"u\"}:\n    y = y_raw.astype(int)\nelse:\n    y = y_raw.astype(\"category\").cat.codes\n\n# X: wszystkie cechy liczbowe z wykluczeniem targetu\nX = df.select_dtypes(include=[\"number\"]).drop(columns=[target_col], errors=\"ignore\")\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.25, random_state=42, stratify=y\n)\n\n# 3) Pipeline: skalowanie + SVC\npipe = Pipeline([\n    (\"scaler\", StandardScaler()),\n    (\"svc\", SVC(kernel=\"rbf\"))\n])\n\nparam_grid = {\n    \"svc__C\": [0.1, 1, 10, 100],\n    \"svc__gamma\": [0.01, 0.1, 1, 10]\n}\n\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\nsearch = GridSearchCV(pipe, param_grid=param_grid, scoring=\"accuracy\", cv=cv, n_jobs=-1)\nsearch.fit(X_train, y_train)\n\nbest = search.best_estimator_\nprint(\"Best params:\", search.best_params_)\n\ny_pred = best.predict(X_test)\nacc = accuracy_score(y_test, y_pred)\nprint(\"Test accuracy:\", acc)\nprint(classification_report(y_test, y_pred))\n\ncm = confusion_matrix(y_test, y_pred)\ndisp = ConfusionMatrixDisplay(cm)\nplt.figure(figsize=(4.5,4))\ndisp.plot(values_format=\"d\")\nplt.title(\"SVM (RBF): macierz pomyłek (Iris)\")\nplt.tight_layout()\nplt.show()\n\n\nBest params: {'svc__C': 1, 'svc__gamma': 0.1}\nTest accuracy: 1.0\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        12\n           1       1.00      1.00      1.00        13\n           2       1.00      1.00      1.00        13\n\n    accuracy                           1.00        38\n   macro avg       1.00      1.00      1.00        38\nweighted avg       1.00      1.00      1.00        38\n\n\n\n&lt;Figure size 432x384 with 0 Axes&gt;",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Support Vector Machines</span>"
    ]
  },
  {
    "objectID": "chapters/08-svm.html#przykład-regresyjny",
    "href": "chapters/08-svm.html#przykład-regresyjny",
    "title": "9  Support Vector Machines",
    "section": "9.10 Przykład regresyjny",
    "text": "9.10 Przykład regresyjny\nW drugim przykładzie pokazujemy SVR na zbiorze scikit-learn/Fish. Ponownie stosujemy skalowanie i stroimy \\(C\\), \\(\\varepsilon\\) oraz \\(\\gamma\\) (dla RBF) przez walidację krzyżową.\n\n\nKod\nimport numpy as np\nimport pandas as pd\n\nfrom datasets import load_dataset\nfrom sklearn.model_selection import train_test_split, KFold, GridSearchCV\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVR\nfrom sklearn.metrics import r2_score, mean_squared_error\n\n# 1) Dane: tabular regression (masa ryby) z Hugging Face\n# Dataset: scikit-learn/Fish\nfish = load_dataset(\"scikit-learn/Fish\", split=\"train\")\ndf = fish.to_pandas()\n\n# 2) Target i cechy\n# Target: 'Weight' (waga w gramach). Kolumna 'Species' jest kategoryczna;\n# dla prostoty przykładu SVR używamy wyłącznie cech liczbowych.\ntarget = \"Weight\"\nif target not in df.columns:\n    raise ValueError(\n        \"Nie znaleziono kolumny 'Weight'. Dostępne kolumny: \" + \", \".join(df.columns)\n    )\n\ny = df[target].astype(float)\nX = df.select_dtypes(include=[\"number\"]).drop(columns=[target], errors=\"ignore\").copy()\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.25, random_state=42\n)\n\npipe = Pipeline([\n    (\"scaler\", StandardScaler()),\n    (\"svr\", SVR(kernel=\"rbf\"))\n])\n\nparam_grid = {\n    \"svr__C\": [1, 10, 100, 300],\n    \"svr__epsilon\": [1, 5, 10, 20],\n    \"svr__gamma\": [0.001, 0.01, 0.1, 1]\n}\n\ncv = KFold(n_splits=5, shuffle=True, random_state=42)\nsearch = GridSearchCV(pipe, param_grid=param_grid, scoring=\"r2\", cv=cv, n_jobs=-1)\nsearch.fit(X_train, y_train)\n\nbest = search.best_estimator_\nprint(\"Best params:\", search.best_params_)\n\npred = best.predict(X_test)\nprint(\"Test R2:\", r2_score(y_test, pred))\nprint(\"Test RMSE:\", np.sqrt(mean_squared_error(y_test, pred)))\n\n\nBest params: {'svr__C': 300, 'svr__epsilon': 1, 'svr__gamma': 0.01}\nTest R2: 0.964401219567342\nTest RMSE: 68.12953630825862",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Support Vector Machines</span>"
    ]
  },
  {
    "objectID": "chapters/09-dalsze-zagadnienia.html",
    "href": "chapters/09-dalsze-zagadnienia.html",
    "title": "10  Inne metody",
    "section": "",
    "text": "10.1 Reinforcement learning",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Inne metody</span>"
    ]
  },
  {
    "objectID": "chapters/09-dalsze-zagadnienia.html#reinforcement-learning",
    "href": "chapters/09-dalsze-zagadnienia.html#reinforcement-learning",
    "title": "10  Inne metody",
    "section": "",
    "text": "Uczenie ze wzmocnieniem: agent uczy sie przez interakcje ze srodowiskiem, maksymalizujac sume przyszlych nagrod.\nElementy: agent, srodowisko, stan, akcja, nagroda, polityka i funkcja wartosci; modelowane jako zadanie Markowa.\nSzczegolowe algorytmy (Q-learning, SARSA, aktor-krytyk) omawiane sa w innym kursie.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Inne metody</span>"
    ]
  },
  {
    "objectID": "chapters/09-dalsze-zagadnienia.html#modele-koszykowe-market-basket-analysis",
    "href": "chapters/09-dalsze-zagadnienia.html#modele-koszykowe-market-basket-analysis",
    "title": "10  Inne metody",
    "section": "10.2 Modele koszykowe (market basket analysis)",
    "text": "10.2 Modele koszykowe (market basket analysis)\n\nAnaliza koszykowa wykrywa wzorce zakupowe i produkty kupowane razem.\nReguly asocjacyjne “jezeli-to”: antecedent i consequent.\nMiary: support, confidence i lift.\nAlgorytmy: Apriori, FP-Growth.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Inne metody</span>"
    ]
  },
  {
    "objectID": "chapters/09-dalsze-zagadnienia.html#wykrywanie-anomalii-ponowne",
    "href": "chapters/09-dalsze-zagadnienia.html#wykrywanie-anomalii-ponowne",
    "title": "10  Inne metody",
    "section": "10.3 Wykrywanie anomalii (ponowne)",
    "text": "10.3 Wykrywanie anomalii (ponowne)\n\nPrzypomnienie definicji anomalii w kontekście oszustw, awarii i bezpieczenstwa.\nTypy anomalii: punktowe, kontekstowe, zbiorowe.\nMetody detekcji omawiane sa szczegolowo w innych kursach.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Inne metody</span>"
    ]
  },
  {
    "objectID": "chapters/reference.html",
    "href": "chapters/reference.html",
    "title": "References",
    "section": "",
    "text": "Breiman, Leo. 1996. “Bagging Predictors.” Machine\nLearning 24 (2): 123–40. https://doi.org/10.1023/a:1018054314350.\n\n\n———. 2001. “Random Forests.” Machine Learning 45\n(1): 5–32. https://doi.org/10.1023/a:1010933404324.\n\n\nBreiman, Leo, Jerome Friedman, R. A. Olshen, and Charles J. Stone. 2017.\nClassification and Regression Trees. New York: Chapman;\nHall/CRC.\n\n\nChen, Tianqi, and Carlos Guestrin. 2016. “XGBoost.”\nProceedings of the 22nd ACM SIGKDD International Conference on\nKnowledge Discovery and Data Mining, August, 785–94. https://doi.org/10.1145/2939672.2939785.\n\n\nEfron, B. 1979. “Bootstrap Methods: Another Look at the\nJackknife.” The Annals of Statistics 7 (1). https://doi.org/10.1214/aos/1176344552.\n\n\nFisher, R. A. 1936. “The Use of Multiple Measurements in Taxonomic\nProblems.” Annals of Eugenics 7 (2): 179–88. https://doi.org/10.1111/j.1469-1809.1936.tb02137.x.\n\n\nFreund, Yoav, and Robert E Schapire. 1997. “A Decision-Theoretic\nGeneralization of On-Line Learning and an Application to\nBoosting.” Journal of Computer and System Sciences 55\n(1): 119–39. https://doi.org/10.1006/jcss.1997.1504.\n\n\nFriedman, Jerome H. 2001. “Greedy Function Approximation: A\nGradient Boosting Machine.” The Annals of Statistics 29\n(5). https://doi.org/10.1214/aos/1013203451.\n\n\nGeurts, Pierre, Damien Ernst, and Louis Wehenkel. 2006. “Extremely\nRandomized Trees.” Machine Learning 63 (1): 3–42. https://doi.org/10.1007/s10994-006-6226-1.\n\n\nHothorn, Torsten, Kurt Hornik, and Achim Zeileis. 2006. “Unbiased\nRecursive Partitioning: A Conditional Inference Framework.”\nJournal of Computational and Graphical Statistics 15 (3):\n651–74. https://doi.org/10.1198/106186006X133933.\n\n\nKass, G. V. 1980. “An Exploratory Technique for Investigating\nLarge Quantities of Categorical Data.” Applied\nStatistics 29 (2): 119. https://doi.org/10.2307/2986296.\n\n\nKe, Guolin, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma,\nQiwei Ye, and Tie-Yan Liu. 2017. “LightGBM: A Highly Efficient\nGradient Boosting Decision Tree.” In. Vol. 30. Curran Associates,\nInc. https://proceedings.neurips.cc/paper_files/paper/2017/hash/6449f44a102fde848669bdd9eb6b76fa-Abstract.html.\n\n\nKuhn, Max, and Ross Quinlan. 2018. C50: C5.0 Decision Trees and\nRule-Based Models. https://CRAN.R-project.org/package=C50.\n\n\nQuinlan, J Ross. 1993. C4. 5: Programs for Machine Learning.\nMorgan Kaufmann.\n\n\nQuinlan, J. R. 1986. “Induction of Decision Trees.”\nMachine Learning 1 (1): 81–106. https://doi.org/10.1007/BF00116251.\n\n\nSchapire, Robert E. 1990. “The Strength of Weak\nLearnability.” Machine Learning 5 (2): 197–227. https://doi.org/10.1023/a:1022648800760.\n\n\nSingh, Pranjal Kumar, and Seba Susan. 2025.\n“EfficientNetB0-CatBoost: Deep Learning With Categorical Boosting\nfor Food Image Recognition.” Cureus Journal of Computer\nScience, October. https://doi.org/10.7759/s44389-025-03791-2.\n\n\nTin Kam Ho. 1998. “The Random Subspace Method for Constructing\nDecision Forests.” IEEE Transactions on Pattern Analysis and\nMachine Intelligence 20 (8): 832–44. https://doi.org/10.1109/34.709601.\n\n\nWardJR, Joe H., and Marion E. Hook. 1963. “Application of an\nHierarchical Grouping Procedure to a Problem of Grouping\nProfiles.” Educational and Psychological Measurement 23\n(1): 69–81. https://doi.org/10.1177/001316446302300107.",
    "crumbs": [
      "References"
    ]
  }
]