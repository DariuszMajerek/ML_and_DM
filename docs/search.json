[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Wstep",
    "section": "",
    "text": "Cel i charakter kursu\nKsiążka jest przygotowana dla studentów kierunków matematyka oraz inżynieria i analiza danych.\nCelem niniejszego kursu jest stworzenie studentom możliwości zapoznania się z metodami eksploracji danych oraz klasycznego uczenia maszynowego, ze szczególnym naciskiem na zrozumienie ich założeń, mechanizmów działania oraz miejsca w procesie analizy danych. Kurs ma umożliwić nie tylko poznanie konkretnych algorytmów, lecz przede wszystkim wykształcenie świadomości metodologicznej, która pozwala na świadomy dobór narzędzi analitycznych do charakteru problemu i struktury danych. Eksploracja danych i uczenie maszynowe są w ramach kursu traktowane jako spójny zbiór technik służących do wydobywania wiedzy z danych empirycznych oraz budowy modeli predykcyjnych i decyzyjnych.\nKurs jest osadzony w kontekście analizy danych tablicowych oraz klasycznych problemów klasyfikacji i regresji. Studenci będą mogli zapoznać się z metodami, które odgrywają fundamentalną rolę w praktyce analitycznej i stanowią podstawę wielu współczesnych rozwiązań stosowanych w analizie danych. Omawiane techniki, takie jak regresja logistyczna, analiza dyskryminacyjna, drzewa decyzyjne, metody zespołowe, klasyfikatory bayesowskie, algorytm k najbliższych sąsiadów, modele addytywne oraz maszyny wektorów nośnych, są prezentowane jako elementy spójnej tradycji metod eksploracyjnych i predykcyjnych. Kurs celowo pomija zagadnienia wdrażania modeli oraz głębokich sieci neuronowych, które są realizowane w ramach innych przedmiotów, aby zachować koncentrację na klasycznych fundamentach uczenia maszynowego.\nW trakcie kursu studenci będą mogli nauczyć się, że eksploracja danych nie ogranicza się wyłącznie do wizualizacji czy statystyk opisowych, lecz obejmuje również budowę modeli umożliwiających identyfikację wzorców, segmentację obserwacji oraz podejmowanie decyzji na podstawie danych. Jednocześnie będą mogli zrozumieć, że uczenie maszynowe nie sprowadza się jedynie do stosowania algorytmów optymalizacyjnych, lecz stanowi formalny aparat pozwalający na automatyczne uczenie się zależności oraz ocenę jakości predykcji. Takie ujęcie ma umożliwić lepsze zrozumienie, dlaczego te same metody znajdują zastosowanie zarówno w analizie eksploracyjnej, jak i w zadaniach stricte predykcyjnych.\nIstotnym elementem kursu jest umożliwienie studentom zapoznania się z etapem przygotowania danych, który w praktyce analizy danych ma kluczowe znaczenie dla jakości modeli. Studenci będą mogli nauczyć się zasad czyszczenia danych, obsługi braków, kodowania zmiennych, standaryzacji oraz wstępnej analizy rozkładów i zależności. Etap ten jest przedstawiany nie jako czynność czysto techniczna, lecz jako integralna część procesu eksploracji danych, wymagająca podejmowania świadomych decyzji analitycznych i uwzględniania kontekstu problemu. Takie podejście ma umożliwić lepsze przygotowanie do pracy z rzeczywistymi danymi empirycznymi.\nW dalszej części kursu studenci będą mogli zapoznać się z metodami uczenia nadzorowanego w sposób systematyczny, z naciskiem na ich interpretację, założenia statystyczne oraz różnice pomiędzy podejściami generatywnymi i dyskryminacyjnymi. Kurs ma umożliwić zrozumienie, w jakich sytuacjach zasadne jest stosowanie prostych modeli liniowych, a kiedy warto sięgnąć po metody nieliniowe lub zespołowe. Szczególna uwaga poświęcona jest zagadnieniu interpretowalności modeli oraz kompromisowi pomiędzy złożonością modelu a jego zdolnością do generalizacji, co stanowi jeden z kluczowych problemów w analizie danych stosowanej.\nChoć kurs koncentruje się na klasycznych metodach, jego celem jest również umożliwienie studentom zrozumienia szerszego kontekstu uczenia maszynowego. Krótkie wprowadzenia do reinforcement learning, modeli koszykowych oraz wykrywania anomalii mają pozwolić na zapoznanie się z podstawowymi zasadami działania innych paradygmatów uczenia. Zagadnienia te są omawiane na poziomie koncepcyjnym, tak aby studenci mogli rozpoznać, jakie typy problemów wymagają odmiennych podejść oraz jakie są granice stosowalności klasycznych metod eksploracji danych.\nOstatecznie kurs ma umożliwić studentom wykształcenie umiejętności krytycznego myślenia o danych i modelach. Po jego ukończeniu studenci będą mogli nie tylko znać podstawowe algorytmy eksploracji danych i uczenia maszynowego, lecz także rozumieć ich genezę, założenia oraz konsekwencje ich zastosowania. Kurs stanowi fundament dla dalszych zajęć z zakresu zaawansowanego uczenia maszynowego, analizy dużych zbiorów danych oraz systemów opartych na sztucznej inteligencji, dostarczając solidnych podstaw teoretycznych i metodologicznych.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Wstep</span>"
    ]
  },
  {
    "objectID": "index.html#struktura-ksiazki",
    "href": "index.html#struktura-ksiazki",
    "title": "Wstep",
    "section": "Struktura ksiazki",
    "text": "Struktura ksiazki\n\nWprowadzenie i historia\nPrzygotowanie i czyszczenie danych\nModele liniowe i dyskryminacyjne\nDrzewa decyzyjne i zespoly modeli\nKlasyfikatory bayesowskie\nk-NN\nModele addytywne\nSVM\nInne metody\nPodsumowanie",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Wstep</span>"
    ]
  },
  {
    "objectID": "chapters/01-wprowadzenie.html",
    "href": "chapters/01-wprowadzenie.html",
    "title": "2  Wprowadzenie i historia",
    "section": "",
    "text": "2.1 Historia eksploracji danych i uczenia maszynowego\nHistoria eksploracji danych oraz uczenia maszynowego jest ściśle związana z rozwojem statystyki, informatyki i sztucznej inteligencji, jednak obie dziedziny wywodzą się z odmiennych tradycji badawczych. Eksploracja danych (data mining) rozwijała się przede wszystkim na gruncie statystyki matematycznej, analizy danych oraz teorii baz danych i była odpowiedzią na rosnącą dostępność dużych zbiorów danych empirycznych. Jej głównym celem było odkrywanie wzorców, struktur i zależności, które nie były bezpośrednio widoczne przy użyciu klasycznych narzędzi analizy. Uczenie maszynowe (machine learning) natomiast wyrosło z badań nad sztuczną inteligencją i algorytmami uczącymi się, kładąc nacisk na formalne modele predykcyjne, automatyczne dostosowywanie się do danych oraz własności generalizacji. W kontekście niniejszego wykładu oba podejścia spotykają się w obszarze metod klasyfikacyjnych i regresyjnych, które jednocześnie służą eksploracji danych i budowie modeli uczących się.\nPierwsze istotne fundamenty eksploracji danych pojawiły się już na początku XX wieku wraz z rozwojem statystyki wielowymiarowej. Szczególne znaczenie miały badania nad klasyfikacją i redukcją wymiaru, prowadzone w ramach wnioskowania statystycznego. Przełomową rolę odegrała praca Ronalda A. Fishera z 1936 roku, w której zaproponowano liniową analizę dyskryminacyjną jako metodę rozróżniania klas na podstawie kombinacji liniowych zmiennych. Choć Fisher nie posługiwał się pojęciem uczenia maszynowego, jego metoda stanowi bezpośredni pierwowzór współczesnych modeli dyskryminacyjnych, takich jak LDA, QDA oraz ich późniejsze uogólnienia. W tym okresie analiza danych była traktowana przede wszystkim jako narzędzie inferencyjne, służące do testowania hipotez i opisu struktury populacji.\nRównolegle do rozwoju statystyki, w połowie XX wieku zaczęły kształtować się idee sztucznej inteligencji. To właśnie na tym gruncie narodziło się uczenie maszynowe jako odrębna dziedzina. Jednym z najczęściej cytowanych momentów symbolicznych jest publikacja Arthura Samuela z 1959 roku, w której autor opisał algorytm uczący się gry w warcaby poprzez doświadczenie. Samuel zaproponował definicję uczenia maszynowego jako procesu, w którym system poprawia swoje działanie na podstawie obserwacji danych, co do dziś pozostaje centralnym elementem tej dziedziny. W tym samym czasie rozwijano probabilistyczne metody klasyfikacji, oparte na twierdzeniu Bayesa, które umożliwiały formalne łączenie danych empirycznych z wiedzą a priori. Klasyfikatory ML, MAP oraz naiwny klasyfikator Bayesa znalazły szerokie zastosowanie zarówno w eksploracji danych, jak i w uczeniu maszynowym.\nLata siedemdziesiąte i osiemdziesiąte XX wieku przyniosły intensywny rozwój metod klasyfikacyjnych, które do dziś stanowią podstawę analizy danych. Szczególnie istotne okazały się drzewa decyzyjne, ugruntowane w monografii Breimana, Friedmana, Olshena i Stone’a z 1984 roku. Metody CART wprowadziły ideę rekurencyjnego podziału przestrzeni cech oraz kryteria optymalizacji oparte na nieczystości węzłów, co pozwoliło na budowę modeli jednocześnie skutecznych i interpretowalnych. Drzewa decyzyjne szybko stały się jednym z podstawowych narzędzi eksploracji danych, zwłaszcza w analizie zbiorów o złożonej strukturze i mieszanych typach zmiennych.\nW tym samym okresie rozwijano metody oparte na podobieństwie obserwacji, w szczególności algorytm k najbliższych sąsiadów. Praca Covera i Harta z 1967 roku formalnie opisała własności klasyfikatora k-NN, który nie wymaga jawnej estymacji parametrów modelu, a decyzje podejmuje na podstawie lokalnej struktury danych. Metody te odegrały ważną rolę w eksploracji danych, ponieważ umożliwiały analizę bez silnych założeń rozkładowych i stanowiły punkt odniesienia dla późniejszych, bardziej złożonych algorytmów.\nIstotnym krokiem w stronę formalizacji uczenia maszynowego było wprowadzenie maszyn wektorów nośnych w latach dziewięćdziesiątych. Praca Cortes i Vapnika z 1995 roku zaproponowała model klasyfikacyjny oparty na maksymalizacji marginesu separacji klas, osadzony w aparacie optymalizacji wypukłej. Dzięki zastosowaniu funkcji jądrowych SVM umożliwiły efektywną klasyfikację danych nieliniowo separowalnych w przestrzeniach o bardzo wysokim wymiarze. Metoda ta stała się jednym z najlepiej ugruntowanych teoretycznie algorytmów uczenia maszynowego i do dziś pełni istotną rolę w analizie danych.\nKolejnym przełomem w eksploracji danych było pojawienie się metod zespołowych. Leo Breiman wprowadził ideę baggingu w 1996 roku, pokazując, że agregacja wielu niestabilnych modeli może znacząco poprawić jakość predykcji. Rozwinięciem tej koncepcji były lasy losowe, które połączyły bagging z losowym wyborem cech, tworząc jeden z najbardziej uniwersalnych algorytmów analizy danych. Równolegle rozwijał się nurt boostingu, zapoczątkowany przez Freund i Schapire, który polegał na sekwencyjnym uczeniu słabych klasyfikatorów i ich adaptacyjnym ważeniu. Współczesne algorytmy, takie jak Gradient Boosting, XGBoost czy CatBoost, stanowią bezpośrednie rozwinięcie tych idei i są dziś standardem w analizie danych tablicowych.\nNa styku statystyki i uczenia maszynowego rozwijały się także modele addytywne. Prace Hastiego i Tibshiraniego z lat osiemdziesiątych wprowadziły uogólnione modele addytywne, które umożliwiają elastyczne modelowanie zależności nieliniowych przy zachowaniu interpretowalnej struktury. Modele te stanowią naturalne rozszerzenie klasycznej regresji i dobrze ilustrują ewolucję metod eksploracji danych w kierunku większej elastyczności bez rezygnacji z kontroli nad złożonością modelu.\nZ perspektywy dydaktycznej historia eksploracji danych i uczenia maszynowego pokazuje, że większość współczesnych algorytmów opiera się na ideach rozwijanych od dziesięcioleci. Metody omawiane w dalszej części wykładu nie są oderwanymi narzędziami, lecz elementami spójnej ewolucji pojęć takich jak klasyfikacja, estymacja, generalizacja i kompromis między złożonością a interpretowalnością. Zrozumienie tego kontekstu historycznego pozwala lepiej interpretować zachowanie modeli, ich założenia oraz ograniczenia w praktycznej analizie danych.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Wprowadzenie i historia</span>"
    ]
  },
  {
    "objectID": "chapters/02-przygotowanie-danych.html",
    "href": "chapters/02-przygotowanie-danych.html",
    "title": "3  Przygotowanie i czyszczenie danych",
    "section": "",
    "text": "3.1 Import i czyszczenie danych\nPrzygotowanie danych stanowi jeden z kluczowych etapów procesu eksploracji danych i uczenia maszynowego. Jakość danych wejściowych w dużej mierze determinuje jakość modeli, niezależnie od stopnia ich złożoności. W praktyce analizy danych etap ten obejmuje zarówno wstępną eksplorację danych, jak i ich transformację, czyszczenie oraz konstrukcję cech, które będą następnie wykorzystywane przez algorytmy uczenia nadzorowanego. W niniejszym rozdziale przygotowanie danych jest traktowane jako proces analityczny, a nie jedynie techniczny etap przetwarzania.\nImport danych jest pierwszym momentem, w którym można świadomie zminimalizować późniejsze problemy jakościowe. W praktyce ważne jest nie tylko „wczytanie pliku”, ale kontrola takich elementów jak kodowanie znaków, separator, typy danych, format dat, niestandardowe znaczniki braków oraz interpretacja wartości logicznych. pandas daje w tym zakresie bardzo duże możliwości, a poprawna konfiguracja importu bywa istotniejsza niż późniejsze „naprawy” wykonywane ad hoc.\nNajczęściej spotykanym formatem jest CSV, który bywa myląco prosty: różne pliki mogą używać separatora , lub ;, różnych separatorów dziesiętnych (kropka/przecinek), mogą zawierać znaki narodowe (tu: polskie nazwy miast), a braki mogą być kodowane jako puste pole, NA, N/A, null, -999 itd. Dlatego przy imporcie CSV bardzo często warto jawnie ustawić: sep, encoding, na_values, parse_dates, a w razie potrzeby także dtype. Przykładowo1:\nKod\nimport pandas as pd\n\ndf = pd.read_csv(\n    \"data.csv\",\n    encoding=\"utf-8\",\n    na_values=[\"\", \"NA\", \"N/A\", \"null\", \"None\"],\n    parse_dates=[\"signup_date\", \"last_purchase_date\"]\n)\ndf.head()\n\n\n\n\n\n\n\n\n\ncustomer_id\nsignup_date\nlast_purchase_date\nage\nincome_eur\ncity\nsegment\ndevice_type\nvisits_last30\navg_basket_eur\nreturns_last90\nhas_promo\nsatisfaction_1_5\nchurned\nnotes\n\n\n\n\n0\n100000\n2022-04-08\n2022-10-19\nNaN\n72955.0\nWarszawa\nB2B\ndesktop\n8.0\n39.88\n1.0\nFalse\n4.0\n0\nklient powracający\n\n\n1\n100001\n2024-04-27\n2024-05-13\n52.0\n24863.0\nKraków\nB2C\nmobile\n12.0\n67.80\n1.0\nFalse\n3.0\n0\nbrak uwag\n\n\n2\n100002\n2023-12-18\n2024-04-05\nNaN\n25333.0\nWrocław\nB2C\ndesktop\n6.0\n27.04\n1.0\nTrue\n4.0\n0\nreklamacja\n\n\n3\n100003\n2023-04-26\n2023-10-01\n16.0\nNaN\nŁódź\nB2B\nmobile\n6.0\n149.03\n1.0\nFalse\n3.0\n1\nbrak uwag\n\n\n4\n100004\n2023-04-20\n2024-04-07\n42.0\n27954.0\nWarszawa\nB2C\ndesktop\n7.0\n49.73\n1.0\nTrue\n3.0\n0\nNaN\nW tym przykładzie na_values powoduje, że zarówno puste pola, jak i tekstowe znaczniki braków zostaną zamienione na NaN, a parse_dates zadba o automatyczną konwersję wskazanych kolumn do typu daty. To jest szczególnie ważne, bo daty wczytane jako tekst utrudniają analizę sezonowości, czasu od rejestracji, czy prostych agregacji po miesiącach.\nW przypadku danych w Excelu (.xlsx) import odbywa się przez read_excel. W praktyce warto pamiętać, że Excel często zawiera dodatkowe arkusze, nagłówki „opisowe” nad tabelą, albo mieszane typy w kolumnach. Gdy dane są w konkretnym arkuszu i zaczynają się od konkretnego wiersza, przydają się parametry sheet_name oraz skiprows. Dla dołączonego pliku:\nKod\ndf = pd.read_excel(\n    \"data.xlsx\",\n    sheet_name=0,\n    na_values=[\"\", \"NA\", \"N/A\", \"null\", \"None\"],\n    parse_dates=[\"signup_date\", \"last_purchase_date\"]\n)\n\ndf.head()\n\n\n\n\n\n\n\n\n\ncustomer_id\nsignup_date\nlast_purchase_date\nage\nincome_eur\ncity\nsegment\ndevice_type\nvisits_last30\navg_basket_eur\nreturns_last90\nhas_promo\nsatisfaction_1_5\nchurned\nnotes\n\n\n\n\n0\n100000\n2022-04-08\n2022-10-19\nNaN\n72955.0\nWarszawa\nB2B\ndesktop\n8\n39.88\n1\nFalse\n4.0\n0\nklient powracający\n\n\n1\n100001\n2024-04-27\n2024-05-13\n52.0\n24863.0\nKraków\nB2C\nmobile\n12\n67.80\n1\nFalse\n3.0\n0\nbrak uwag\n\n\n2\n100002\n2023-12-18\n2024-04-05\nNaN\n25333.0\nWrocław\nB2C\ndesktop\n6\n27.04\n1\nTrue\n4.0\n0\nreklamacja\n\n\n3\n100003\n2023-04-26\n2023-10-01\n16.0\nNaN\nŁódź\nB2B\nmobile\n6\n149.03\n1\nFalse\n3.0\n1\nbrak uwag\n\n\n4\n100004\n2023-04-20\n2024-04-07\n42.0\n27954.0\nWarszawa\nB2C\ndesktop\n7\n49.73\n1\nTrue\n3.0\n0\nNaN\nKlasyczny format JSON (.json) przechowuje dane jako jedną spójną strukturę, najczęściej listę obiektów (rekordów). Taki zapis jest szczególnie często spotykany w interfejsach API, plikach konfiguracyjnych oraz w wymianie danych pomiędzy systemami informatycznymi. Z punktu widzenia analizy danych format ten jest bardziej „opisowy” i czytelny dla człowieka, ale jednocześnie wymaga załadowania całej struktury do pamięci.\nW przygotowanym pliku data.json dane zapisane są jako lista rekordów, gdzie każdy rekord odpowiada jednej obserwacji, a klucze obiektów odpowiadają nazwom zmiennych. Taki układ bardzo naturalnie mapuje się na strukturę ramki danych w pandas. Podstawowy import danych JSON do pandas odbywa się za pomocą funkcji read_json.\nKod\ndf = pd.read_json(\"data.json\")\ndf.head()\n\n\n\n\n\n\n\n\n\ncustomer_id\nsignup_date\nlast_purchase_date\nage\nincome_eur\ncity\nsegment\ndevice_type\nvisits_last30\navg_basket_eur\nreturns_last90\nhas_promo\nsatisfaction_1_5\nchurned\nnotes\n\n\n\n\n0\n100000\n1649376000000\n1666137600000\nNaN\n72955.0\nWarszawa\nB2B\ndesktop\n8\n39.88\n1\nFalse\n4.0\n0\nklient powracający\n\n\n1\n100001\n1714176000000\n1715558400000\n52.0\n24863.0\nKraków\nB2C\nmobile\n12\n67.80\n1\nFalse\n3.0\n0\nbrak uwag\n\n\n2\n100002\n1702857600000\n1712275200000\nNaN\n25333.0\nWrocław\nB2C\ndesktop\n6\n27.04\n1\nTrue\n4.0\n0\nreklamacja\n\n\n3\n100003\n1682467200000\n1696118400000\n16.0\nNaN\nŁódź\nB2B\nmobile\n6\n149.03\n1\nFalse\n3.0\n1\nbrak uwag\n\n\n4\n100004\n1681948800000\n1712448000000\n42.0\n27954.0\nWarszawa\nB2C\ndesktop\n7\n49.73\n1\nTrue\n3.0\n0\nNA\nPo wczytaniu danych pandas automatycznie spróbuje rozpoznać typy zmiennych, jednak – podobnie jak w przypadku CSV – nie zawsze zrobi to zgodnie z oczekiwaniami analityka. W szczególności kolumny datowe są często wczytywane jako typ object, dlatego dobrą praktyką jest ich jawna konwersja:\nKod\ndf[\"signup_date\"] = pd.to_datetime(df[\"signup_date\"], errors=\"coerce\")\ndf[\"last_purchase_date\"] = pd.to_datetime(df[\"last_purchase_date\"], errors=\"coerce\")\ndf.head()\n\n\n\n\n\n\n\n\n\ncustomer_id\nsignup_date\nlast_purchase_date\nage\nincome_eur\ncity\nsegment\ndevice_type\nvisits_last30\navg_basket_eur\nreturns_last90\nhas_promo\nsatisfaction_1_5\nchurned\nnotes\n\n\n\n\n0\n100000\n1970-01-01 00:27:29.376000\n1970-01-01 00:27:46.137600\nNaN\n72955.0\nWarszawa\nB2B\ndesktop\n8\n39.88\n1\nFalse\n4.0\n0\nklient powracający\n\n\n1\n100001\n1970-01-01 00:28:34.176000\n1970-01-01 00:28:35.558400\n52.0\n24863.0\nKraków\nB2C\nmobile\n12\n67.80\n1\nFalse\n3.0\n0\nbrak uwag\n\n\n2\n100002\n1970-01-01 00:28:22.857600\n1970-01-01 00:28:32.275200\nNaN\n25333.0\nWrocław\nB2C\ndesktop\n6\n27.04\n1\nTrue\n4.0\n0\nreklamacja\n\n\n3\n100003\n1970-01-01 00:28:02.467200\n1970-01-01 00:28:16.118400\n16.0\nNaN\nŁódź\nB2B\nmobile\n6\n149.03\n1\nFalse\n3.0\n1\nbrak uwag\n\n\n4\n100004\n1970-01-01 00:28:01.948800\n1970-01-01 00:28:32.448000\n42.0\n27954.0\nWarszawa\nB2C\ndesktop\n7\n49.73\n1\nTrue\n3.0\n0\nNA\nParametr errors=\"coerce\" powoduje, że ewentualne niepoprawne formaty dat zostaną zamienione na NaT, co jest bezpieczniejsze niż przerwanie importu błędem. W kontekście eksploracji danych takie zachowanie pozwala szybko zidentyfikować problemy jakościowe bez utraty całego zbioru. Warto podkreślić, że format JSON nie posiada natywnego pojęcia braków danych w sensie znanym z analizy statystycznej. Braki mogą być reprezentowane jako null, brak klucza lub wartość tekstowa (np. NA). pandas zamienia null na NaN, ale nie rozpoznaje automatycznie tekstowych znaczników braków. Dlatego po imporcie zalecane jest jawne czyszczenie takich wartości:\nKod\ndf.replace([\"NA\", \"N/A\", \"\"], pd.NA, inplace=True)\ndf.head()\n\n\n\n\n\n\n\n\n\ncustomer_id\nsignup_date\nlast_purchase_date\nage\nincome_eur\ncity\nsegment\ndevice_type\nvisits_last30\navg_basket_eur\nreturns_last90\nhas_promo\nsatisfaction_1_5\nchurned\nnotes\n\n\n\n\n0\n100000\n1970-01-01 00:27:29.376000\n1970-01-01 00:27:46.137600\nNaN\n72955.0\nWarszawa\nB2B\ndesktop\n8\n39.88\n1\nFalse\n4.0\n0\nklient powracający\n\n\n1\n100001\n1970-01-01 00:28:34.176000\n1970-01-01 00:28:35.558400\n52.0\n24863.0\nKraków\nB2C\nmobile\n12\n67.80\n1\nFalse\n3.0\n0\nbrak uwag\n\n\n2\n100002\n1970-01-01 00:28:22.857600\n1970-01-01 00:28:32.275200\nNaN\n25333.0\nWrocław\nB2C\ndesktop\n6\n27.04\n1\nTrue\n4.0\n0\nreklamacja\n\n\n3\n100003\n1970-01-01 00:28:02.467200\n1970-01-01 00:28:16.118400\n16.0\nNaN\nŁódź\nB2B\nmobile\n6\n149.03\n1\nFalse\n3.0\n1\nbrak uwag\n\n\n4\n100004\n1970-01-01 00:28:01.948800\n1970-01-01 00:28:32.448000\n42.0\n27954.0\nWarszawa\nB2C\ndesktop\n7\n49.73\n1\nTrue\n3.0\n0\n&lt;NA&gt;\nW porównaniu do CSV, format JSON lepiej zachowuje strukturę danych (np. brak problemów z separatorami czy kodowaniem znaków), ale gorzej skaluje się dla bardzo dużych zbiorów. Z tego względu klasyczny JSON jest szczególnie przydatny w dydaktyce oraz w pracy z danymi średniej wielkości, gdzie czytelność i jednoznaczność struktury są ważniejsze niż wydajność. Z perspektywy dalszych etapów kursu istotne jest, aby rozumieć, że import danych nie jest neutralnym technicznie krokiem, lecz pierwszym momentem, w którym podejmowane są decyzje wpływające na całą analizę. Różnice pomiędzy CSV, JSONL i JSON przekładają się nie tylko na sposób wczytania danych, ale także na późniejsze możliwości ich walidacji, przetwarzania i skalowania.￼\nW praktyce import często wymaga kontroli typów. Jeśli np. identyfikator klienta ma wyglądać jak liczba, ale nie wolno dopuścić do utraty wiodących zer (częsty przypadek dla kodów), należy wymusić typ tekstowy przez dtype={\"customer_id\": \"string\"}. W tym konkretnym zbiorze customer_id jest liczbowy, ale w realnych danych biznesowych to częsty problem. Analogicznie, gdy w jednej kolumnie występują liczby i tekst (np. „brak”), pandas może ustawić typ object, a to utrudni obliczenia – lepiej wczytać z na_values i później rzutować typy jawnie.\nIstotne są też parametry wpływające na wydajność i kontrolę pamięci. Dla dużych plików CSV warto rozważyć usecols (czytać tylko potrzebne kolumny), chunksize (czytanie porcjami) oraz low_memory=False (mniej błędnych inferencji typów kosztem RAM). Przykład importu porcjami:\nKod\nchunks = pd.read_csv(\"data.csv\", chunksize=50_000)\nfor chunk in chunks:\n    # walidacje / czyszczenie / zapis częściowy\n    pass\nNa końcu, dobrą praktyką po imporcie jest natychmiastowa „kontrola jakości importu”: df.info(), df.isna().sum(), sprawdzenie liczby unikatów w kategoriach oraz szybkie oględziny podejrzanych wartości (np. wiek 120). To pozwala wcześnie odróżnić problemy wynikające z danych od problemów wynikających z błędnego importu.\nW praktyce analizy danych bardzo często spotyka się zbiory danych, których nazwy kolumn są niewygodne lub wręcz problematyczne z punktu widzenia języka Python oraz bibliotek analitycznych. Dotyczy to w szczególności nazw zawierających spacje, znaki specjalne, polskie znaki diakrytyczne, rozpoczynających się od cyfr, a także nazw bardzo długich lub opisowych. Choć pandas technicznie dopuszcza niemal dowolne nazwy kolumn, ich nieprzemyślane użycie prowadzi do błędów, nieczytelnego kodu oraz problemów w dalszych etapach analizy i modelowania. Problem ten ujawnia się szczególnie wyraźnie wtedy, gdy użytkownik próbuje korzystać z notacji kropkowej (df.column_name), budować formuły modelowe, pisać potoki przetwarzania lub eksportować dane do innych narzędzi analitycznych. Nazwy kolumn zawierające spacje, znaki -, %, (), #, rozpoczynające się od cyfr lub zawierające znaki typowe dla języków innych niż angielski (np. ś, ć, ń) nie mogą być używane jako poprawne identyfikatory w Pythonie. W efekcie kod staje się mniej czytelny i bardziej podatny na błędy.\nRozważmy przykładowy zbiór danych, w którym nazwy kolumn zostały nadane w sposób typowy dla arkuszy Excel lub raportów biznesowych:\nKod\ndf2 = pd.DataFrame({\n    \"Customer ID\": [1, 2, 3],\n    \"2023 Revenue (€)\": [12000, 34000, 18000],\n    \"Avg Basket Value\": [45.5, 78.2, 33.1],\n    \"% Return\": [0.02, 0.01, 0.05],\n    \"Very long column name describing customer behaviour in detail\": [1, 0, 1]\n})\ndf2\n\n\n\n\n\n\n\n\n\nCustomer ID\n2023 Revenue (€)\nAvg Basket Value\n% Return\nVery long column name describing customer behaviour in detail\n\n\n\n\n0\n1\n12000\n45.5\n0.02\n1\n\n\n1\n2\n34000\n78.2\n0.01\n0\n\n\n2\n3\n18000\n33.1\n0.05\n1\nZ punktu widzenia pandas taki zbiór danych jest poprawny, jednak już próba użycia notacji kropkowej zakończy się błędem:\nKod\ndf2.2023 Revenue (€) # błąd składni\n\n\n\n  Cell In[7], line 1\n    df2.2023 Revenue (€) # błąd składni\n                      ^\nSyntaxError: invalid character '€' (U+20AC)\nKażdorazowo konieczne byłoby odwoływanie się do kolumn przez nawiasy i łańcuchy znaków, co znacząco obniża czytelność kodu:\nKod\ndf2[\"2023 Revenue (€)\"].mean()\n\n\nnp.float64(21333.333333333332)\nDlatego dobrą praktyką w analizie danych jest normalizacja nazw kolumn bezpośrednio po imporcie danych. Najczęściej stosowana konwencja obejmuje użycie wyłącznie małych liter, znaków ASCII, podkreśleń zamiast spacji oraz nazw rozpoczynających się literą. Taki styl jest zgodny z konwencją snake_case powszechnie stosowaną w Pythonie. Podstawowa korekta nazw kolumn może zostać wykonana w kilku krokach. Najpierw usuwa się nadmiarowe spacje, zamienia litery na małe, a spacje na podkreślenia:\nKod\ndf2.columns = (\n    df2.columns\n    .str.strip()\n    .str.lower()\n    .str.replace(r\"\\s+\", \"_\", regex=True)\n)\nJednak w praktyce to zwykle nie wystarcza, ponieważ w nazwach mogą występować znaki specjalne, symbole walut, procenty czy nawiasy. W takim przypadku warto usunąć wszystkie znaki niedozwolone i pozostawić jedynie litery, cyfry i podkreślenia:\nKod\ndf2.columns = df2.columns.str.replace(r\"[^a-zA-Z0-9_]\", \"\", regex=True)\n\n# opcjonalnie: porządkujemy podkreślenia (np. po usunięciu znaków specjalnych)\ndf2.columns = (\n    df2.columns\n    .str.replace(r\"_+\", \"_\", regex=True)\n    .str.strip(\"_\")\n)\ndf2.columns\n\n\nIndex(['customer_id', '2023_revenue', 'avg_basket_value', 'return',\n       'very_long_column_name_describing_customer_behaviour_in_detail'],\n      dtype='object')\nWarto zauważyć, że po czyszczeniu może powstać nazwa będąca słowem kluczowym Pythona (np. return). Jeśli zależy nam na możliwości użycia notacji kropkowej, warto takie przypadki automatycznie zmienić, np. przez dodanie sufiksu:\nKod\nimport keyword\n\ndf2.columns = [\n    f\"{c}_var\" if keyword.iskeyword(c) else c\n    for c in df2.columns\n]\nWciąż jednak pozostaje problem nazw rozpoczynających się od cyfr. W Pythonie identyfikator nie może zaczynać się od liczby, dlatego zaleca się automatyczne dodanie prefiksu, na przykład x_:\nKod\ndf2.columns = [\n    f\"x_{c}\" if (c != \"\" and c[0].isdigit()) else c\n    for c in df2.columns\n]\ndf2.columns\n\n\nIndex(['customer_id', 'x_2023_revenue', 'avg_basket_value', 'return_var',\n       'very_long_column_name_describing_customer_behaviour_in_detail'],\n      dtype='object')\nKolejnym, często niedocenianym problemem są zbyt długie nazwy kolumn. Choć technicznie są poprawne, znacząco utrudniają pracę z kodem, zwłaszcza w dalszych etapach, takich jak budowa modeli, interpretacja współczynników czy wizualizacja wyników. W takich sytuacjach dobrą praktyką jest ręczne lub półautomatyczne skracanie nazw, przy zachowaniu ich semantyki:\nKod\ndf2 = df2.rename(columns={\n    \"very_long_column_name_describing_customer_behaviour_in_detail\": \"customer_behavior_flag\"\n})\ndf2.columns\n\n\nIndex(['customer_id', 'x_2023_revenue', 'avg_basket_value', 'return_var',\n       'customer_behavior_flag'],\n      dtype='object')\nW projektach analitycznych o większej skali często stosuje się funkcję pomocniczą, która automatycznie „czyści” nazwy kolumn według ustalonej reguły:\nKod\ndf2 = pd.DataFrame({\n    \"Customer ID\": [1, 2, 3],\n    \"2023 Revenue (€)\": [12000, 34000, 18000],\n    \"Avg Basket Value\": [45.5, 78.2, 33.1],\n    \"% Return\": [0.02, 0.01, 0.05],\n    \"Very long column name describing customer behaviour in detail\": [1, 0, 1]\n})\n\ndef clean_column_names(df, max_len=40):\n    cleaned = (\n        pd.Index(df.columns).map(str)\n        .str.strip()\n        .str.lower()\n        .str.replace(r\"\\s+\", \"_\", regex=True)\n        .str.replace(r\"[^a-z0-9_]\", \"\", regex=True)\n        .str.replace(r\"_+\", \"_\", regex=True)\n        .str.strip(\"_\")\n    )\n\n    seen = {}\n    final_cols = []\n    for c in cleaned:\n        # pusta nazwa po czyszczeniu\n        if c == \"\":\n            c = \"col\"\n\n        # nazwa zaczyna się od cyfry\n        if c[0].isdigit():\n            c = f\"x_{c}\"\n\n        # słowo kluczowe Pythona\n        if keyword.iskeyword(c):\n            c = f\"{c}_var\"\n\n        # opcjonalnie: skracamy bardzo długie nazwy (np. do pracy z modelami/tabelami)\n        if max_len is not None and len(c) &gt; max_len:\n            c = c[:max_len].rstrip(\"_\")\n            if c == \"\":\n                c = \"col\"\n\n        # unikamy kolizji nazw (np. dwie różne kolumny mogą się „zlać” po czyszczeniu)\n        count = seen.get(c, 0)\n        seen[c] = count + 1\n        if count &gt; 0:\n            suffix = f\"_{count+1}\"\n            base = c\n            if max_len is not None and len(base) + len(suffix) &gt; max_len:\n                base = base[: max_len - len(suffix)].rstrip(\"_\")\n                if base == \"\":\n                    base = \"col\"\n            c = f\"{base}{suffix}\"\n\n        final_cols.append(c)\n\n    df.columns = final_cols\n    return df\n\ndf2 = clean_column_names(df2, max_len = 22)\ndf2\n\n\n\n\n\n\n\n\n\ncustomer_id\nx_2023_revenue\navg_basket_value\nreturn_var\nvery_long_column_name\n\n\n\n\n0\n1\n12000\n45.5\n0.02\n1\n\n\n1\n2\n34000\n78.2\n0.01\n0\n\n\n2\n3\n18000\n33.1\n0.05\n1\nTakie podejście sprawia, że już na wczesnym etapie analizy dane zostają dostosowane do dalszej pracy z modelami, formułami i potokami przetwarzania. Co istotne, normalizacja nazw kolumn nie jest kosmetyką, lecz elementem przygotowania danych, który wpływa na czytelność, reprodukowalność oraz odporność kodu na błędy.\nW kontekście tego kursu warto podkreślić, że praca z „brudnymi” nazwami kolumn jest codziennością analityka danych. Umiejętność ich systematycznego korygowania powinna być traktowana na równi z obsługą braków danych czy standaryzacją zmiennych. Jest to jeden z pierwszych kroków w kierunku tworzenia stabilnych i profesjonalnych analiz.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Przygotowanie i czyszczenie danych</span>"
    ]
  },
  {
    "objectID": "chapters/02-przygotowanie-danych.html#import-i-czyszczenie-danych",
    "href": "chapters/02-przygotowanie-danych.html#import-i-czyszczenie-danych",
    "title": "3  Przygotowanie i czyszczenie danych",
    "section": "",
    "text": "1 Dane przypominają prosty wycinek danych klient–zakupy. Zawierają zmienne liczbowe (np. income_eur, avg_basket_eur, visits_last30), jakościowe (city, segment, device_type, notes), logiczne (has_promo), daty (signup_date, last_purchase_date) oraz zmienną docelową (churned). Celowo wprowadzono braki danych (w tym puste pola i znaczniki NA), wartości odstające (np. nienaturalnie wysokie dochody, koszyki zakupowe i liczby wizyt) oraz błędy jakościowe (np. wiek 5 i 120, ujemne zwroty, rozbieżności w kodowaniu kategorii: spacje i różne wielkości liter).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Przygotowanie i czyszczenie danych</span>"
    ]
  },
  {
    "objectID": "chapters/02-przygotowanie-danych.html#obserwacje-odstające-i-braki-danych",
    "href": "chapters/02-przygotowanie-danych.html#obserwacje-odstające-i-braki-danych",
    "title": "3  Przygotowanie i czyszczenie danych",
    "section": "3.2 Obserwacje odstające i braki danych",
    "text": "3.2 Obserwacje odstające i braki danych\nW praktyce analizy danych obserwacje odstające oraz braki danych są jednymi z najczęstszych źródeł błędnych wniosków i niestabilnych modeli. W EDA traktujemy je jako sygnał diagnostyczny: mogą oznaczać błędy pomiaru lub wprowadzania danych, ale mogą też być prawdziwymi rzadkimi zdarzeniami (np. klienci o bardzo wysokich dochodach). W modelowaniu ML odstające wartości i braki wpływają na estymację parametrów, stabilność uczenia i jakość generalizacji; dlatego kluczowe jest, aby postępowanie z nimi było konsekwentne, udokumentowane oraz wykonywane bez wycieku informacji (wszystkie „uczące się” transformacje dopasowujemy tylko na zbiorze treningowym).\n\n3.2.1 Obserwacje odstające\nObserwacje odstające (outliers) to wartości, które są „nietypowe” względem reszty danych. „Nietypowość” trzeba rozumieć w kontekście: może wynikać z rozkładu, jednostki miary, specyfiki biznesowej oraz tego, jakiego modelu używamy. W EDA zwykle rozróżnia się: (1) odstające wartości w pojedynczej zmiennej (univariate), (2) obserwacje odstające w relacji między zmiennymi (bivariate), oraz (3) odstające obserwacje wielowymiarowo (multivariate).\n\n3.2.1.1 Reguły matematyczne identyfikacji odstających (univariate)\nNajczęściej stosuje się regułę IQR (Tukeya) oraz regułę opartą o standaryzację. W regule IQR wyznaczamy kwartyle \\(Q_1\\) i \\(Q_3\\) oraz rozstęp międzykwartylowy \\(IQR = Q_3 - Q_1\\). Obserwację \\(x\\) uznaje się za odstającą, gdy:\n\\[\nx &lt; Q_1 - 1.5\\cdot IQR \\quad \\text{lub} \\quad x &gt; Q_3 + 1.5\\cdot IQR.\n\\]\nReguła standaryzacyjna używa z-score:\n\\[\nz = \\frac{x-\\mu}{\\sigma}.\n\\]\nTypowo za odstające przyjmuje się obserwacje o \\(|z| &gt; 3\\), ale jest to reguła wrażliwa na odstające wartości, ponieważ \\(\\mu\\) i \\(\\sigma\\) same ulegają zniekształceniu. W danych skośnych (np. dochody) często lepsze są metody odporne.\nOdporna standaryzacja opiera się na medianie i medianowym odchyleniu bezwzględnym MAD. Dla \\(MAD = \\mathrm{median}(|x - \\mathrm{median}(x)|)\\) stosuje się tzw. odporny z-score:\n\\[\nz_{\\mathrm{rob}} = \\frac{x-\\mathrm{median}(x)}{1.4826\\cdot MAD}.\n\\]\nWspółczynnik (1.4826 2) skaluje MAD do odpowiednika odchylenia standardowego przy rozkładzie normalnym.\n2 Liczba 1.4826 pochodzi ze „skalowania” miary MAD tak, aby była porównywalna z odchyleniem standardowym przy rozkładzie normalnym. Dla rozkładu normalnego \\(X \\sim \\mathcal N(\\mu,\\sigma)\\) zachodzi zależność \\(MAD = \\Phi^{-1}(0.75)\\,\\sigma \\approx 0.6745,\\sigma\\) bo dla \\(Z\\sim\\mathcal N(0,1)\\) wartość \\(\\mathrm{median}(|Z|)\\) to dokładnie 75. percentyl rozkładu normalnego: \\(\\Phi^{-1}(0.75)\\). Żeby z MAD zrobić estymator „w skali \\(\\sigma\\)”, mnoży się przez odwrotność tej stałej \\(\\sigma \\approx \\frac{MAD}{0.6745} \\approx 1.4826 \\cdot MAD\\) i stąd bierze się 1.4826 (dokładniej \\(1 / \\Phi^{-1}(0.75)\\)).W naszych danych sensownymi kandydatami do analizy odstających są: income_eur, avg_basket_eur, visits_last30, a także „błędy logiczne” w age i returns_last90.\n\n\nKod\nimport numpy as np\n\nnum_cols = [\"age\", \"income_eur\", \"visits_last30\", \"avg_basket_eur\", \"returns_last90\", \"satisfaction_1_5\"]\n\n# Szybka diagnostyka: podstawowe statystyki + percentyle\ndf[num_cols].describe(percentiles=[0.01, 0.05, 0.95, 0.99]).T\n\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n1%\n5%\n50%\n95%\n99%\nmax\n\n\n\n\nage\n333.0\n38.207207\n14.428125\n5.00\n16.0000\n16.6000\n38.000\n58.4000\n69.0000\n120.0\n\n\nincome_eur\n317.0\n40776.328076\n44393.786000\n4302.00\n8218.6800\n11883.6000\n32632.000\n83047.6000\n177796.6000\n490032.0\n\n\nvisits_last30\n350.0\n6.045714\n2.714320\n0.00\n1.0000\n2.0000\n6.000\n11.0000\n13.0000\n24.0\n\n\navg_basket_eur\n332.0\n52.835060\n48.248423\n16.05\n18.0016\n20.9775\n43.835\n99.6435\n150.4721\n668.8\n\n\nreturns_last90\n350.0\n0.777143\n0.880572\n-1.00\n0.0000\n0.0000\n1.000\n3.0000\n3.0000\n4.0\n\n\nsatisfaction_1_5\n327.0\n3.645260\n0.887628\n1.00\n1.0000\n2.0000\n4.000\n5.0000\n5.0000\n5.0\n\n\n\n\n\n\n\n\n\n3.2.1.2 Reguła IQR dla jednej zmiennej\n\n\nKod\ndef iqr_outliers(s: pd.Series, k: float = 1.5):\n    s = s.dropna()\n    q1, q3 = s.quantile(0.25), s.quantile(0.75)\n    iqr = q3 - q1\n    lower, upper = q1 - k * iqr, q3 + k * iqr\n    return lower, upper\n\nlower_inc, upper_inc = iqr_outliers(df[\"income_eur\"])\nout_income = df[(df[\"income_eur\"] &lt; lower_inc) | (df[\"income_eur\"] &gt; upper_inc)]\nout_income[[\"customer_id\", \"income_eur\", \"city\", \"segment\"]].head(10)\n\n\n\n\n\n\n\n\n\ncustomer_id\nincome_eur\ncity\nsegment\n\n\n\n\n39\n100039\n485953.0\nŁódź\nVIP\n\n\n55\n100055\n146032.0\nŁódź\nB2C\n\n\n59\n100059\n86006.0\nWrocław\nVIP\n\n\n66\n100066\n98746.0\nWrocław\nStudent\n\n\n67\n100067\n85420.0\nWrocław\nB2C\n\n\n82\n100082\n80700.0\nLublin\nB2C\n\n\n83\n100083\n82821.0\nNone\nB2C\n\n\n94\n100094\n490032.0\nLublin\nB2C\n\n\n95\n100095\n86004.0\nPoznań\nB2B\n\n\n151\n100151\n87909.0\nWrocław\nB2C\n\n\n\n\n\n\n\n\n\n3.2.1.3 Odporny z-score (MAD)\n\n\nKod\ndef robust_zscore(s: pd.Series):\n    s = s.astype(float)\n    med = np.nanmedian(s)\n    mad = np.nanmedian(np.abs(s - med))\n    return (s - med) / (1.4826 * mad)\n\ndf[\"income_robust_z\"] = robust_zscore(df[\"income_eur\"])\ndf.loc[df[\"income_robust_z\"].abs() &gt; 3, [\"customer_id\", \"income_eur\", \"income_robust_z\"]].head(10)\n\n\n\n\n\n\n\n\n\ncustomer_id\nincome_eur\nincome_robust_z\n\n\n\n\n39\n100039\n485953.0\n26.659763\n\n\n55\n100055\n146032.0\n6.669043\n\n\n59\n100059\n86006.0\n3.138920\n\n\n66\n100066\n98746.0\n3.888158\n\n\n67\n100067\n85420.0\n3.104457\n\n\n94\n100094\n490032.0\n26.899649\n\n\n95\n100095\n86004.0\n3.138802\n\n\n151\n100151\n87909.0\n3.250835\n\n\n153\n100153\n91242.0\n3.446848\n\n\n155\n100155\n83954.0\n3.018242\n\n\n\n\n\n\n\n\n\n3.2.1.4 „Odstające” jako błąd jakości danych\nCzęść wartości odstających to nie „rzadkie przypadki”, tylko po prostu błędne dane. W data.csv celowo pojawiają się np. wiek 5 i 120 oraz ujemna liczba zwrotów (returns_last90 == -1). To nie są outliery w sensie rozkładu, tylko naruszenia dziedziny wartości.\nW EDA warto wprost zdefiniować reguły walidacji, np.: \\(16 \\leq \\text{age} \\leq 80,\\) \\(\\text{returns\\_last90} \\geq 0.\\)\nW Pythonie:\n\n\nKod\nbad_age = df[(df[\"age\"] &lt; 16) | (df[\"age\"] &gt; 80)]\nbad_returns = df[df[\"returns_last90\"] &lt; 0]\n\nbad_age[[\"customer_id\",\"age\"]].head(), bad_returns[[\"customer_id\",\"returns_last90\"]].head()\n\n\n(     customer_id    age\n 43        100043  120.0\n 105       100105  120.0\n 144       100144    5.0\n 250       100250    5.0\n 340       100340  120.0,\n      customer_id  returns_last90\n 5         100005              -1\n 93        100093              -1\n 203       100203              -1)\n\n\nTakie przypadki najczęściej traktuje się jako: (1) poprawa na podstawie źródła (jeśli możliwa), albo (2) ustawienie na brak (NaN) i późniejsza imputacja, albo (3) usunięcie obserwacji (jeżeli jest ich mało i są ewidentnie błędne).\n\n\n3.2.1.5 Co robimy z obserwacjami odstającymi w EDA i ML\nW EDA celem jest zrozumienie przyczyny i konsekwencji odstających wartości. Zwykle robimy trzy rzeczy: po pierwsze, oceniamy skalę zjawiska (ile obserwacji, w których zmiennych), po drugie, sprawdzamy czy są to błędy (np. wartości niemożliwe), a po trzecie, analizujemy wpływ na wnioski (np. średnie, korelacje, wykresy). Warto pamiętać, że średnia i odchylenie standardowe są wrażliwe, dlatego do EDA często stosuje się medianę i IQR.\nW ML decyzja zależy od modelu i celu. Dla modeli liniowych i dyskryminacyjnych (np. regresja logistyczna, LDA/QDA) odstające wartości mogą znacząco zaburzyć dopasowanie, dlatego typowe strategie to: transformacje (np. \\(\\log\\) dla income_eur), winsoryzacja/obcięcie ogonów, zastosowanie skalowania odpornego (np. RobustScaler), albo świadome usunięcie obserwacji błędnych. Dla drzew i ich zespołów (Random Forest, boosting) pojedyncze ekstremalne wartości często są mniej groźne, ale nadal mogą wpływać na podziały lub na stabilność w małych próbkach. Dla metod odległościowych i SVM skalowanie jest kluczowe: outliery w skali potrafią zdominować metrykę lub margines.\nNajczęściej spotykane „operacyjne” podejścia to:\n\ntransformacja rozkładu (np. \\(\\log\\), Box–Cox/Yeo–Johnson),\nwinsoryzacja: obcięcie do percentyli (np. 1%–99%),\nflaga odstających jako dodatkowa cecha (model uczy się, że to przypadek nietypowy),\nusunięcie tylko wtedy, gdy jest to błąd lub obserwacja spoza domeny problemu.\n\nPrzykład winsoryzacji (percentyle) dla income_eur:\n\n\nKod\np01, p99 = df[\"income_eur\"].quantile([0.01, 0.99])\ndf[\"income_eur_capped\"] = df[\"income_eur\"].clip(lower=p01, upper=p99)\n\n\n\n\n\n3.2.2 Braki danych i imputacja\nBraki danych (missing values) mogą wynikać z problemów pomiaru, integracji źródeł, błędów ETL, ale także z „logiki procesu” (np. klient nie ma ocen satysfakcji, bo nie wypełnił ankiety). W EDA kluczowe jest rozpoznanie: (1) gdzie braki występują, (2) ile ich jest, (3) czy są zależne od innych zmiennych (braki „systematyczne”). W ML brak danych wymaga decyzji, ponieważ większość klasycznych modeli nie obsługuje NaN wprost.\nNa początek warto policzyć braki:\n\n\nKod\nna_counts = df.isna().sum().sort_values(ascending=False)\nna_share = (df.isna().mean().sort_values(ascending=False) * 100).round(2)\npd.DataFrame({\"missing_count\": na_counts, \"missing_%\": na_share})\n\n\n\n\n\n\n\n\n\nmissing_count\nmissing_%\n\n\n\n\nincome_eur_capped\n33\n9.43\n\n\nincome_robust_z\n33\n9.43\n\n\nincome_eur\n33\n9.43\n\n\nsatisfaction_1_5\n23\n6.57\n\n\ndevice_type\n21\n6.00\n\n\navg_basket_eur\n18\n5.14\n\n\nage\n17\n4.86\n\n\ncity\n17\n4.86\n\n\nnotes\n16\n4.57\n\n\nhas_promo\n0\n0.00\n\n\nchurned\n0\n0.00\n\n\ncustomer_id\n0\n0.00\n\n\nreturns_last90\n0\n0.00\n\n\nsignup_date\n0\n0.00\n\n\nsegment\n0\n0.00\n\n\nlast_purchase_date\n0\n0.00\n\n\nvisits_last30\n0\n0.00\n\n\n\n\n\n\n\n\n3.2.2.1 Co robimy z brakami w EDA i ML\nW EDA braki traktujemy jako informację o jakości danych i procesie ich powstawania. Sprawdzamy, czy braki są losowe, czy dotyczą specyficznych segmentów (np. brak income_eur tylko dla jednego segmentu), oraz czy ich usunięcie nie zmieni populacji (bias). W ML trzeba z kolei zadbać o to, aby imputacja była wykonywana bez wycieku informacji: parametry imputacji wyznaczamy na zbiorze treningowym i stosujemy do walidacyjnego/testowego.\nDecyzje praktyczne obejmują: usuwanie cech/wierszy (gdy braków jest bardzo dużo), imputację prostą (średnia/mediana/moda), imputację warunkową (np. mediana w grupach), metody oparte na podobieństwie (k-NN), metody iteracyjne (MICE), a także dodawanie wskaźników braków (missing indicators), które pozwalają modelowi uczyć się samego faktu braku jako sygnału.\n\n\n3.2.2.2 Metody imputacji\n\nImputacja stałą lub modą (kategorie i zmienne dyskretne)\n\nDla zmiennych kategorycznych często stosuje się modę lub specjalną kategorię ‘missing’. Dlaczego to bywa dobre? Ponieważ w wielu danych fakt, że czegoś brakuje, jest informacją samą w sobie. Na przykład: brak miasta może oznaczać, że klient nie podał danych adresowych, co może korelować z innymi cechami lub nawet z churned. Jeśli zamienimy brak na osobną kategorię “missing”, model może się tego „nauczyć”.\n\n\nKod\ndf[\"city\"] = df[\"city\"].fillna(\"missing\")\ndf[\"device_type\"] = df[\"device_type\"].fillna(\"missing\")\n\n\n\nImputacja medianą/średnią (zmienne liczbowe)\n\nJest to metoda szybka, ale ignoruje zależności między cechami:\n\n\nKod\ndf[\"income_eur\"] = df[\"income_eur\"].fillna(df[\"income_eur\"].median())\n\n\n\nImputacja grupowa (warunkowa)\n\nBardzo użyteczna, gdy rozkłady różnią się w segmentach, np. dochody w segmentach VIP vs Student:\n\n\nKod\ndf[\"income_eur\"] = df[\"income_eur\"].fillna(\n    df.groupby(\"segment\")[\"income_eur\"].transform(\"median\")\n)\n\n\n\nk-NN Imputer (podobieństwo obserwacji)\n\nWykorzystuje sąsiadów w przestrzeni cech do uzupełniania braków. Wymaga skalowania i pracy na liczbach.\n\n\nKod\nfrom sklearn.impute import KNNImputer\n\nnum_features = [\"age\", \"income_eur\", \"visits_last30\", \"avg_basket_eur\", \"returns_last90\", \"satisfaction_1_5\"]\nimputer = KNNImputer(n_neighbors=5, weights=\"distance\") # obserwacje imputujące będą ważone odległością\n\ndf[num_features] = imputer.fit_transform(df[num_features])\n\n\n\nIterative Imputer (MICE – imputacja modelowa)\n\nUzupełnia jedną zmienną na podstawie pozostałych (iteracyjnie), co lepiej zachowuje relacje w danych. Domyślną opcją IterativeImputer jest regresja liniowa z regularyzacją typu ridge w ujęciu bayesowskim (sklearn.linear_model.BayesianRidge). Można jednak wybrać inne techniki jako model uzupelniający\n\n\nKod\nfrom sklearn.ensemble import RandomForestRegressor\nimp = IterativeImputer(estimator=RandomForestRegressor(n_estimators=200, random_state=42))\n\n\n\n\nKod\nfrom sklearn.experimental import enable_iterative_imputer  # noqa: F401\nfrom sklearn.impute import IterativeImputer\n\nimp = IterativeImputer(random_state=42, max_iter=20)\ndf[num_features] = imp.fit_transform(df[num_features])\n\n\n\nWskaźniki braków jako cechy\n\nCzasem sam fakt braku jest informacyjny (np. brak satysfakcji może korelować z churn). Można dodać flagi:\n\n\nKod\n# ponieważ wcześniej zmienna income_eur była zastępowana medianami to poniższy kod da same 0 dla income_missing. Podobnie dla zmiennej satisfaction_1_5\ndf[\"income_missing\"] = df[\"income_eur\"].isna().astype(int)\ndf[\"satisfaction_missing\"] = df[\"satisfaction_1_5\"].isna().astype(int)\n\n\nW praktyce (szczególnie w ML) flagi braków często działają dobrze w połączeniu z imputacją, bo pozwalają modelowi rozróżnić „wartość prawdziwą” od „wartości wstawionej”.\n\n\n\n\n\n\nOstrzeżenie\n\n\n\nW modelowaniu nie imputujemy „ręcznie na całym df”, tylko budujemy potok, który dopasowuje imputery na treningu, a potem stosuje na walidacji/teście:\n\n\nKod\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.linear_model import LogisticRegression\n\nX = df.drop(columns=[\"churned\"])\ny = df[\"churned\"]\n\n# Uwaga: pandas może trzymać braki jako `pd.NA` (np. typy StringDtype/boolean).\n# scikit-learn oczekuje braków jako `np.nan` i potrafi się wysypać na `pd.NA`.\nX = X.replace({pd.NA: np.nan})\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n\nnum_cols = [\"age\", \"income_eur\", \"visits_last30\", \"avg_basket_eur\", \"returns_last90\", \"satisfaction_1_5\"]\ncat_cols = [\"city\", \"segment\", \"device_type\", \"has_promo\", \"notes\"]\n\n# (opcjonalnie) upewniamy się, że kategorie są zwykłym `object`, a nie np. `string[python]` z `pd.NA`\nX_train[cat_cols] = X_train[cat_cols].astype(\"object\")\nX_test[cat_cols] = X_test[cat_cols].astype(\"object\")\n\npreprocess = ColumnTransformer(\n    transformers=[\n        (\"num\", Pipeline(steps=[\n            (\"imputer\", SimpleImputer(strategy=\"median\"))\n        ]), num_cols),\n        (\"cat\", Pipeline(steps=[\n            (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n            (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n        ]), cat_cols),\n    ],\n    remainder=\"drop\"\n)\n\nmodel = Pipeline(steps=[\n    (\"prep\", preprocess),\n    (\"clf\", LogisticRegression(max_iter=200))\n])\n\nmodel.fit(X_train, y_train)\n\n\nPipeline(steps=[('prep',\n                 ColumnTransformer(transformers=[('num',\n                                                  Pipeline(steps=[('imputer',\n                                                                   SimpleImputer(strategy='median'))]),\n                                                  ['age', 'income_eur',\n                                                   'visits_last30',\n                                                   'avg_basket_eur',\n                                                   'returns_last90',\n                                                   'satisfaction_1_5']),\n                                                 ('cat',\n                                                  Pipeline(steps=[('imputer',\n                                                                   SimpleImputer(strategy='most_frequent')),\n                                                                  ('onehot',\n                                                                   OneHotEncoder(handle_unknown='ignore'))]),\n                                                  ['city', 'segment',\n                                                   'device_type', 'has_promo',\n                                                   'notes'])])),\n                ('clf', LogisticRegression(max_iter=200))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  Pipeline?Documentation for PipelineiFittedPipeline(steps=[('prep',\n                 ColumnTransformer(transformers=[('num',\n                                                  Pipeline(steps=[('imputer',\n                                                                   SimpleImputer(strategy='median'))]),\n                                                  ['age', 'income_eur',\n                                                   'visits_last30',\n                                                   'avg_basket_eur',\n                                                   'returns_last90',\n                                                   'satisfaction_1_5']),\n                                                 ('cat',\n                                                  Pipeline(steps=[('imputer',\n                                                                   SimpleImputer(strategy='most_frequent')),\n                                                                  ('onehot',\n                                                                   OneHotEncoder(handle_unknown='ignore'))]),\n                                                  ['city', 'segment',\n                                                   'device_type', 'has_promo',\n                                                   'notes'])])),\n                ('clf', LogisticRegression(max_iter=200))])  prep: ColumnTransformer?Documentation for prep: ColumnTransformerColumnTransformer(transformers=[('num',\n                                 Pipeline(steps=[('imputer',\n                                                  SimpleImputer(strategy='median'))]),\n                                 ['age', 'income_eur', 'visits_last30',\n                                  'avg_basket_eur', 'returns_last90',\n                                  'satisfaction_1_5']),\n                                ('cat',\n                                 Pipeline(steps=[('imputer',\n                                                  SimpleImputer(strategy='most_frequent')),\n                                                 ('onehot',\n                                                  OneHotEncoder(handle_unknown='ignore'))]),\n                                 ['city', 'segment', 'device_type', 'has_promo',\n                                  'notes'])]) num['age', 'income_eur', 'visits_last30', 'avg_basket_eur', 'returns_last90', 'satisfaction_1_5']  SimpleImputer?Documentation for SimpleImputerSimpleImputer(strategy='median') cat['city', 'segment', 'device_type', 'has_promo', 'notes']  SimpleImputer?Documentation for SimpleImputerSimpleImputer(strategy='most_frequent')  OneHotEncoder?Documentation for OneHotEncoderOneHotEncoder(handle_unknown='ignore')  LogisticRegression?Documentation for LogisticRegressionLogisticRegression(max_iter=200) \n\n\nTo podejście jest spójne z „dobrą praktyką” w ML: wszystkie transformacje uczące się parametrów (imputacja, skalowanie, kodowanie) są częścią pipeline i nie „podglądają” testu.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Przygotowanie i czyszczenie danych</span>"
    ]
  },
  {
    "objectID": "chapters/02-przygotowanie-danych.html#eda-i-preprocessing-danych",
    "href": "chapters/02-przygotowanie-danych.html#eda-i-preprocessing-danych",
    "title": "3  Przygotowanie i czyszczenie danych",
    "section": "3.3 EDA i preprocessing danych",
    "text": "3.3 EDA i preprocessing danych\nTen rozdział porządkuje praktyczny przebieg pracy „od surowego pliku do macierzy cech gotowej dla modelu”. Najpierw wykonujemy EDA (ang. exploratory data analysis), aby zrozumieć dane (struktura, rozkłady, zależności, potencjalne problemy jakościowe), a dopiero potem przechodzimy do preprocessingu, który ma zapewnić poprawne działanie modeli i powtarzalność całego procesu. Kwestie braków danych i obserwacji odstających są tu jedynie sygnalizowane (szczegółowe strategie były w osobnych podrozdziałach).\n\n3.3.1 Szybki „sanity check” po imporcie\nPo wczytaniu danych pierwszym krokiem jest szybka kontrola struktury: rozmiar zbioru, typy kolumn, podstawowe podsumowania i liczba braków. W tym momencie warto również ujednolicić nazwy kolumn (np. snake_case), aby dalszy kod był czytelny i odporny na błędy.\n\n\nKod\ndf = pd.read_csv(\n    \"data.csv\",\n    na_values=[\"\", \"NA\", \"N/A\", \"null\", \"None\"],\n    parse_dates=[\"signup_date\", \"last_purchase_date\"],\n    encoding=\"utf-8\"\n)\n\ndf.shape\ndf.info()\ndf.head()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 350 entries, 0 to 349\nData columns (total 15 columns):\n #   Column              Non-Null Count  Dtype         \n---  ------              --------------  -----         \n 0   customer_id         350 non-null    int64         \n 1   signup_date         350 non-null    datetime64[ns]\n 2   last_purchase_date  350 non-null    datetime64[ns]\n 3   age                 333 non-null    float64       \n 4   income_eur          317 non-null    float64       \n 5   city                333 non-null    object        \n 6   segment             350 non-null    object        \n 7   device_type         329 non-null    object        \n 8   visits_last30       350 non-null    float64       \n 9   avg_basket_eur      332 non-null    float64       \n 10  returns_last90      350 non-null    float64       \n 11  has_promo           350 non-null    bool          \n 12  satisfaction_1_5    327 non-null    float64       \n 13  churned             350 non-null    int64         \n 14  notes               334 non-null    object        \ndtypes: bool(1), datetime64[ns](2), float64(6), int64(2), object(4)\nmemory usage: 38.8+ KB\n\n\n\n\n\n\n\n\n\ncustomer_id\nsignup_date\nlast_purchase_date\nage\nincome_eur\ncity\nsegment\ndevice_type\nvisits_last30\navg_basket_eur\nreturns_last90\nhas_promo\nsatisfaction_1_5\nchurned\nnotes\n\n\n\n\n0\n100000\n2022-04-08\n2022-10-19\nNaN\n72955.0\nWarszawa\nB2B\ndesktop\n8.0\n39.88\n1.0\nFalse\n4.0\n0\nklient powracający\n\n\n1\n100001\n2024-04-27\n2024-05-13\n52.0\n24863.0\nKraków\nB2C\nmobile\n12.0\n67.80\n1.0\nFalse\n3.0\n0\nbrak uwag\n\n\n2\n100002\n2023-12-18\n2024-04-05\nNaN\n25333.0\nWrocław\nB2C\ndesktop\n6.0\n27.04\n1.0\nTrue\n4.0\n0\nreklamacja\n\n\n3\n100003\n2023-04-26\n2023-10-01\n16.0\nNaN\nŁódź\nB2B\nmobile\n6.0\n149.03\n1.0\nFalse\n3.0\n1\nbrak uwag\n\n\n4\n100004\n2023-04-20\n2024-04-07\n42.0\n27954.0\nWarszawa\nB2C\ndesktop\n7.0\n49.73\n1.0\nTrue\n3.0\n0\nNaN\n\n\n\n\n\n\n\n\n\nKod\n# szybki przegląd braków\n(df.isna().mean().sort_values(ascending=False) * 100).round(2)\n\n\nincome_eur            9.43\nsatisfaction_1_5      6.57\ndevice_type           6.00\navg_basket_eur        5.14\nage                   4.86\ncity                  4.86\nnotes                 4.57\ncustomer_id           0.00\nsignup_date           0.00\nlast_purchase_date    0.00\nsegment               0.00\nvisits_last30         0.00\nreturns_last90        0.00\nhas_promo             0.00\nchurned               0.00\ndtype: float64\n\n\nNa tym etapie sprawdzamy też duplikaty oraz ewentualne naruszenia prostych reguł domenowych (np. ujemne wartości tam, gdzie nie powinny wystąpić). Jeśli widzimy ewidentne błędy, zazwyczaj oznaczamy je do późniejszej korekty, zamiast natychmiast usuwać dane „w ciemno”.\n\n\nKod\ndf.duplicated().sum()\n\n\nnp.int64(0)\n\n\n\n\n3.3.2 EDA - rozkłady i struktura zmiennych\nEDA zaczynamy od uporządkowania typów zmiennych: rozdzielenia zmiennych liczbowych i kategorycznych oraz rozpoznania, które kolumny są zmiennymi docelowymi, identyfikatorami lub metadanymi (np. daty). W naszym data.csv naturalnie wyróżniają się liczby (np. income_eur, avg_basket_eur), kategorie (np. segment, city) i daty.\n\n\nKod\nnum_cols = df.select_dtypes(include=\"number\").columns.tolist()\ncat_cols = df.select_dtypes(include=[\"object\", \"string\", \"bool\"]).columns.tolist()\n\nnum_cols, cat_cols\n\n\n(['customer_id',\n  'age',\n  'income_eur',\n  'visits_last30',\n  'avg_basket_eur',\n  'returns_last90',\n  'satisfaction_1_5',\n  'churned'],\n ['city', 'segment', 'device_type', 'has_promo', 'notes'])\n\n\nW analizie rozkładów dla zmiennych liczbowych bardzo dobre są histogramy z nakładką gęstości oraz wykresy pudełkowe. Warto pamiętać, że część zmiennych może być skośna (np. dochody), więc czasem sensowne jest pokazanie wykresu w skali logarytmicznej.\n\n\nKod\nimport matplotlib.pyplot as plt\n\ndef univariate_view(series, title):\n    fig = plt.figure(figsize=(10, 4))\n    gs = fig.add_gridspec(1, 2, width_ratios=[3, 1])\n\n    ax1 = fig.add_subplot(gs[0, 0])\n    ax1.hist(series.dropna(), bins=30)\n    ax1.set_title(f\"Histogram: {title}\")\n    ax1.set_xlabel(title)\n    ax1.set_ylabel(\"Liczność\")\n\n    ax2 = fig.add_subplot(gs[0, 1])\n    ax2.boxplot(series.dropna(), vert=True)\n    ax2.set_title(\"Boxplot\")\n    ax2.set_xticks([])\n\n    plt.tight_layout()\n    plt.show()\n\nunivariate_view(df[\"income_eur\"], \"income_eur\")\nunivariate_view(df[\"avg_basket_eur\"], \"avg_basket_eur\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDla zmiennych kategorycznych kluczowe jest porównanie częstości. Prosty wykres słupkowy umożliwia identyfikację rzadkich kategorii oraz niespójności kodowania (np. różne wielkości liter, dodatkowe spacje).\n\n\nKod\ndef top_categories(series, title, top=10):\n    counts = series.value_counts(dropna=False).head(top)\n    plt.figure(figsize=(10, 4))\n    plt.bar(counts.index.astype(str), counts.values)\n    plt.title(f\"Top {top} kategorii: {title}\")\n    plt.xticks(rotation=30, ha=\"right\")\n    plt.ylabel(\"Liczność\")\n    plt.tight_layout()\n    plt.show()\n\ntop_categories(df[\"segment\"], \"segment\")\ntop_categories(df[\"device_type\"], \"device_type\")\ntop_categories(df[\"city\"], \"city\", top=12)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.3.3 EDA - zależności między zmiennymi\nW celu odkrycia zależności pomiędzy cechami wykreślamy macierz korelacji.\n\n\nKod\n# macierz korelacji (numeryczne)\ncorr = df[num_cols].corr(numeric_only=True)\n\nfig, ax = plt.subplots(figsize=(8, 6))\nim = ax.imshow(corr, aspect=\"auto\")\nfig.colorbar(im, ax=ax)\n\nax.set_xticks(range(len(corr.columns)))\nax.set_xticklabels(corr.columns, rotation=30, ha=\"right\")\nax.set_yticks(range(len(corr.index)))\nax.set_yticklabels(corr.index)\nax.set_title(\"Macierz korelacji (numeryczne)\")\n\n# wartości korelacji w komórkach\nfor i in range(corr.shape[0]):\n    for j in range(corr.shape[1]):\n        val = corr.iloc[i, j]\n        color = \"black\" if abs(val) &gt;= 0.5 else \"white\"\n        ax.text(j, i, f\"{val:.2f}\", ha=\"center\", va=\"center\", color=color, fontsize=8)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nW kontekście klasyfikacji warto też zobaczyć, jak rozkłady cech różnią się między klasami (tu: churned). Bardzo czytelny jest wykres pudełkowy/wiolinowy „cecha vs klasa” albo porównanie histogramów. Poniżej przykład boxplotu cechy w podziale na churned.\n\n\nKod\ndef box_by_target(df, feature, target=\"churned\"):\n    groups = [df.loc[df[target] == t, feature].dropna() for t in sorted(df[target].dropna().unique())]\n    plt.figure(figsize=(7, 4))\n    plt.boxplot(groups, labels=[f\"{target}={t}\" for t in sorted(df[target].dropna().unique())])\n    plt.title(f\"{feature} względem {target}\")\n    plt.ylabel(feature)\n    plt.tight_layout()\n    plt.show()\n\nbox_by_target(df, \"avg_basket_eur\")\nbox_by_target(df, \"visits_last30\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDla zmiennych kategorycznych przydatne są wykresy udziałów klas w kategoriach (np. wskaźnik churn w segmentach). W prostym wariancie liczymy średnią churned w grupach (to jest odsetek „1” w danej kategorii) i wykreślamy słupki.\n\n\nKod\nrate = df.groupby(\"segment\")[\"churned\"].mean().sort_values(ascending=False)\n\nplt.figure(figsize=(7, 4))\nplt.bar(rate.index.astype(str), rate.values)\nplt.title(\"Średni churn w segmentach\")\nplt.ylabel(\"P(churned=1)\")\nplt.xticks(rotation=20, ha=\"right\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nJeżeli w danych występują daty, dobrze jest wykonać podstawowe przekroje czasowe: liczba rejestracji w czasie, średni koszyk w czasie, itp. Pozwala to zidentyfikować sezonowość, zmiany procesu zbierania danych, kampanie marketingowe.\n\n\nKod\ntmp = df.copy()\ntmp[\"signup_month\"] = tmp[\"signup_date\"].dt.to_period(\"M\").dt.to_timestamp()\n\ncounts = tmp.groupby(\"signup_month\")[\"customer_id\"].count()\n\nplt.figure(figsize=(10, 4))\nplt.plot(counts.index, counts.values)\nplt.title(\"Liczba rejestracji w czasie\")\nplt.ylabel(\"Liczność\")\nplt.xlabel(\"Miesiąc\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nNa tym etapie jedynie notujemy, czy widać: braki danych (np. całe grupy z brakami), wartości podejrzanie ekstremalne lub naruszenia domeny. Szczegółowe postępowanie (imputacja, outliery) realizujemy według strategii opisanych w dedykowanych podrozdziałach.\n\n3.3.3.1 Narzędzia i biblioteki\nW praktycznej EDA najczęściej bazuje się na zestawie narzędzi „rdzeniowych”: pandas służy do przeglądu struktury danych, typów, braków, agregacji i podstawowych statystyk, natomiast matplotlib i seaborn odpowiadają za wizualizacje. matplotlib daje pełną kontrolę nad wykresem i jest dobry do precyzyjnych, publikacyjnych rysunków, a seaborn pozwala szybciej tworzyć estetyczne wykresy statystyczne typowe dla EDA (rozkłady, zależności, porównania grup). Jeżeli zależy Ci na interaktywności (zoom, podgląd wartości, selekcja), warto sięgnąć po plotly albo altair, które dobrze sprawdzają się w notebookach i podczas zajęć, bo ułatwiają „badanie danych w locie”.\nDo szybkiej diagnostyki całego zbioru danych, bez ręcznego pisania wielu wykresów i tabel, przydatne są biblioteki raportujące: ydata-profiling (dawniej pandas-profiling) oraz sweetviz. Generują one raporty HTML z podsumowaniami rozkładów, braków, korelacji i potencjalnych problemów jakościowych, a sweetviz dodatkowo dobrze nadaje się do porównywania zbiorów (np. train vs test) lub analiz w odniesieniu do zmiennej docelowej. Z kolei dtale daje interfejs „jak arkusz kalkulacyjny”, umożliwiając interaktywne filtrowanie i szybkie sprawdzanie danych w przeglądarce, co bywa bardzo wygodne w dydaktyce i debugowaniu.\nJeżeli chcemy rozszerzyć EDA o wątki jakości danych i zależności specyficznych dla danych mieszanych, pomocne są narzędzia wyspecjalizowane: missingno służy do wizualizacji wzorców braków (gdzie i jak współwystępują), great_expectations pozwala formalizować reguły jakości danych i testować je w sposób powtarzalny, a phik bywa użyteczne do badania zależności również wtedy, gdy zmienne są kategoryczne lub relacje są nieliniowe. W praktyce dobry „stos” do Twojego kursu to: pandas + (matplotlib/seaborn) jako fundament, missingno do braków oraz ydata-profiling lub sweetviz do szybkiego raportu całości.\n\n\n\n3.3.4 Preprocessing - przygotowanie macierzy cech do modeli\nPo EDA przechodzimy do preprocessingu, którego cel jest stricte „modelowy”: wytworzyć wejście w formacie akceptowalnym przez algorytmy oraz zapewnić powtarzalność transformacji. Kluczowa zasada brzmi: transformacje, które uczą się parametrów (imputer, scaler, encoder), dopasowujemy na zbiorze treningowym i stosujemy do walidacji/testu – najlepiej w pipeline.\n\n3.3.4.1 Podział na zbiory treningowy i testowy\nPodział na train/test wykonujemy przed dopasowaniem transformacji. Dla klasyfikacji zwykle stosujemy stratyfikację.\n\n\nKod\nX = df.drop(columns=[\"churned\"])\ny = df[\"churned\"]\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\n\n\nJeżeli dodatkowo używamy walidacji, możemy wydzielić X_val z X_train analogicznie albo korzystać z cross-validation.\n\n\n3.3.4.2 Kodowanie kategorii\nWiększość klasycznych modeli (regresja logistyczna, SVM, LDA/QDA) wymaga liczb, więc zmienne kategoryczne kodujemy. Standardem jest one-hot encoding z obsługą nieznanych kategorii w teście.\n\n\n3.3.4.3 Skalowanie zmiennych liczbowych\nSkalowanie jest szczególnie ważne dla metod opartych na odległości i marginesie (k-NN, SVM) oraz dla modeli liniowych przy regularyzacji. Dla drzew i metod zespołowych zwykle nie jest konieczne, ale utrzymywanie jednolitego pipeline’u ułatwia porównania.\n\n\n3.3.4.4 Pipeline preprocessingu\nPoniżej wzorcowa konstrukcja preprocessingu: dla liczb imputacja (tu jako placeholder) + skalowanie; dla kategorii imputacja (placeholder) + one-hot encoding. Strategię imputacji i obsługę outlierów możesz później łatwo podmienić.\n\n\nKod\nfrom sklearn.preprocessing import StandardScaler\n\nnum_cols = [\"age\", \"income_eur\", \"visits_last30\", \"avg_basket_eur\", \"returns_last90\", \"satisfaction_1_5\"]\ncat_cols = [\"city\", \"segment\", \"device_type\", \"has_promo\", \"notes\"]\n\nnumeric_pipe = Pipeline(steps=[\n    (\"imputer\", SimpleImputer(strategy=\"median\")),   # szczegóły w rozdziale o imputacji\n    (\"scaler\", StandardScaler())\n])\n\ncategorical_pipe = Pipeline(steps=[\n    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n])\n\npreprocess = ColumnTransformer(\n    transformers=[\n        (\"num\", numeric_pipe, num_cols),\n        (\"cat\", categorical_pipe, cat_cols),\n    ],\n    remainder=\"drop\"\n)\n\n\nNastępnie łączymy preprocessing z modelem w jeden pipeline. Dzięki temu cały proces jest powtarzalny i bezpieczny względem testu:\n\n\nKod\nclf = Pipeline(steps=[\n    (\"prep\", preprocess),\n    (\"model\", LogisticRegression(max_iter=300))\n])\n\nclf.fit(X_train, y_train)\n\n\nPipeline(steps=[('prep',\n                 ColumnTransformer(transformers=[('num',\n                                                  Pipeline(steps=[('imputer',\n                                                                   SimpleImputer(strategy='median')),\n                                                                  ('scaler',\n                                                                   StandardScaler())]),\n                                                  ['age', 'income_eur',\n                                                   'visits_last30',\n                                                   'avg_basket_eur',\n                                                   'returns_last90',\n                                                   'satisfaction_1_5']),\n                                                 ('cat',\n                                                  Pipeline(steps=[('imputer',\n                                                                   SimpleImputer(strategy='most_frequent')),\n                                                                  ('onehot',\n                                                                   OneHotEncoder(handle_unknown='ignore'))]),\n                                                  ['city', 'segment',\n                                                   'device_type', 'has_promo',\n                                                   'notes'])])),\n                ('model', LogisticRegression(max_iter=300))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  Pipeline?Documentation for PipelineiFittedPipeline(steps=[('prep',\n                 ColumnTransformer(transformers=[('num',\n                                                  Pipeline(steps=[('imputer',\n                                                                   SimpleImputer(strategy='median')),\n                                                                  ('scaler',\n                                                                   StandardScaler())]),\n                                                  ['age', 'income_eur',\n                                                   'visits_last30',\n                                                   'avg_basket_eur',\n                                                   'returns_last90',\n                                                   'satisfaction_1_5']),\n                                                 ('cat',\n                                                  Pipeline(steps=[('imputer',\n                                                                   SimpleImputer(strategy='most_frequent')),\n                                                                  ('onehot',\n                                                                   OneHotEncoder(handle_unknown='ignore'))]),\n                                                  ['city', 'segment',\n                                                   'device_type', 'has_promo',\n                                                   'notes'])])),\n                ('model', LogisticRegression(max_iter=300))])  prep: ColumnTransformer?Documentation for prep: ColumnTransformerColumnTransformer(transformers=[('num',\n                                 Pipeline(steps=[('imputer',\n                                                  SimpleImputer(strategy='median')),\n                                                 ('scaler', StandardScaler())]),\n                                 ['age', 'income_eur', 'visits_last30',\n                                  'avg_basket_eur', 'returns_last90',\n                                  'satisfaction_1_5']),\n                                ('cat',\n                                 Pipeline(steps=[('imputer',\n                                                  SimpleImputer(strategy='most_frequent')),\n                                                 ('onehot',\n                                                  OneHotEncoder(handle_unknown='ignore'))]),\n                                 ['city', 'segment', 'device_type', 'has_promo',\n                                  'notes'])]) num['age', 'income_eur', 'visits_last30', 'avg_basket_eur', 'returns_last90', 'satisfaction_1_5']  SimpleImputer?Documentation for SimpleImputerSimpleImputer(strategy='median')  StandardScaler?Documentation for StandardScalerStandardScaler() cat['city', 'segment', 'device_type', 'has_promo', 'notes']  SimpleImputer?Documentation for SimpleImputerSimpleImputer(strategy='most_frequent')  OneHotEncoder?Documentation for OneHotEncoderOneHotEncoder(handle_unknown='ignore')  LogisticRegression?Documentation for LogisticRegressionLogisticRegression(max_iter=300)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Przygotowanie i czyszczenie danych</span>"
    ]
  },
  {
    "objectID": "chapters/02-przygotowanie-danych.html#inżynieria-cech",
    "href": "chapters/02-przygotowanie-danych.html#inżynieria-cech",
    "title": "3  Przygotowanie i czyszczenie danych",
    "section": "3.4 Inżynieria cech",
    "text": "3.4 Inżynieria cech\nInżynieria cech (feature engineering) to etap, w którym surowe dane przekształcamy w reprezentację bardziej użyteczną dla modeli uczenia maszynowego. Jej celem nie jest „upiększanie” danych, lecz zwiększenie ilości informacji, jaką model może efektywnie wykorzystać, oraz dopasowanie formatu cech do wymagań algorytmów. W praktyce inżynieria cech obejmuje zarówno proste transformacje (np. logarytmowanie zmiennej skośnej), jak i budowę cech pochodnych, interakcji, agregacji czy cech czasowych. W tym kursie inżynieria cech jest szczególnie istotna, ponieważ omawiane modele klasyczne (regresja logistyczna, LDA/QDA, SVM, k-NN) są wrażliwe na sposób reprezentacji danych i często korzystają bardziej z dobrze zaprojektowanych cech niż z samej „mocy” algorytmu.\n\n3.4.1 Ujednolicanie i czyszczenie kategorii jako element inżynierii cech\nZanim zakodujemy kategorie, warto je znormalizować, ponieważ w danych rzeczywistych często występują spacje, różne wielkości liter oraz warianty zapisu tej samej kategorii. To nie jest „czyszczenie kosmetyczne” – bez tego model będzie traktował np. Mobile i mobile jako różne wartości, co rozbije informację na wiele sztucznych kategorii.\n\n\nKod\n# ujednolicenie prostych kolumn kategorycznych\nfor col in [\"city\", \"segment\", \"device_type\", \"notes\"]:\n    df[col] = (df[col]\n               .astype(\"string\")\n               .str.strip()\n               .str.lower())\n\n# przykładowo: ujednolicenie nazw urządzeń\ndf[\"device_type\"] = df[\"device_type\"].replace({\"mobile\": \"mobile\", \"desktop\": \"desktop\", \"tablet\": \"tablet\"})\n\n\nW wielu projektach opłaca się również „skleić” rzadkie kategorie do wspólnej kategorii other, aby ograniczyć wymiar po kodowaniu one-hot.\n\n\nKod\ntop_cities = df[\"city\"].value_counts().head(6).index\ndf[\"city_reduced\"] = df[\"city\"].where(df[\"city\"].isin(top_cities), other=\"other\")\n\n\n\n\n3.4.2 Cechy czasowe z dat\nDaty rzadko trafiają do modelu w postaci surowej. Zwykle tworzy się z nich cechy o sensie behawioralnym: czas od zdarzenia, miesiąc, dzień tygodnia, czy wskaźniki sezonowe. Dla danych klient–zakupy naturalne są cechy typu tenure oraz recency.\nNiech „datą odniesienia” będzie np. maksymalna obserwowana data zakupu (w realnym projekcie byłby to moment as-of).\n\n\nKod\nimport pandas as pd\n\nas_of = df[\"last_purchase_date\"].max()\n\ndf[\"tenure_days\"] = (as_of - df[\"signup_date\"]).dt.days\ndf[\"recency_days\"] = (as_of - df[\"last_purchase_date\"]).dt.days\n\n# sezonowość i kalendarz\ndf[\"signup_month\"] = df[\"signup_date\"].dt.month\ndf[\"signup_dow\"] = df[\"signup_date\"].dt.dayofweek  # 0=pon, 6=niedz\n\n\nTakie cechy są zwykle dużo bardziej informacyjne dla klasyfikacji churn niż same daty, ponieważ mają bezpośredni związek z aktywnością klienta.\n\n\n3.4.3 Transformacje rozkładów\nW danych ekonomicznych i behawioralnych (np. income_eur, avg_basket_eur) rozkłady są często prawoskośne. Dla modeli liniowych oraz metod odległościowych zyskujemy, gdy przekształcimy skalę tak, aby relacje były bardziej „liniowe” i mniej zdominowane przez ogon rozkładu.\nStandardowa transformacja logarytmiczna (z zabezpieczeniem na zera) to:\n\\[\nx' = \\log(1 + x).\n\\]\n\n\nKod\nimport numpy as np\n\ndf[\"log_income\"] = np.log1p(df[\"income_eur\"])\ndf[\"log_avg_basket\"] = np.log1p(df[\"avg_basket_eur\"])\ndf[\"log_visits\"] = np.log1p(df[\"visits_last30\"])\n\n\nW praktyce często lepsze jest utrzymywanie zarówno cechy pierwotnej, jak i przekształconej – modele drzewiaste zwykle skorzystają z obu, a modele liniowe częściej z wersji log.\n\n\n3.4.4 Cechy intensywności i „normalizacja przez czas” (rate features)\nTypowym zabiegiem jest tworzenie cech typu „na jednostkę czasu”. Jeśli klient jest w systemie krótko, jego liczby bezwzględne mogą być nieporównywalne do klienta z długim stażem. Stąd tworzy się wskaźniki intensywności:\n\n\nKod\n# zabezpieczenie przed dzieleniem przez 0\ndf[\"tenure_days_safe\"] = df[\"tenure_days\"].clip(lower=1)\n\ndf[\"visits_per_day\"] = df[\"visits_last30\"] / 30.0\ndf[\"returns_rate_90\"] = df[\"returns_last90\"] / 90.0\ndf[\"visits_per_tenure\"] = df[\"visits_last30\"] / df[\"tenure_days_safe\"]\n\n\nTe cechy są szczególnie wartościowe w klasycznych modelach liniowych, bo stabilizują skalę i często lepiej korelują z decyzją niż wartości surowe.\n\n\n3.4.5 Interakcje i cechy „biznesowe” z sensowną interpretacją\nInterakcje to bardzo ważna klasa cech w klasycznych modelach. Jeżeli nie używasz modeli, które „same” łatwo uczą się złożonych interakcji (np. boosting), to możesz jawnie dodać kilka logicznych interakcji. Przykładowo: wartość koszyka może działać inaczej dla segmentu VIP, a liczba wizyt może inaczej wpływać na churn, gdy satysfakcja jest niska.\n\n\nKod\n# przykładowe interakcje liczbowe\ndf[\"basket_x_visits\"] = df[\"avg_basket_eur\"] * df[\"visits_last30\"]\ndf[\"income_x_visits\"] = df[\"income_eur\"] * df[\"visits_last30\"]\n\n\nMożemy również budować interakcje „warunkowe”, np. flaga „niska satysfakcja” i interakcja z wizytami:\n\n\nKod\ndf[\"low_satisfaction\"] = (df[\"satisfaction_1_5\"] &lt;= 2).astype(int)\ndf[\"visits_if_low_satisfaction\"] = df[\"visits_last30\"] * df[\"low_satisfaction\"]\n\n\nTo jest szczególnie dydaktyczne, bo pokazuje, jak złożone hipotezy można przenieść do prostego modelu.\n\n\n3.4.6 Grupowanie zmiennych ciągłych do przedziałów (binning)\nCzasem warto przekształcić zmienną ciągłą do kategorii, zwłaszcza gdy zależność jest progowa. Przykładowo: recency_days może mieć silny efekt dopiero po przekroczeniu pewnego progu. Binning bywa użyteczny też w analizie dyskryminacyjnej i prostych modelach interpretowalnych.\n\n\nKod\ndf[\"recency_bin\"] = pd.cut(\n    df[\"recency_days\"],\n    bins=[-1, 7, 30, 90, 180, 365, 10_000],\n    labels=[\"&lt;=7\", \"8-30\", \"31-90\", \"91-180\", \"181-365\", \"&gt;365\"]\n)\n\n\n\n\n3.4.7 Kodowanie kategorii pod modele\nNajbardziej standardowym kodowaniem jest one-hot encoding. Możemy je zrobić w pandas, ale w modelowaniu lepszy jest pipeline w sklearn. Dla celów EDA i demonstracji:\n\n\nKod\nX_cat = pd.get_dummies(df[[\"segment\", \"device_type\", \"city_reduced\"]], dummy_na=True)\nX_cat.head()\n\n\n\n\n\n\n\n\n\nsegment_b2b\nsegment_b2c\nsegment_student\nsegment_vip\nsegment_&lt;NA&gt;\ndevice_type_desktop\ndevice_type_mobile\ndevice_type_tablet\ndevice_type_&lt;NA&gt;\ncity_reduced_katowice\ncity_reduced_kraków\ncity_reduced_lublin\ncity_reduced_other\ncity_reduced_poznań\ncity_reduced_wrocław\ncity_reduced_łódź\ncity_reduced_&lt;NA&gt;\n\n\n\n\n0\nTrue\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\n\n\n1\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n2\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\n\n\n3\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n4\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\n\n\n\n\n\n\n\n\n\n3.4.8 Składanie cech do macierzy modelowej\nPo inżynierii cech zwykle wybieramy zestaw kolumn do modelu. Na potrzeby naszych klasycznych algorytmów rozsądne jest przygotowanie „bazowego” zestawu cech liczbowych oraz zakodowanych kategorii.\n\n\nKod\nfeature_cols_num = [\n    \"tenure_days\", \"recency_days\",\n    \"income_eur\", \"avg_basket_eur\", \"visits_last30\", \"returns_last90\",\n    \"log_income\", \"log_avg_basket\", \"log_visits\",\n    \"visits_per_day\", \"returns_rate_90\",\n    \"low_satisfaction\", \"basket_x_visits\"\n]\n\nfeature_cols_cat = [\"segment\", \"device_type\", \"city_reduced\", \"recency_bin\"]\n\nX_num = df[feature_cols_num]\nX_cat = pd.get_dummies(df[feature_cols_cat], dummy_na=True)\n\nX = pd.concat([X_num, X_cat], axis=1)\ny = df[\"churned\"]\n\n\n\n\n\n\n\n\nAdnotacja\n\n\n\nPrzedstawione powyżej przykłady inżynierii cech mają charakter ilustracyjny i służą pokazaniu typowych mechanizmów oraz kierunków pracy z danymi. W kontekście konkretnego problemu analitycznego część tych kroków może zostać zmodyfikowana, uproszczona lub całkowicie pominięta, a inne – niewystępujące w przykładach – mogą okazać się kluczowe. Inżynieria cech nie jest zestawem sztywnych reguł, lecz procesem zależnym od charakteru danych, celu analizy oraz stosowanego modelu.\nW praktyce najlepszym podejściem jest realizowanie inżynierii cech w sposób systematyczny i powtarzalny, z wykorzystaniem pipeline’ów. Pozwala to połączyć imputację, skalowanie, kodowanie oraz konstrukcję cech w jeden spójny proces, który jest dopasowywany wyłącznie na zbiorze treningowym i następnie stosowany do walidacji oraz testu. Takie podejście minimalizuje ryzyko wycieku informacji, ułatwia porównywanie modeli oraz sprawia, że eksperymenty analityczne są w pełni reprodukowalne.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Przygotowanie i czyszczenie danych</span>"
    ]
  },
  {
    "objectID": "chapters/03-klasyfikacja-liniowa.html",
    "href": "chapters/03-klasyfikacja-liniowa.html",
    "title": "4  Klasyfikacja liniowa",
    "section": "",
    "text": "4.1 Regresja logistyczna",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Klasyfikacja liniowa</span>"
    ]
  },
  {
    "objectID": "chapters/03-klasyfikacja-liniowa.html#regresja-logistyczna",
    "href": "chapters/03-klasyfikacja-liniowa.html#regresja-logistyczna",
    "title": "4  Klasyfikacja liniowa",
    "section": "",
    "text": "Regresja logistyczna to algorytm klasyfikacji binarnej wykorzystujacy funkcje sigmoidalna do estymacji prawdopodobienstwa przynaleznosci do klasy.\nJest to metoda nadzorowana, w ktorej logit jest funkcja liniowa predyktorow.\nZalozenia: niezaleznosc obserwacji, dwumianowa zmienna zalezna, liniowa relacja predyktorow z log-ilorazem szans, brak silnych odstepstw i dostatecznie duza proba.\nInterpretacja wspolczynnikow, ocena jakosci (funkcja kosztu, AUC), ograniczenia przy duzej nakladce klas lub wielu cechach.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Klasyfikacja liniowa</span>"
    ]
  },
  {
    "objectID": "chapters/03-klasyfikacja-liniowa.html#dyskryminacja-liniowa-i-kwadratowa",
    "href": "chapters/03-klasyfikacja-liniowa.html#dyskryminacja-liniowa-i-kwadratowa",
    "title": "4  Klasyfikacja liniowa",
    "section": "4.2 Dyskryminacja liniowa i kwadratowa",
    "text": "4.2 Dyskryminacja liniowa i kwadratowa\n\nLDA szuka liniowych kombinacji predyktorow maksymalizujacych separacje klas; zaklada normalny rozklad cech i identyczne macierze kowariancji.\nQDA dopuszcza rozne macierze kowariancji, przez co jest bardziej elastyczna.\nWarianty:\n\nRDA: regularyzacja (skurcz) estymacji macierzy kowariancji.\nMDA: kazda klasa jako mieszanka kilku gaussowskich podklas.\nFDA: nieliniowe kombinacje predyktorow, np. funkcje sklejane.\n\nDyskryminacja bywa korzystniejsza od regresji logistycznej dla problemow wieloklasowych i roznych rozkladow cech.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Klasyfikacja liniowa</span>"
    ]
  },
  {
    "objectID": "chapters/04-drzewa-zespoly.html",
    "href": "chapters/04-drzewa-zespoly.html",
    "title": "5  Drzewa decyzyjne i zespoly modeli",
    "section": "",
    "text": "5.1 Podstawowe drzewa decyzyjne",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Drzewa decyzyjne i zespoly modeli</span>"
    ]
  },
  {
    "objectID": "chapters/04-drzewa-zespoly.html#podstawowe-drzewa-decyzyjne",
    "href": "chapters/04-drzewa-zespoly.html#podstawowe-drzewa-decyzyjne",
    "title": "5  Drzewa decyzyjne i zespoly modeli",
    "section": "",
    "text": "Drzewo decyzyjne dzieli przestrzen cech rekurencyjnie wedlug kryterium czystosci wezla (np. entropia lub indeks Gini).\nProblem przeuczenia i techniki przycinania.\nAlgorytmy: ID3, C4.5, CART.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Drzewa decyzyjne i zespoly modeli</span>"
    ]
  },
  {
    "objectID": "chapters/04-drzewa-zespoly.html#bagging-i-lasy-losowe",
    "href": "chapters/04-drzewa-zespoly.html#bagging-i-lasy-losowe",
    "title": "5  Drzewa decyzyjne i zespoly modeli",
    "section": "5.2 Bagging i lasy losowe",
    "text": "5.2 Bagging i lasy losowe\n\nBagging (bootstrap aggregating) tworzy wiele modeli na probkach losowanych z powtorzeniem i laczy ich wyniki, zmniejszajac wariancje.\nLas losowy (random forest) to bagging z dodatkowym losowaniem cech przy kazdym podziale.\nParametry: liczba drzew, liczba losowanych cech, ocena waznosci cech.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Drzewa decyzyjne i zespoly modeli</span>"
    ]
  },
  {
    "objectID": "chapters/04-drzewa-zespoly.html#boosting",
    "href": "chapters/04-drzewa-zespoly.html#boosting",
    "title": "5  Drzewa decyzyjne i zespoly modeli",
    "section": "5.3 Boosting",
    "text": "5.3 Boosting\n\nBoosting to sekwencyjne tworzenie modeli, gdzie kazdy kolejny koncentruje sie na poprawie bledow poprzednikow.\nAdaBoost uzywa wag przykladów do korekty bledow.\nGradient Boosting minimalizuje funkcje bledu w oparciu o gradient; XGBoost to wydajna implementacja z mozliwoscia rownoleglosci.\nCatBoost obsluguje natywnie zmienne kategoryczne i redukuje przeuczenie.\nRoznice miedzy baggingiem a boostingiem oraz ryzyko nadmiernego dopasowania.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Drzewa decyzyjne i zespoly modeli</span>"
    ]
  },
  {
    "objectID": "chapters/05-bayes.html",
    "href": "chapters/05-bayes.html",
    "title": "6  Klasyfikatory bayesowskie",
    "section": "",
    "text": "6.1 Teoria Bayesa i dwa podejscia estymacji",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Klasyfikatory bayesowskie</span>"
    ]
  },
  {
    "objectID": "chapters/05-bayes.html#teoria-bayesa-i-dwa-podejscia-estymacji",
    "href": "chapters/05-bayes.html#teoria-bayesa-i-dwa-podejscia-estymacji",
    "title": "6  Klasyfikatory bayesowskie",
    "section": "",
    "text": "Twierdzenie Bayesa oraz pojecia: rozklad aprioryczny i a posteriori.\nMLE maksymalizuje prawdopodobienstwo danych przy danym parametrze.\nMAP maksymalizuje iloczyn prawdopodobienstwa i rozkladu apriorycznego, co wprowadza regularizacje.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Klasyfikatory bayesowskie</span>"
    ]
  },
  {
    "objectID": "chapters/05-bayes.html#klasyfikator-mapml",
    "href": "chapters/05-bayes.html#klasyfikator-mapml",
    "title": "6  Klasyfikatory bayesowskie",
    "section": "6.2 Klasyfikator MAP/ML",
    "text": "6.2 Klasyfikator MAP/ML\n\nNaiwny Bayes w wariancie ML (parametry z estymacji wiarygodnosci).\nNaiwny Bayes w wariancie MAP (parametry z uwzglednieniem priory).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Klasyfikatory bayesowskie</span>"
    ]
  },
  {
    "objectID": "chapters/05-bayes.html#naive-bayes",
    "href": "chapters/05-bayes.html#naive-bayes",
    "title": "6  Klasyfikatory bayesowskie",
    "section": "6.3 Naive Bayes",
    "text": "6.3 Naive Bayes\n\nProbabilistyczny algorytm oparty na twierdzeniu Bayesa z zalozeniem niezaleznosci cech.\nSkuteczny w klasyfikacji tekstu, filtracji spamu i analizie uczuc.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Klasyfikatory bayesowskie</span>"
    ]
  },
  {
    "objectID": "chapters/06-knn.html",
    "href": "chapters/06-knn.html",
    "title": "7  k-NN",
    "section": "",
    "text": "k-NN to nieparametryczny algorytm, ktory klasyfikuje obserwacje na podstawie wiekszosci klasy wśród k najblizszych sasiadow.\nAlgorytm przechowuje dane treningowe i wykonuje obliczenia w momencie predykcji.\nWybor metryki odleglosci (Euclidesowa, Manhattan) i liczby sasiadow wplywa na dokladnosc i zlozonosc obliczeniowa.\nWady: wysoki koszt obliczen i wrazliwosc na skale cech.\nZalety: prostota i brak zalozen o rozkladzie danych.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>k-NN</span>"
    ]
  },
  {
    "objectID": "chapters/07-splajny-gam.html",
    "href": "chapters/07-splajny-gam.html",
    "title": "8  Modele addytywne i splajny",
    "section": "",
    "text": "8.1 Regresje sklejane (splajny)",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Modele addytywne i splajny</span>"
    ]
  },
  {
    "objectID": "chapters/07-splajny-gam.html#regresje-sklejane-splajny",
    "href": "chapters/07-splajny-gam.html#regresje-sklejane-splajny",
    "title": "8  Modele addytywne i splajny",
    "section": "",
    "text": "Splajny regresyjne to wielomianowa regresja kawalkowa; przedzialy rozdzielone sa wezlami, a wielomiany lacza sie gladko na wezlach.\nLiczba wezlow steruje elastycznoscia i ryzykiem przeuczenia.\nSplajny sluza do modelowania nieliniowosci, m.in. w FDA i GAM.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Modele addytywne i splajny</span>"
    ]
  },
  {
    "objectID": "chapters/07-splajny-gam.html#generalized-additive-models-gam",
    "href": "chapters/07-splajny-gam.html#generalized-additive-models-gam",
    "title": "8  Modele addytywne i splajny",
    "section": "8.2 Generalized Additive Models (GAM)",
    "text": "8.2 Generalized Additive Models (GAM)\n\nGAM sumuja wygładzone funkcje zmiennych (np. splajny), co pozwala modelowac zlozone relacje.\nW porownaniu z regresja liniowa GAM nie wymaga scisle liniowego zwiazku, ale jest bardziej zlozony.\nZalety: elastycznosc, interpretowalnosc, uchwycenie nieliniowosci.\nWady: wieksza zlozonosc obliczeniowa i wrazliwosc na parametry wygladzania.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Modele addytywne i splajny</span>"
    ]
  },
  {
    "objectID": "chapters/08-svm.html",
    "href": "chapters/08-svm.html",
    "title": "9  Metoda SVM",
    "section": "",
    "text": "Support Vector Machines (SVM) szukaja hiperpłaszczyzny maksymalizujacej margines miedzy klasami.\nKluczowe pojecia: hiperpłaszczyzna, wektory nosne, margines i jadra.\nTwardy margines wymaga braku bledow klasyfikacji, miekkim marginesem steruje parametr regularyzacji.\nTypy jader: liniowe, wielomianowe, RBF.\nZalety: skutecznosc w danych wysokowymiarowych i przy malych probach.\nWady: koniecznosc doboru parametrow i wrazliwosc na skale cech.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Metoda SVM</span>"
    ]
  },
  {
    "objectID": "chapters/09-dalsze-zagadnienia.html",
    "href": "chapters/09-dalsze-zagadnienia.html",
    "title": "10  Inne metody",
    "section": "",
    "text": "10.1 Reinforcement learning",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Inne metody</span>"
    ]
  },
  {
    "objectID": "chapters/09-dalsze-zagadnienia.html#reinforcement-learning",
    "href": "chapters/09-dalsze-zagadnienia.html#reinforcement-learning",
    "title": "10  Inne metody",
    "section": "",
    "text": "Uczenie ze wzmocnieniem: agent uczy sie przez interakcje ze srodowiskiem, maksymalizujac sume przyszlych nagrod.\nElementy: agent, srodowisko, stan, akcja, nagroda, polityka i funkcja wartosci; modelowane jako zadanie Markowa.\nSzczegolowe algorytmy (Q-learning, SARSA, aktor-krytyk) omawiane sa w innym kursie.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Inne metody</span>"
    ]
  },
  {
    "objectID": "chapters/09-dalsze-zagadnienia.html#modele-koszykowe-market-basket-analysis",
    "href": "chapters/09-dalsze-zagadnienia.html#modele-koszykowe-market-basket-analysis",
    "title": "10  Inne metody",
    "section": "10.2 Modele koszykowe (market basket analysis)",
    "text": "10.2 Modele koszykowe (market basket analysis)\n\nAnaliza koszykowa wykrywa wzorce zakupowe i produkty kupowane razem.\nReguly asocjacyjne “jezeli-to”: antecedent i consequent.\nMiary: support, confidence i lift.\nAlgorytmy: Apriori, FP-Growth.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Inne metody</span>"
    ]
  },
  {
    "objectID": "chapters/09-dalsze-zagadnienia.html#wykrywanie-anomalii-ponowne",
    "href": "chapters/09-dalsze-zagadnienia.html#wykrywanie-anomalii-ponowne",
    "title": "10  Inne metody",
    "section": "10.3 Wykrywanie anomalii (ponowne)",
    "text": "10.3 Wykrywanie anomalii (ponowne)\n\nPrzypomnienie definicji anomalii w kontekście oszustw, awarii i bezpieczenstwa.\nTypy anomalii: punktowe, kontekstowe, zbiorowe.\nMetody detekcji omawiane sa szczegolowo w innych kursach.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Inne metody</span>"
    ]
  },
  {
    "objectID": "chapters/10-podsumowanie.html",
    "href": "chapters/10-podsumowanie.html",
    "title": "11  Podsumowanie",
    "section": "",
    "text": "W tym wykladzie przeszlismy od rozroznienia eksploracji danych i uczenia maszynowego, przez przygotowanie danych, do przegladu algorytmow uczenia nadzorowanego. Kolejnosc tematow wspiera stopniowe budowanie wiedzy: definicje, modele liniowe i dyskryminacyjne, drzewa i zespoly, klasyfikatory bayesowskie, k-NN, modele addytywne, SVM, a nastepnie krotkie wprowadzenie do reinforcement learning, analiz koszykowych i detekcji anomalii.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Podsumowanie</span>"
    ]
  }
]