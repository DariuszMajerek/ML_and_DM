---
title: "k-NN"
---

## Metoda k-NN - intuicja i formalizacja

Metoda k-NN jest klasycznym przykładem podejścia nieparametrycznego i *instance-based learning* - nie budujemy jawnej postaci funkcji decyzyjnej podczas treningu, lecz przechowujemy zbiór uczący i podejmujemy decyzję dopiero w momencie predykcji. W sensie uczenia nadzorowanego dane treningowe $\{(x_i,y_i)\}_{i=1}^n$ pełnią rolę „pamięci”, a nową obserwację $x$ klasyfikujemy (lub przewidujemy wartość) na podstawie najbliższych obserwacji w przestrzeni cech.

Formalnie wybieramy metrykę odległości $d(\cdot,\cdot)$. Dla każdej obserwacji testowej $x$ wyznaczamy zbiór indeksów $N_k(x)$ odpowiadający $k$ najbliższym sąsiadom:

$$
N_k(x)=\operatorname{arg\,min}_{S\subset\{1,\dots,n\},\,|S|=k} \sum_{i\in S} d(x,x_i).
$$

W praktyce jest to po prostu wybór $k$ najmniejszych wartości z listy $\{d(x,x_i)\}_{i=1}^n$.

### Metryki odległości

Najczęściej stosujemy metryki typu Minkowskiego:

$$
d_p(x,z)=\left(\sum_{j=1}^p |x_j-z_j|^p\right)^{1/p}.
$$

Szczególne przypadki:

- $p=2$ - odległość euklidesowa,
- $p=1$ - odległość Manhattan,
- $p\to\infty$ - odległość Czebyszewa $d_\infty(x,z)=\max_j |x_j-z_j|$.

W praktyce wybór metryki i skali cech jest krytyczny, ponieważ k-NN opiera się bezpośrednio na geometrii przestrzeni cech.

### Reguła decyzyjna: klasyfikacja i regresja

#### Klasyfikacja

Dla klasyfikacji wieloklasowej $y\in\{1,\dots,K\}$ predykcję definiujemy przez głosowanie większościowe wśród sąsiadów:

$$
\hat y(x)=\arg\max_{c\in\{1,\dots,K\}} \sum_{i\in N_k(x)} \mathbb{1}\{y_i=c\}.
$$

Często stosujemy wersję ważoną odległością (bliżsi sąsiedzi mają większy wpływ), np. z wagami:

$$
w_i(x)=\frac{1}{d(x,x_i)+\varepsilon},
$$

i wtedy:

$$
\hat y(x)=\arg\max_{c} \sum_{i\in N_k(x)} w_i(x)\,\mathbb{1}\{y_i=c\}.
$$

#### Regresja

W regresji $y\in\mathbb{R}$ najprostsza wersja to średnia wśród sąsiadów:

$$
\hat y(x)=\frac{1}{k}\sum_{i\in N_k(x)} y_i,
$$

a wersja ważona:

$$
\hat y(x)=\frac{\sum_{i\in N_k(x)} w_i(x) y_i}{\sum_{i\in N_k(x)} w_i(x)}.
$$

### Rola parametru $k$ - kompromis bias–variance

Parametr $k$ kontroluje gładkość granicy decyzyjnej i jest klasycznym przykładem kompromisu bias–variance.

- Dla małego $k$ (np. 1–5) otrzymujemy model bardzo elastyczny - granice są poszarpane, *bias* jest niski, a wariancja wysoka (łatwiej dopasowujemy szum).
- Dla dużego $k$ otrzymujemy granice gładkie - rośnie *bias* (uśredniamy lokalne struktury), ale maleje wariancja.

W praktyce $k$ dobieramy przez walidację krzyżową, a w danych wielowymiarowych potrzebujemy dodatkowo zadbać o metrykę, ważenie oraz selekcję/skalę cech.

### Zalety i wady k-NN

#### Zalety 

k-NN jest prosty koncepcyjnie, nie zakłada konkretnej postaci funkcji decyzyjnej (nieparametryczny), potrafi modelować nieliniowe granice decyzyjne i często działa dobrze jako baseline. Jest też naturalny wtedy, gdy „podobieństwo” obserwacji ma sens merytoryczny i chcemy bezpośrednio na nim oprzeć decyzję.

#### Wady

k-NN jest wrażliwy na skalę cech (zwykle musimy standaryzować), na obecność nieistotnych cech oraz na dobór metryki. Jest też kosztowny obliczeniowo w predykcji (dla dużego $n$ liczymy wiele odległości), chyba że stosujemy struktury przyspieszające (*KD-tree, Ball-tree*) lub wyszukiwanie przybliżone. Wreszcie, k-NN jest podatny na *curse of dimensionality* - w przestrzeni wielowymiarowej „wszyscy są daleko”, a pojęcie bliskości traci sens.

### Wpływ $k$ na brzeg decyzyjny

```{python}
import numpy as np
import matplotlib.pyplot as plt

from sklearn.datasets import make_moons
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler

# 1) Dane 2D: dwie klasy (nieliniowa separacja)
X, y = make_moons(n_samples=800, noise=0.35, random_state=44)

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.25, random_state=42, stratify=y
)

def plot_decision_boundary(ax, model, X, y, title):
    # siatka
    x_min, x_max = X[:, 0].min() - 0.8, X[:, 0].max() + 0.8
    y_min, y_max = X[:, 1].min() - 0.8, X[:, 1].max() + 0.8
    xx, yy = np.meshgrid(
        np.linspace(x_min, x_max, 400),
        np.linspace(y_min, y_max, 400)
    )
    grid = np.c_[xx.ravel(), yy.ravel()]
    Z = model.predict(grid).reshape(xx.shape)

    ax.contourf(xx, yy, Z, alpha=0.25)
    ax.scatter(X[:, 0], X[:, 1], c=y, s=20, alpha=0.85)
    ax.set_title(title)
    ax.set_xlabel("x1")
    ax.set_ylabel("x2")

ks = [5, 20, 100]

fig, axes = plt.subplots(1, 3, figsize=(15, 4), constrained_layout=True)

for ax, k in zip(axes, ks):
    knn = Pipeline(steps=[
        ("scaler", StandardScaler()),   # k-NN jest wrażliwy na skalę
        ("model", KNeighborsClassifier(n_neighbors=k))
    ])
    knn.fit(X_train, y_train)
    plot_decision_boundary(ax, knn, X_test, y_test, f"k-NN: k={k} (brzeg decyzyjny)")

plt.show()
```


## Przykład

Poniżej stosujemy k-NN na danych `mstz/breast` (konfiguracja `cancer`). Dobieramy $k$, metrykę i ważenie przez walidację krzyżową, a następnie oceniamy model na zbiorze testowym (metryki + macierz pomyłek + ROC). Ponieważ k-NN opiera się na odległościach, w pipeline stosujemy standaryzację.

```{python}
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from datasets import load_dataset

from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.neighbors import KNeighborsClassifier

from sklearn.metrics import (
    confusion_matrix, ConfusionMatrixDisplay,
    accuracy_score, balanced_accuracy_score, f1_score,
    roc_auc_score, roc_curve
)

# 1) Dane z Hugging Face
ds = load_dataset("mstz/breast", "cancer")["train"]
df = ds.to_pandas()

target = "is_cancer"
y = df[target].astype(int)
X = df.drop(columns=[target])

# 2) Podział train/test
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.25, random_state=42, stratify=y
)

# 3) Pipeline k-NN (imputacja + standaryzacja + model)
pipe = Pipeline(steps=[
    ("imp", SimpleImputer(strategy="median")),
    ("scaler", StandardScaler()),
    ("knn", KNeighborsClassifier())
])

# 4) Dobór k przez CV
param_grid = {
    "knn__n_neighbors": [1, 3, 5, 7, 9, 15, 25, 35, 50],
    "knn__weights": ["uniform", "distance"],
    "knn__metric": ["minkowski"],   # domyślnie p=2 (euklides)
    "knn__p": [1, 2],               # Manhattan vs Euclidean
}

cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

search = GridSearchCV(
    pipe,
    param_grid=param_grid,
    scoring="roc_auc",
    cv=cv,
    n_jobs=-1
)
search.fit(X_train, y_train)

best_model = search.best_estimator_
print("Najlepsze parametry:", search.best_params_)
print("CV ROC-AUC:", search.best_score_)

# 5) Predykcja na teście
y_pred = best_model.predict(X_test)
y_proba = best_model.predict_proba(X_test)[:, 1]

acc = accuracy_score(y_test, y_pred)
bacc = balanced_accuracy_score(y_test, y_pred)
f1m = f1_score(y_test, y_pred)
auc = roc_auc_score(y_test, y_proba)

print("\nMetryki (test):")
print(f"  Accuracy          : {acc:.4f}")
print(f"  Balanced accuracy : {bacc:.4f}")
print(f"  F1                : {f1m:.4f}")
print(f"  ROC-AUC           : {auc:.4f}")

# 6) Macierz pomyłek
cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(cm, display_labels=["0", "1"])

plt.figure(figsize=(4.5, 4))
disp.plot(values_format="d")
plt.title("k-NN: macierz pomyłek (test)")
plt.tight_layout()
plt.show()

# 7) ROC curve
fpr, tpr, thr = roc_curve(y_test, y_proba)

plt.figure(figsize=(5.5, 5))
plt.plot(fpr, tpr, label=f"AUC={auc:.3f}")
plt.plot([0, 1], [0, 1], linestyle="--")
plt.xlabel("FPR")
plt.ylabel("TPR")
plt.title("k-NN: ROC curve (test)")
plt.legend()
plt.tight_layout()
plt.show()
```



