---
title: "Drzewa decyzyjne i zespoly modeli"
---

## Podstawowe drzewa decyzyjne

### Klasyczne rodziny algorytmów drzew i ich różnice

W literaturze i praktyce spotyka się kilka „rodzin” algorytmów drzew decyzyjnych. Różnią się one m.in. sposobem doboru podziału (kryterium jakości), dopuszczalną liczbą gałęzi w węźle, obsługą braków danych, strategią przycinania oraz tym, czy wnioski statystyczne są wbudowane w procedurę uczenia.

**ID3 (Iterative Dichotomiser 3)** (Quinlan) to jeden z najwcześniejszych, wpływowych algorytmów drzew klasyfikacyjnych. Uczy drzewo *wielogałęziowe* (tzn. węzeł może mieć tyle gałęzi, ile kategorii ma dana cecha) i wybiera podział na podstawie **zysku informacji** (*information gain*), który jest redukcją entropii po podziale. W klasycznej postaci ID3 był projektowany głównie dla cech kategorycznych, nie posiadał pełnego, ustandaryzowanego mechanizmu przycinania i jest wrażliwy na cechy o dużej liczbie kategorii (mogą sztucznie „wygrywać” kryterium entropijne). Z perspektywy ogólnego spojrzenia na drzewa decyzyjne ważne jest to, że ID3 ustanowił wzorzec: *rekurencyjne dzielenie + entropia/zysk informacji*.

**C4.5** (Quinlan) jest rozwinięciem ID3 i przez wiele lat był stosowanym standardem. Wprowadza on m.in. (i) obsługę **zmiennych ciągłych** przez poszukiwanie progu i podziały binarne dla cech liczbowych, (ii) modyfikację kryterium jakości podziału w postaci **współczynnika zysku informacji** (*gain ratio*), który koryguje preferencję dla cech o wielu kategoriach, (iii) obsługę **braków danych** przez rozdzielanie obserwacji z brakami „miękko” (z wagami) między gałęzie lub przez dopasowane heurystyki, oraz (iv) przycinanie oparte o oszacowania błędu (tzw. *error-based pruning*). W praktyce C4.5 produkuje drzewa, które są zwykle mniejsze i bardziej uogólniające niż ID3.

**C5.0** to następca C4.5 (Quinlan), zaprojektowany jako szybszy i bardziej skalowalny, z licznymi usprawnieniami inżynieryjnymi i heurystycznymi. Typowo oferuje lepszą wydajność obliczeniową, mniejsze zużycie pamięci oraz dodatkowe możliwości praktyczne (np. kosztowną klasyfikację, mechanizmy „wzmocnienia” w stylu *boosting/committee* w niektórych implementacjach). Koncepcyjnie nadal jest to drzewo „w duchu Quinlana”: kryteria entropijne, sprawna obsługa cech ciągłych i braków oraz silny nacisk na praktyczne uogólnianie.

**CART (Classification and Regression Trees)** (Breiman i in.) ujednolica podejście do **klasyfikacji i regresji** w ramach jednego formalizmu. Charakterystyczne cechy CART są następujące: (i) podziały są zazwyczaj **binarne** (nawet dla cech kategorycznych – kategorie dzieli się na dwie grupy), (ii) w klasyfikacji stosuje się zwykle **indeks Giniego** (lub entropię), a w regresji kryterium oparte o **SSE/wariancję**, (iii) kluczowym elementem jest **przycinanie złożoności** (*cost-complexity pruning*), tj. wybór drzewa przez kompromis między dopasowaniem i złożonością, oraz (iv) mechanizm **reguł zastępczych (surrogate splits)** jako systematyczna odpowiedź na braki danych. CART jest dziś szczególnie ważne, bo stanowi bazę dla wielu metod zespołowych (*random forest, gradient boosting*) i jest najczęściej spotykaną „architekturą” drzew w ML.

**Conditional inference trees (*conditional trees, ctree*)** (Hothorn, Hornik, Zeileis) powstały jako odpowiedź na znane uprzedzenia klasycznych kryteriów podziału (Gini/entropia/SSE), które mogą faworyzować cechy o wielu możliwych podziałach (np. zmienne ciągłe lub kategoryczne o wielu poziomach). W ctree wybór podziału ma charakter **statystyczny** i opiera się na **testach permutacyjnych niezależności**. Procedura jest dwuetapowa: najpierw w danym węźle testuje się hipotezę globalną, że zmienna docelowa jest niezależna od wszystkich cech (jeśli brak podstaw do jej odrzucenia, węzeł staje się liściem), a następnie – w razie istotności – wykonuje się testy cząstkowe dla każdej cechy i wybiera tę o najsilniejszej zależności z odpowiednią korektą na wielokrotne porównania. Typ testu zależy od typu zmiennej docelowej i predyktora: dla klasyfikacji ($Y$ kategoryczne) stosuje się permutacyjne testy niezależności odpowiadające m.in. testom $\chi^2$ (gdy $X$ kategoryczne) lub testom porównania rozkładów/średnich typu ANOVA/F (gdy $X$ ciągłe), natomiast dla regresji ($Y$ ciągłe) stosuje się permutacyjne testy zależności odpowiadające testom korelacyjnym/regresyjnym (gdy $X$ ciągłe) albo testom różnic średnich typu ANOVA (gdy $X$ kategoryczne) – zawsze jednak w wersji permutacyjnej. Dzięki temu conditional trees redukują tendencyjność selekcji zmiennych, a kryterium stopu jest naturalnie powiązane z istotnością statystyczną, a nie wyłącznie z heurystycznymi parametrami złożoności.

**CHAID (Chi-squared Automatic Interaction Detection)** to klasyczna metoda drzew wielogałęziowych, szczególnie popularna w analizie marketingowej i badaniach społecznych. Jej znakiem rozpoznawczym są: (i) podziały wybierane na podstawie **testów chi-kwadrat** (dla klasyfikacji) lub testów analogicznych dla zmiennych ciągłych, (ii) możliwość **łączenia kategorii** cech kategorycznych w większe grupy przed wykonaniem podziału, oraz (iii) częste stosowanie podziałów **wielogałęziowych** (niekoniecznie binarnych). CHAID jest ceniony za interpretowalność i naturalne traktowanie interakcji w kategoriach, ale w porównaniu do CART bywa mniej „ML-owy” w sensie typowych współczesnych pipeline’ów i częściej występuje w narzędziach statystycznych/BI.

Zasada działania drzewa jest rekurencyjna: przestrzeń cech jest dzielona na coraz mniejsze obszary przez kolejne pytania o wartości cech. Każdy podział wybiera jedną cechę i warunek (np. $x_j \le t$), który rozdziela obserwacje na dwie (lub więcej) grupy. Proces powtarza się w powstałych podzbiorach aż do spełnienia kryterium stopu (np. minimalna liczba obserwacji w węźle, maksymalna głębokość, brak istotnej poprawy jakości). W efekcie otrzymujemy model złożony z węzłów wewnętrznych (podziały) oraz liści (obszary decyzyjne).

### Rodzaje drzew i podstawowe elementy konstrukcji

W praktyce wyróżnia się przede wszystkim:

-   **drzewa klasyfikacyjne** – gdy zmienna docelowa jest dyskretna (klasy),
-   **drzewa regresyjne** – gdy zmienna docelowa jest ciągła,
-   **drzewa wieloklasowe** – naturalne rozszerzenie klasyfikacji binarnej,
-   **drzewa binarne vs wielogałęziowe** – CART stosuje zwykle podziały binarne, natomiast w niektórych wariantach dopuszcza się podziały na wiele gałęzi (częściej dla cech kategorycznych).

Drzewo składa się z:

-   **korzenia (*root*)** – węzła startowego zawierającego cały zbiór uczący,
-   **węzłów wewnętrznych (*internal nodes*)** – zawierają regułę podziału (cecha + warunek),
-   **gałęzi (*branches*)** – odpowiadają wynikom reguły (np. „tak/nie” dla podziału binarnego),
-   **liści (*leaves/terminal nodes*)** – węzłów końcowych przechowujących predykcję (klasę lub wartość),
-   (opcjonalnie) **wag/rozkładów w liściu** – np. częstości klas lub parametry prostej regresji w liściu.

![](../images/cart.png)

### Reguły podziału: klasyfikacja i regresja

Węzeł drzewa ma za zadanie wybrać taki podział, który „poprawia jednorodność” powstałych grup. Formalnie dla węzła zawierającego zbiór obserwacji $S$ rozważamy kandydatów podziału $s$ (cecha $j$ i próg $t$, ewentualnie podział kategorii). Podział rozdziela $S$ na $S_L$ i $S_R$. Wybieramy $s$, który maksymalizuje spadek nieczystości (*impurity decrease*):

$$
\Delta I(s) = I(S) - \frac{|S_L|}{|S|} I(S_L) - \frac{|S_R|}{|S|} I(S_R).
$$

**Drzewa klasyfikacyjne.** Nieczystość $I(S)$ definiuje się najczęściej jako:

-   **indeks Giniego**: $$
    I_G(S) = 1 - \sum_{k=1}^K p_k^2,
    $$
-   **entropię (Shannon)**: $$
    I_H(S) = -\sum_{k=1}^K p_k \log p_k,
    $$

gdzie $p_k$ to odsetek obserwacji klasy $k$ w węźle. Intuicyjnie, im bardziej rozkład klas jest skoncentrowany w jednej klasie, tym mniejsza nieczystość. Podział jest „dobry”, jeśli znacząco zwiększa jednorodność klas w dzieciach.

**Drzewa regresyjne.** W regresji nieczystość mierzy się rozproszeniem wartości $y$ w węźle. Najczęściej stosuje się wariancję lub równoważnie sumę kwadratów odchyleń od średniej (SSE):

$$
I_{\text{SSE}}(S) = \sum_{i\in S} (y_i - \bar{y}_S)^2.
$$

Podział wybieramy tak, aby minimalizować łączną SSE po podziale (czyli maksymalizować jej redukcję). W liściu predykcją jest zwykle $\bar{y}_S$ (średnia w liściu), co jest rozwiązaniem minimalizującym SSE w obrębie liścia.

### Jak szuka się optymalnego podziału

W idealnym (globalnym) ujęciu chcielibyśmy znaleźć takie drzewo, które minimalizuje błąd predykcji przy zadanej „złożoności” (np. liczbie liści). Taki problem jest jednak obliczeniowo bardzo trudny: liczba możliwych drzew rośnie wykładniczo wraz z liczbą obserwacji i cech, a wybór optymalnej struktury wymagałby przeszukania ogromnej przestrzeni kombinatorycznej. Dlatego praktyczne algorytmy drzew (CART, C4.5, CHAID, ctree) stosują podejście **zachłanne** (*greedy, top–down recursive partitioning*): w każdym węźle wybierają najlepszy lokalnie podział według zadanego kryterium i dopiero potem powtarzają procedurę w węzłach potomnych.

W podejściu zachłannym konstrukcja drzewa ma postać iteracji:

1. W węźle z danymi $S$ rozważamy zbiór kandydatów podziału $s$ (cecha $j$ i warunek podziału).
2. Dla każdego kandydata obliczamy spadek nieczystości $\Delta I(s)$ (dla klasyfikacji: Gini/entropia; dla regresji: SSE/wariancja).
3. Wybieramy $s^* = \arg\max_s \Delta I(s)$, wykonujemy podział $S \to (S_L,S_R)$.
4. Rekurencyjnie powtarzamy kroki 1–3 w węzłach potomnych, dopóki nie zajdzie warunek stopu.

#### Cechy ciągłe: jak wyznacza się kandydatów progów

Dla cechy ciągłej $x_j$ naturalną regułą podziału jest próg $t$: $x_j \le t$ vs $x_j > t$. Kandydatami progów nie są wszystkie liczby rzeczywiste, lecz wartości „pomiędzy” obserwacjami. W praktyce postępuje się tak:

- sortuje się obserwacje według $x_j$,
- rozważa się progi będące środkami między kolejnymi *różnymi* wartościami $x_j$, tj. $t = (v_{(m)} + v_{(m+1)})/2$,
- często pomija się progi, które nie zmieniają przypisań (np. wiele powtórzeń wartości) lub które łamią ograniczenia typu `min_samples_leaf`.

Dzięki temu liczba kandydatów dla jednej cechy jest rzędu $O(n)$, a nie nieskończona. W implementacjach produkcyjnych dodatkowo używa się trików obliczeniowych: po posortowaniu można aktualizować liczności klas (lub sumy/kwadraty sum w regresji) „przesuwając” próg krok po kroku, bez przeliczania wszystkiego od zera, co istotnie przyspiesza selekcję najlepszego $t$.

#### Cechy kategoryczne: podziały binarne i wielogałęziowe

Dla cech kategorycznych istnieją dwie główne szkoły:

- **podziały wielogałęziowe** (np. w ID3/C4.5/CHAID): węzeł może mieć osobną gałąź dla każdej kategorii; to bywa bardzo interpretowalne, ale może prowadzić do „fragmentacji” danych (małe liczności w gałęziach),
- **podziały binarne** (CART): kategorie dzieli się na dwie grupy $A$ i $\bar A$, tzn. $x_j \in A$ vs $x_j \notin A$.

Podziały binarne dla cechy o $m$ kategoriach mają w najgorszym razie $2^{m-1}-1$ możliwych podziałów, co szybko staje się niepraktyczne. Dlatego stosuje się heurystyki i uproszczenia. Przykładowo:

- w regresji porządkuje się kategorie według średniej $\bar y$ i rozważa rozcięcia jak dla zmiennej uporządkowanej,
- w klasyfikacji można porządkować kategorie według $\hat p(y=1\mid x_j)$ (dla binarnej klasy) lub stosować przybliżone przeszukiwanie,
- przy dużej liczbie poziomów stosuje się łączenie rzadkich kategorii do `other` lub narzuca minimalne liczności.

#### Braki danych a wybór podziału

W zależności od algorytmu i implementacji, braki mogą być obsługiwane na kilka sposobów: przez wcześniejszą imputację, przez traktowanie „braku” jako osobnej kategorii (częste w praktyce), przez wybór domyślnego kierunku podziału (np. brak $\to$ lewa gałąź) lub przez mechanizmy takie jak **surrogate splits** w CART [^04-drzewa-zespoly-1]. Istotne jest, że sposób obsługi braków wpływa zarówno na wynik $\Delta I(s)$, jak i na stabilność drzewa.

[^04-drzewa-zespoly-1]: Reguły zastępcze to mechanizm kojarzony przede wszystkim z CART, używany gdy dla obserwacji brakuje wartości cechy, według której w danym węźle wykonywany jest podział. Zamiast odrzucać obserwację lub imputować brak, drzewo może wybrać **alternatywną regułę podziału** opartą o inną cechę, która możliwie najlepiej „naśladuje” podział główny. W praktyce buduje się ranking reguł zastępczych na podstawie zgodności przypisań do gałęzi (np. jak często podział zastępczy wysyła obserwacje do tej samej strony co podział główny w danych treningowych). Dzięki temu drzewo zachowuje spójność działania nawet przy brakach danych.

#### Kontrola złożoności: pre-pruning i (w CART) post-pruning

Ponieważ strategia zachłanna łatwo prowadzi do bardzo głębokich drzew, w praktyce kontroluje się złożoność na dwa sposoby:

- **pre-pruning** (ograniczenia w trakcie budowy): zamiast budować bardzo głębokie drzewo, można narzucić ograniczenia już na etapie wzrostu, aby zmniejszyć wariancję i ryzyko przeuczenia.

  - `max_depth` – maksymalna głębokość drzewa (liczba krawędzi/poziomów od korzenia do liścia). Małe wartości ograniczają liczbę kolejnych podziałów, co zwykle zwiększa bias, ale zmniejsza wariancję.
  - `min_samples_split` – minimalna liczba obserwacji w węźle, aby w ogóle rozważać jego podział. Jeśli węzeł ma mniej obserwacji, staje się liściem.
  - `min_samples_leaf` – minimalna liczba obserwacji, która musi pozostać w każdym liściu po podziale. W praktyce eliminuje to podziały tworzące bardzo małe, niestabilne liście (np. podział 3 vs 97 przy `min_samples_leaf=10` jest niedozwolony).
  - `max_leaf_nodes` – maksymalna liczba liści w całym drzewie. To bezpośrednia kontrola złożoności, bo liczba liści determinuje liczbę regionów decyzyjnych.
  - `min_impurity_decrease` – minimalna wymagana redukcja nieczystości $\Delta I(s)$, aby zaakceptować podział. Jeśli najlepszy możliwy podział w węźle nie daje spadku nieczystości większego od progu, algorytm przerywa wzrost i tworzy liść.
- **post-pruning** (przycinanie po zbudowaniu dużego drzewa): w CART standardowo buduje się najpierw drzewo „maksymalne” (silnie dopasowane, często aż do spełnienia minimalnych ograniczeń liczności), a następnie usuwa się jego gałęzie, wybierając takie poddrzewo, które najlepiej równoważy **dopasowanie** i **złożoność**. Klasyczne podejście to *cost-complexity pruning* (znane też jako *weakest-link pruning*), w którym rozważa się rodzinę poddrzew $T_\alpha$ minimalizujących kompromis

  $$
  R(T) + \alpha\,|T|,
  $$

  gdzie:

  - $R(T)$ jest miarą „błędu” lub „straty” drzewa (w CART często jest to błąd resubstytucji / suma nieczystości w liściach, np. suma SSE w regresji lub suma nieczystości Giniego ważona licznościami w klasyfikacji; w praktyce interesuje nas przede wszystkim zgodność tej miary z błędem generalizacji),
  - $|T|$ to liczba liści (*terminal nodes*) – prosta miara złożoności drzewa,
  - $\alpha \ge 0$ to parametr kary za złożoność: dla $\alpha \to 0$ preferowane jest drzewo duże (mały nacisk na prostotę), a dla dużych $\alpha$ otrzymujemy coraz płytsze drzewa.

  **Jak przebiega przycinanie w CART (idea „najsłabszego ogniwa”).** Dla każdego węzła wewnętrznego $t$ rozważa się zastąpienie całego poddrzewa $T_t$ pojedynczym liściem i ocenia się, „jak dużo poprawy dopasowania” daje to poddrzewo w przeliczeniu na „ile dodatkowych liści kosztuje”. Formalnie wyznacza się wskaźnik krytyczny (tzw. wartość $\alpha$ dla węzła):

  $$
  g(t) = \frac{R(t) - R(T_t)}{|T_t| - 1},
  $$

  gdzie $R(t)$ to strata, gdy $T_t$ zostanie zastąpione jednym liściem, a $R(T_t)$ to strata pełnego poddrzewa. W każdym kroku usuwa się to poddrzewo, które ma najmniejsze $g(t)$ (czyli „najmniej opłaca się” je utrzymywać) – stąd nazwa *weakest-link*. Powtarzając ten krok, otrzymuje się **skończoną sekwencję zagnieżdżonych poddrzew**

  $$
  T_0 \supset T_1 \supset \cdots \supset T_M,
  $$

  odpowiadających rosnącym wartościom $\alpha$. Dzięki temu zamiast przeszukiwać wszystkie możliwe drzewa, analizujemy tylko tę sekwencję kandydatów.

  **Dobór $\alpha$ (czyli wybór końcowego drzewa).** Parametr $\alpha$ dobiera się zwykle przez walidację krzyżową: dla każdego kandydata $T_m$ estymuje się błąd generalizacji i wybiera drzewo o najlepszej jakości. Często stosuje się też zasadę **1-SE**: wybiera się najmniejsze drzewo, którego błąd walidacyjny jest nie większy niż minimum powiększone o jedno odchylenie standardowe, aby preferować model prostszy i stabilniejszy.

### Predykcja z drzewa

Predykcja polega na „przejściu” obserwacji przez drzewo od korzenia do liścia, wykonując po drodze kolejne testy.

-   **Drzewo klasyfikacyjne:** w liściu przechowuje się rozkład klas (częstości) $\hat{p}_k$. Predykcją klasy jest zwykle $\arg\max_k \hat{p}_k$, a predykcją probabilistyczną – wektor $(\hat{p}_1,\dots,\hat{p}_K)$.
-   **Drzewo regresyjne:** w liściu przechowuje się wartość liczbową, najczęściej średnią $\bar{y}_S$ (lub medianę, zależnie od kryterium). Predykcja to ta wartość przypisana do liścia, do którego trafia obserwacja.

## Bagging i lasy losowe

Bagging (*bootstrap aggregating*) i lasy losowe (*random forests*) należą do metod **zespołowych** (*ensemble methods*), w których wiele słabych/średnich modeli (zwykle drzew) łączy się w jeden silniejszy predyktor. Kluczowa intuicja jest taka, że pojedyncze drzewo decyzyjne ma zwykle **niskie obciążenie (bias)**, ale **wysoką wariancję** – niewielka zmiana danych uczących może prowadzić do zauważalnie innej struktury drzewa i innych predykcji. Bagging i random forest redukują wariancję przez **uśrednianie (agregację) wielu „odmian” tego samego algorytmu**, uczonych na lekko zmodyfikowanych próbkach.

### Rys historyczny: od bootstrapu do lasów losowych

Rozwój tych metod można zrozumieć jako sekwencję pomysłów, które stopniowo zwiększały stabilność modeli i jakość uogólniania:

- **Bootstrap** (Efron, 1979)[^bagging-1] - technika resamplingu „z powtórzeniami” do przybliżania rozkładów estymatorów i błędu. To właśnie bootstrap dał naturalny mechanizm generowania wielu „wersji” zbioru treningowego.  
- **Bagging** (Breiman, 1996)[^bagging-2] - pomysł, aby trenować ten sam algorytm na wielu próbkach bootstrapowych i agregować wyniki. Breiman pokazał, że bagging szczególnie dobrze stabilizuje metody niestabilne (jak drzewa).  
- **Random subspace / losowanie cech** (Ho, 1998)[^bagging-3] - idea, aby dodatkowo losować podzbiór cech, na których uczony jest model. To zmniejsza korelację między modelami w zespole.  
- **Random Forest** (Breiman, 2001)[^bagging-4] - połączenie baggingu drzew z losowaniem cech *w każdym węźle* drzewa (a nie tylko raz na drzewo). To okazało się bardzo skutecznym i prostym „domyślnym” modelem dla danych tablicowych.  
- **Extremely Randomized Trees** (Geurts i in., 2006)[^bagging-5] - dalsza randomizacja poprzez losowanie progów podziału, co jeszcze bardziej dekoreluje drzewa, czasem poprawiając wynik kosztem większego bias.

### Koncepcja baggingu: redukcja wariancji przez agregację

Niech $\mathcal{D} = \{(x_i, y_i)\}_{i=1}^n$ oznacza zbiór uczący, a $\hat f(\cdot;\mathcal{D})$ – model (np. drzewo) wyuczony na danych $\mathcal{D}$. W baggingu generujemy $B$ próbek bootstrapowych $\mathcal{D}^{(1)},\dots,\mathcal{D}^{(B)}$, gdzie każda $\mathcal{D}^{(b)}$ ma rozmiar $n$ i powstaje przez losowanie obserwacji z $\mathcal{D}$ *z powtórzeniami*.

- **Bagging w regresji (uśrednianie):**

  $$
  \hat f_{\text{bag}}(x) = \frac{1}{B}\sum_{b=1}^B \hat f^{(b)}(x),
  $$

  gdzie $\hat f^{(b)}(x) = \hat f(x;\mathcal{D}^{(b)})$.

- **Bagging w klasyfikacji (głosowanie większościowe):**

  $$
  \hat y_{\text{bag}}(x) = \arg\max_{k\in\{1,\dots,K\}} \sum_{b=1}^B \mathbb{1}\{\hat y^{(b)}(x)=k\}.
  $$

  W wersji probabilistycznej często uśrednia się estymowane prawdopodobieństwa klas.

**Dlaczego to działa (intuicja bias–variance).** Jeśli pojedynczy model ma wariancję $\mathrm{Var}(\hat f(x))$, to uśrednienie $B$ modeli obniża wariancję. W idealnym przypadku niezależności modeli:

$$
\mathrm{Var}\big(\hat f_{\text{bag}}(x)\big) = \frac{1}{B}\,\mathrm{Var}(\hat f(x)).
$$

W praktyce modele nie są niezależne; jeśli ich korelacja w punkcie $x$ wynosi $\rho$, to w przybliżeniu:

$$
\mathrm{Var}\big(\hat f_{\text{bag}}(x)\big) \approx \rho\,\mathrm{Var}(\hat f(x)) + \frac{1-\rho}{B}\,\mathrm{Var}(\hat f(x)).
$$

Zatem kluczowe są dwa elementy: **(i) duża liczba drzew $B$** oraz **(ii) mała korelacja $\rho$** między drzewami. Bagging zwiększa różnorodność przez bootstrap, a random forest dodatkowo obniża korelację przez losowanie cech.

### Algorytm baggingu (schemat)

Dla $b=1,\dots,B$:

1. Wylosuj próbkę bootstrapową $\mathcal{D}^{(b)}$ o rozmiarze $n$ ze zbioru $\mathcal{D}$.
2. Naucz model bazowy $\hat f^{(b)}$ na $\mathcal{D}^{(b)}$ (często jest to drzewo głębokie, słabo przycinane).
3. Zapisz model $\hat f^{(b)}$.

Predykcja: agreguj $\{\hat f^{(b)}\}$ przez średnią (regresja) lub głosowanie (klasyfikacja).

### Lasy losowe: bagging + losowanie cech w węzłach

Random forest jest specjalnym przypadkiem baggingu, gdzie modelem bazowym jest drzewo decyzyjne, ale w każdym węźle drzewa **nie rozważa się wszystkich $p$ cech**, tylko losowy podzbiór $m$ cech (często oznaczany jako *mtry*). Następnie wybiera się najlepszy podział **tylko wśród tych $m$ cech**. Dzięki temu różne drzewa stają się mniej do siebie podobne (mniejsza korelacja), a zespół lepiej redukuje wariancję.

#### Algorytm budowy lasu losowego (RF)

Dla $b=1,\dots,B$:

1. Wylosuj próbkę bootstrapową $\mathcal{D}^{(b)}$.
2. Ucz drzewo $T^{(b)}$ rekurencyjnie:
   - w każdym węźle losuj bez zwracania $m$ cech spośród $p$,
   - wyznacz najlepszy podział (maksymalna redukcja nieczystości) tylko wśród tych $m$ cech,
   - kontynuuj aż do kryterium stopu (często drzewo rośnie „głęboko”, np. do minimalnej liczności liścia).

Predykcja:

- regresja: $\hat f_{\text{RF}}(x)=\frac{1}{B}\sum_{b=1}^B T^{(b)}(x)$,
- klasyfikacja: głosowanie większościowe (lub uśrednianie $\hat p_k(x)$).

### Najważniejsze parametry i ich interpretacja

Poniżej zebrano parametry typowe dla implementacji w stylu scikit-learn (nazwy mogą się różnić w innych bibliotekach, ale sens jest ten sam).

#### Parametry wspólne (bagging drzew i random forest)

- **`n_estimators` ($B$) – liczba drzew.** Zwiększanie $B$ zwykle poprawia stabilność i jakość (wariancja maleje), ale z malejącymi przyrostami. W praktyce dobiera się $B$ tak, aby wynik się „stabilizował”.
- **`bootstrap` / `max_samples` – sposób i rozmiar próbkowania.** Klasycznie losuje się $n$ obserwacji z powtórzeniami; `max_samples` pozwala użyć ułamka danych (czasem przyspiesza, czasem zwiększa różnorodność).
- **Parametry drzewa bazowego (kontrola złożoności):** `max_depth`, `min_samples_leaf`, `min_samples_split`, `max_leaf_nodes`, `min_impurity_decrease`. W zespołach często pozwala się drzewom rosnąć głęboko (niski bias), a wariancję kontroluje się przez agregację.

#### Parametry specyficzne dla random forest

- **`max_features` ($m$, *mtry*) – liczba losowanych cech w węźle.** To parametr krytyczny dla korelacji między drzewami.

  - Jeśli $m=p$, random forest redukuje się do klasycznego baggingu drzew (drzewa są bardziej podobne).
  - Jeśli $m$ jest małe, drzewa są bardziej zróżnicowane (mniejsza korelacja), ale pojedyncze drzewo jest słabsze (większy bias).

  Popularne heurystyki startowe: w klasyfikacji $m\approx\sqrt{p}$, w regresji $m\approx p/3$ (to są reguły kciuka, a nie prawa).

- **`oob_score` – ocena out-of-bag (OOB).** W bootstrapie ok. $1-1/e\approx 63.2\%$ obserwacji trafia do danej próbki, a pozostałe $~36.8\%$ są „poza próbką” dla tego drzewa (*out-of-bag*). Można więc estymować błąd generalizacji bez osobnego zbioru walidacyjnego: dla każdej obserwacji agreguje się predykcje tylko z drzew, które jej nie widziały.

- **Ważność cech (feature importance).** Najczęściej spotkasz dwa podejścia:

  1. **MDI (mean decrease in impurity)** – uśredniona redukcja nieczystości przypisana do danej cechy po wszystkich węzłach i drzewach.
  2. **Permutation importance** – mierzy spadek jakości (np. accuracy/AUC) po losowym przemieszaniu wartości danej cechy; to podejście jest zwykle bardziej wiarygodne, bo mierzy wpływ na predykcję, ale jest droższe obliczeniowo.

  Uwaga praktyczna: MDI może faworyzować cechy ciągłe lub o wielu poziomach; permutation importance jest na to mniej wrażliwa, choć wciąż może cierpieć przy silnie skorelowanych cechach.

### Bagging vs random forest: podobieństwa i różnice

- **Wspólne:** oba podejścia uczą wiele drzew na próbkach bootstrapowych i agregują ich predykcje. Głównym celem jest redukcja wariancji.
- **Różnica kluczowa:** random forest wprowadza dodatkową losowość przez `max_features` w każdym węźle, co **obniża korelację między drzewami** i zwykle poprawia wynik względem czystego baggingu drzew.
- **Kiedy bagging wystarcza:** gdy liczba cech jest mała i drzewa i tak są zróżnicowane, zysk z losowania cech może być niewielki.
- **Kiedy RF wygrywa:** gdy jest wiele cech i/lub część cech dominuje (silnie predykcyjnych) – losowanie cech zapobiega sytuacji, w której wszystkie drzewa w kółko wybierają te same pierwsze podziały.

### Dodatkowe uwagi praktyczne

- **Brak potrzeby skalowania:** drzewom i ich zespołom zwykle nie przeszkadzają różne skale cech (dzielą według progów), więc standaryzacja nie jest wymagana.
- **Odporność na nieliniowości i interakcje:** RF i bagging drzew automatycznie modelują interakcje i nieliniowości, co czyni je mocnym baseline’em.
- **Interpretowalność:** pojedyncze drzewo jest czytelne, ale las jest mniej transparentny. W praktyce stosuje się ważności cech, wykresy PDP/ICE czy SHAP (na innych zajęciach) do interpretacji.
- **Nieregularne braki i kategorie:** w zależności od implementacji potrzebujesz imputacji/kodowania kategorii; część nowoczesnych bibliotek (np. CatBoost) rozwiązuje to natywnie, ale klasyczny RF często wymaga przygotowania danych.

[^bagging-1]: B. Efron (1979), *Bootstrap Methods: Another Look at the Jackknife*, The Annals of Statistics.
[^bagging-2]: L. Breiman (1996), *Bagging Predictors*, Machine Learning.
[^bagging-3]: T. K. Ho (1998), *The Random Subspace Method for Constructing Decision Forests*, IEEE TPAMI.
[^bagging-4]: L. Breiman (2001), *Random Forests*, Machine Learning.
[^bagging-5]: P. Geurts, D. Ernst, L. Wehenkel (2006), *Extremely Randomized Trees*, Machine Learning.

## Boosting

Boosting to druga fundamentalna rodzina metod zespołowych, w której modele buduje się **sekwencyjnie**: każdy kolejny model ma korygować błędy poprzednich. W odróżnieniu od baggingu, który przede wszystkim redukuje wariancję przez uśrednianie wielu podobnych modeli uczonych niezależnie, boosting jest projektowany tak, aby stopniowo zmniejszać **błąd systematyczny (bias)**, budując coraz lepszy predyktor addytywny. W praktyce boosting często daje bardzo wysoką jakość na danych tablicowych, ale wymaga ostrożnej kontroli złożoności, bo potrafi łatwiej niż bagging dopasować się nadmiernie do danych.

### Rys historyczny: od „słabych uczniów” do gradientowego boostingu

Rozwój boostingu można przedstawić jako przejście od idei teoretycznej do bardzo wydajnych implementacji inżynieryjnych:

- **Idea „wzmacniania” słabych klasyfikatorów** (Schapire, 1990)[^boost-1] - pokazano, że jeśli istnieje algorytm osiągający wynik minimalnie lepszy niż losowy (*weak learner*), to można go „wzmocnić” do klasyfikatora o dowolnie małym błędzie treningowym przez odpowiednią procedurę zespołową.  
- **AdaBoost** (Freund & Schapire, 1996/1997)[^boost-2] - praktyczny algorytm boostingu, w którym kolejne klasyfikatory uczą się na danych z wagami, skupiając się na obserwacjach trudnych (wcześniej błędnie klasyfikowanych).  
- **Gradient Boosting / MART** (Friedman, 2001)[^boost-3] - uogólnienie boostingu do postaci optymalizacji funkcji straty w przestrzeni funkcji, interpretowane jako „zejście gradientowe” w modelu addytywnym.  
- **XGBoost** (Chen & Guestrin, 2016)[^boost-4] - bardzo wydajna implementacja gradientowego boostingu drzew z regularizacją, obsługą braków i szeregiem optymalizacji obliczeniowych (m.in. przyspieszone wyznaczanie podziałów, równoległość, przycinanie przez ograniczenia).  
- **CatBoost** (Prokhorenkova i in., 2018)[^boost-5] - boosting drzew z natywną obsługą zmiennych kategorycznych oraz mechanizmami ograniczającymi *target leakage* i przeuczenie (m.in. uporządkowane statystyki docelowe, *ordered boosting*).

### Koncepcja: model addytywny i uczenie „na błędach”

W większości współczesnych wariantów boosting buduje model addytywny postaci:

$$
F_M(x) = \sum_{m=0}^M \nu\, f_m(x),
$$

gdzie $f_m$ to kolejne modele bazowe (często płytkie drzewa), a $\nu\in(0,1]$ to współczynnik uczenia (*learning rate*, *shrinkage*). Sens jest następujący: zamiast uśredniać niezależne modele (jak w baggingu), boosting **dokłada** kolejne składniki tak, aby minimalizować stratę $\mathcal{L}(y, F(x))$. Małe $\nu$ spowalnia uczenie, ale zwykle poprawia uogólnianie (wymaga wtedy większej liczby iteracji).

### AdaBoost: boosting przez wagi obserwacji (klasyfikacja)

W klasycznej wersji AdaBoost dla klasyfikacji binarnej $y_i\in\{-1,+1\}$ uczymy sekwencję klasyfikatorów $h_m$. Algorytm utrzymuje rozkład wag $w_i^{(m)}$ na obserwacjach, który w kolejnych iteracjach zwiększa znaczenie przykładów błędnie klasyfikowanych.

1. Inicjalizacja: $w_i^{(1)}=1/n$.
2. Dla $m=1,\dots,M$:

   - ucz $h_m$ na danych z wagami $w^{(m)}$,
   - oblicz błąd ważony:

     $$
     \varepsilon_m = \frac{\sum_{i=1}^n w_i^{(m)}\,\mathbb{1}\{h_m(x_i)\neq y_i\}}{\sum_{i=1}^n w_i^{(m)}}.
     $$

   - wyznacz wagę klasyfikatora:

     $$
     \alpha_m = \frac{1}{2}\log\frac{1-\varepsilon_m}{\varepsilon_m}.
     $$

   - zaktualizuj wagi obserwacji:

     $$
     w_i^{(m+1)} \propto w_i^{(m)}\,\exp\big(-\alpha_m\,y_i\,h_m(x_i)\big),
     $$

     a następnie znormalizuj tak, aby $\sum_i w_i^{(m+1)}=1$.

Końcowy klasyfikator ma postać ważonego głosowania:

$$
H(x)=\mathrm{sign}\Big(\sum_{m=1}^M \alpha_m h_m(x)\Big).
$$

Interpretacyjnie AdaBoost minimalizuje wykładniczą stratę $\sum_i \exp(-y_i F(x_i))$, a $\alpha_m$ rośnie, gdy $h_m$ jest lepszy (ma mniejszy $\varepsilon_m$). W praktyce jako $h_m$ stosuje się często bardzo proste modele, np. *decision stumps* (drzewa głębokości 1), co wzmacnia efekt „uczenia na błędach”.

### Gradient Boosting: minimalizacja straty w przestrzeni funkcji

Gradient boosting uogólnia ideę „poprawiania błędów” na dowolną funkcję straty $\mathcal{L}$. Model addytywny jest budowany iteracyjnie:

$$
F_m(x) = F_{m-1}(x) + \nu\, f_m(x).
$$

W kroku $m$ dopasowujemy $f_m$ do tzw. **pseudo-reszt** (*pseudo-residuals*), które są ujemnym gradientem straty względem bieżących predykcji:

$$
r_{im} = -\left.\frac{\partial\,\mathcal{L}(y_i, F(x_i))}{\partial F(x_i)}\right|_{F=F_{m-1}}.
$$

Następnie uczymy model bazowy $f_m$ (zwykle płytkie drzewo) tak, aby dobrze aproksymował $r_{im}$ jako funkcję $x_i$. Dla niektórych strat wykonuje się dodatkowo krok „liniowego przeskalowania” (wyszukanie $\gamma_m$):

$$
\gamma_m = \arg\min_{\gamma} \sum_{i=1}^n \mathcal{L}\big(y_i, F_{m-1}(x_i)+\gamma f_m(x_i)\big),
$$

i aktualizuje się $F_m(x)=F_{m-1}(x)+\nu\,\gamma_m f_m(x)$. W wielu implementacjach drzewiastych $\gamma_m$ jest w praktyce „wbudowane” w wartości w liściach.

#### Algorytm (schemat) gradientowego boostingu drzew

1. Ustal inicjalny model $F_0(x)$ (np. stałą minimalizującą stratę: średnią dla MSE, logit priory dla log-loss).
2. Dla $m=1,\dots,M$:
   - oblicz pseudo-reszty $r_{im}$,
   - dopasuj drzewo $f_m$ do par $(x_i, r_{im})$,
   - (opcjonalnie) wyznacz $\gamma_m$ minimalizujące stratę wzdłuż kierunku $f_m$,
   - zaktualizuj $F_m(x)=F_{m-1}(x)+\nu\,\gamma_m f_m(x)$.

Ważna intuicja: w regresji z MSE pseudo-reszty są po prostu resztami $r_{im}=y_i-F_{m-1}(x_i)$, więc boosting faktycznie „doucza” kolejne drzewo na błędach poprzedniego modelu.

### XGBoost: regularizacja i optymalizacje implementacyjne

XGBoost jest implementacją gradient boosting drzew, która dodaje silną **regularizację** oraz usprawnienia obliczeniowe. W typowej postaci minimalizuje się funkcję celu:

$$
\sum_{i=1}^n \mathcal{L}(y_i, \hat y_i) + \sum_{m=1}^M \Omega(f_m),
$$

gdzie $\Omega$ karze złożoność drzew, np. przez liczbę liści i normę wag w liściach (w praktyce: `reg_alpha`, `reg_lambda`, `gamma`, itp.). Implementacyjnie istotne są m.in. efektywne wyznaczanie podziałów, obsługa braków (domyślny kierunek dla `missing` w węzłach), `subsample` i `colsample_*` (losowanie wierszy i cech) oraz możliwość wczesnego zatrzymania (*early stopping*) na zbiorze walidacyjnym.

### CatBoost: kategorie i kontrola wycieku informacji

CatBoost jest boostowaniem drzew ukierunkowanym na dane z licznymi zmiennymi kategorycznymi. Zamiast prostego one-hot dla wielu poziomów stosuje się **statystyki docelowe** (*target statistics*), ale liczone w sposób, który ogranicza wyciek informacji: dla danej obserwacji statystyka jest wyliczana na podstawie „wcześniejszych” obserwacji w losowej permutacji (*ordered target encoding*), a proces uczenia używa wariantu *ordered boosting*. Dzięki temu CatBoost często daje bardzo dobre wyniki na danych mieszanych (numeryczne + kategorie) bez rozbudowanego preprocessingu.

### Najważniejsze hiperparametry i ryzyko przeuczenia

Boosting ma wiele „pokręteł”, ale kilka z nich dominuje praktykę:

- **`n_estimators` ($M$) – liczba iteracji/drzew.** Większa liczba zwiększa potencjał dopasowania; bez kontroli może prowadzić do przeuczenia.
- **`learning_rate` ($\nu$) – shrinkage.** Mniejsze $\nu$ zwykle poprawia uogólnianie, ale wymaga większego $M$. W praktyce parę $(M,\nu)$ traktuje się łącznie.
- **Złożoność drzew:** `max_depth`, `max_leaf_nodes`, `min_samples_leaf` (lub ich odpowiedniki). Płytkie drzewa (np. `max_depth` 2–6) są standardem w boosting (inaczej model łatwo „przepala” dane).
- **Losowanie obserwacji i cech:** `subsample` (stochastic gradient boosting), `colsample_bytree`, `colsample_bylevel` (w XGBoost). Mniejsze wartości działają jak regularizacja i zmniejszają wariancję.
- **Regularizacja w XGBoost:** `reg_lambda` (L2), `reg_alpha` (L1), `gamma` (minimalna poprawa, by wykonać split), `min_child_weight` (minimalna „waga”/liczność w węźle). Te parametry ograniczają tworzenie zbyt szczegółowych podziałów.
- **Wczesne zatrzymanie (early stopping):** w praktyce często monitoruje się błąd na zbiorze walidacyjnym i przerywa uczenie, gdy brak poprawy przez określoną liczbę iteracji. To jedna z najskuteczniejszych metod kontroli przeuczenia w boosting.

### Boosting vs bagging: porównanie i typowe „pułapki”

- **Cel:** bagging redukuje wariancję (średnia wielu modeli), boosting redukuje bias (sekwencyjna korekta błędów).
- **Równoległość:** bagging łatwo zrównoleglić (drzewa niezależne); boosting jest z natury sekwencyjny (choć implementacje optymalizują wnętrze kroku).
- **Wrażliwość na szum/outliery:** boosting może silniej „gonić” obserwacje odstające i szum etykiet, bo kolejne kroki koncentrują się na błędach. Bagging zwykle jest pod tym względem bardziej odporny.
- **Najczęstsze źródła przeuczenia w boosting:** zbyt duże $M$, zbyt duże drzewa, zbyt duży $\nu$ oraz brak regularizacji/losowania.

### Uwaga praktyczna: metryki, ważności cech i interpretacja

W boostingu (podobnie jak w RF) często raportuje się „ważność cech”, ale należy pamiętać o ograniczeniach: miary oparte o redukcję nieczystości mogą faworyzować cechy ciągłe lub o wielu poziomach, a silna korelacja cech rozmywa interpretację. W praktyce bardziej wiarygodne są podejścia oparte o permutacje lub metody wyjaśnialności (np. SHAP), choć są one obliczeniowo droższe.

[^boost-1]: R. E. Schapire (1990), *The Strength of Weak Learnability*, Machine Learning.
[^boost-2]: Y. Freund, R. E. Schapire (1997), *A Decision-Theoretic Generalization of On-Line Learning and an Application to Boosting*, Journal of Computer and System Sciences.
[^boost-3]: J. H. Friedman (2001), *Greedy Function Approximation: A Gradient Boosting Machine*, Annals of Statistics.
[^boost-4]: T. Chen, C. Guestrin (2016), *XGBoost: A Scalable Tree Boosting System*, KDD.
[^boost-5]: L. Prokhorenkova, G. Gusev, A. Vorobev, A. Dorogush, A. Gulin (2018), *CatBoost: unbiased boosting with categorical features*, NeurIPS.