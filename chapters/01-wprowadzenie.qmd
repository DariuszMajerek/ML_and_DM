---
title: "Wprowadzenie i historia"
---

## Historia eksploracji danych i uczenia maszynowego

Historia eksploracji danych oraz uczenia maszynowego jest ściśle związana z rozwojem statystyki, informatyki i sztucznej inteligencji, jednak obie dziedziny wywodzą się z odmiennych tradycji badawczych. Eksploracja danych (data mining) rozwijała się przede wszystkim na gruncie statystyki matematycznej, analizy danych oraz teorii baz danych i była odpowiedzią na rosnącą dostępność dużych zbiorów danych empirycznych. Jej głównym celem było odkrywanie wzorców, struktur i zależności, które nie były bezpośrednio widoczne przy użyciu klasycznych narzędzi analizy. Uczenie maszynowe (machine learning) natomiast wyrosło z badań nad sztuczną inteligencją i algorytmami uczącymi się, kładąc nacisk na formalne modele predykcyjne, automatyczne dostosowywanie się do danych oraz własności generalizacji. W kontekście niniejszego wykładu oba podejścia spotykają się w obszarze metod klasyfikacyjnych i regresyjnych, które jednocześnie służą eksploracji danych i budowie modeli uczących się.

Pierwsze istotne fundamenty eksploracji danych pojawiły się już na początku XX wieku wraz z rozwojem statystyki wielowymiarowej. Szczególne znaczenie miały badania nad klasyfikacją i redukcją wymiaru, prowadzone w ramach wnioskowania statystycznego. Przełomową rolę odegrała praca Ronalda A. Fishera z 1936 roku, w której zaproponowano liniową analizę dyskryminacyjną jako metodę rozróżniania klas na podstawie kombinacji liniowych zmiennych. Choć Fisher nie posługiwał się pojęciem uczenia maszynowego, jego metoda stanowi bezpośredni pierwowzór współczesnych modeli dyskryminacyjnych, takich jak LDA, QDA oraz ich późniejsze uogólnienia. W tym okresie analiza danych była traktowana przede wszystkim jako narzędzie inferencyjne, służące do testowania hipotez i opisu struktury populacji.

Równolegle do rozwoju statystyki, w połowie XX wieku zaczęły kształtować się idee sztucznej inteligencji. To właśnie na tym gruncie narodziło się uczenie maszynowe jako odrębna dziedzina. Jednym z najczęściej cytowanych momentów symbolicznych jest publikacja Arthura Samuela z 1959 roku, w której autor opisał algorytm uczący się gry w warcaby poprzez doświadczenie. Samuel zaproponował definicję uczenia maszynowego jako procesu, w którym system poprawia swoje działanie na podstawie obserwacji danych, co do dziś pozostaje centralnym elementem tej dziedziny. W tym samym czasie rozwijano probabilistyczne metody klasyfikacji, oparte na twierdzeniu Bayesa, które umożliwiały formalne łączenie danych empirycznych z wiedzą a priori. Klasyfikatory ML, MAP oraz naiwny klasyfikator Bayesa znalazły szerokie zastosowanie zarówno w eksploracji danych, jak i w uczeniu maszynowym.

Lata siedemdziesiąte i osiemdziesiąte XX wieku przyniosły intensywny rozwój metod klasyfikacyjnych, które do dziś stanowią podstawę analizy danych. Szczególnie istotne okazały się drzewa decyzyjne, ugruntowane w monografii Breimana, Friedmana, Olshena i Stone’a z 1984 roku. Metody CART wprowadziły ideę rekurencyjnego podziału przestrzeni cech oraz kryteria optymalizacji oparte na nieczystości węzłów, co pozwoliło na budowę modeli jednocześnie skutecznych i interpretowalnych. Drzewa decyzyjne szybko stały się jednym z podstawowych narzędzi eksploracji danych, zwłaszcza w analizie zbiorów o złożonej strukturze i mieszanych typach zmiennych.

W tym samym okresie rozwijano metody oparte na podobieństwie obserwacji, w szczególności algorytm k najbliższych sąsiadów. Praca Covera i Harta z 1967 roku formalnie opisała własności klasyfikatora k-NN, który nie wymaga jawnej estymacji parametrów modelu, a decyzje podejmuje na podstawie lokalnej struktury danych. Metody te odegrały ważną rolę w eksploracji danych, ponieważ umożliwiały analizę bez silnych założeń rozkładowych i stanowiły punkt odniesienia dla późniejszych, bardziej złożonych algorytmów.

Istotnym krokiem w stronę formalizacji uczenia maszynowego było wprowadzenie maszyn wektorów nośnych w latach dziewięćdziesiątych. Praca Cortes i Vapnika z 1995 roku zaproponowała model klasyfikacyjny oparty na maksymalizacji marginesu separacji klas, osadzony w aparacie optymalizacji wypukłej. Dzięki zastosowaniu funkcji jądrowych SVM umożliwiły efektywną klasyfikację danych nieliniowo separowalnych w przestrzeniach o bardzo wysokim wymiarze. Metoda ta stała się jednym z najlepiej ugruntowanych teoretycznie algorytmów uczenia maszynowego i do dziś pełni istotną rolę w analizie danych.

Kolejnym przełomem w eksploracji danych było pojawienie się metod zespołowych. Leo Breiman wprowadził ideę baggingu w 1996 roku, pokazując, że agregacja wielu niestabilnych modeli może znacząco poprawić jakość predykcji. Rozwinięciem tej koncepcji były lasy losowe, które połączyły bagging z losowym wyborem cech, tworząc jeden z najbardziej uniwersalnych algorytmów analizy danych. Równolegle rozwijał się nurt boostingu, zapoczątkowany przez Freund i Schapire, który polegał na sekwencyjnym uczeniu słabych klasyfikatorów i ich adaptacyjnym ważeniu. Współczesne algorytmy, takie jak Gradient Boosting, XGBoost czy CatBoost, stanowią bezpośrednie rozwinięcie tych idei i są dziś standardem w analizie danych tablicowych.

Na styku statystyki i uczenia maszynowego rozwijały się także modele addytywne. Prace Hastiego i Tibshiraniego z lat osiemdziesiątych wprowadziły uogólnione modele addytywne, które umożliwiają elastyczne modelowanie zależności nieliniowych przy zachowaniu interpretowalnej struktury. Modele te stanowią naturalne rozszerzenie klasycznej regresji i dobrze ilustrują ewolucję metod eksploracji danych w kierunku większej elastyczności bez rezygnacji z kontroli nad złożonością modelu.

Z perspektywy dydaktycznej historia eksploracji danych i uczenia maszynowego pokazuje, że większość współczesnych algorytmów opiera się na ideach rozwijanych od dziesięcioleci. Metody omawiane w dalszej części wykładu nie są oderwanymi narzędziami, lecz elementami spójnej ewolucji pojęć takich jak klasyfikacja, estymacja, generalizacja i kompromis między złożonością a interpretowalnością. Zrozumienie tego kontekstu historycznego pozwala lepiej interpretować zachowanie modeli, ich założenia oraz ograniczenia w praktycznej analizie danych.