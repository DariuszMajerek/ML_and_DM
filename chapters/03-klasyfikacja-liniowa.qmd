---
title: "Modele liniowe i dyskryminacyjne"
---

## Modele liniowe w uczeniu nadzorowanym

Modele liniowe stanowią fundament klasycznego uczenia maszynowego dla danych tablicowych. Ich atrakcyjność wynika z połączenia trzech cech: prostoty konstrukcji, relatywnie łatwej interpretacji oraz stabilnych własności matematycznych. W ujęciu uczenia nadzorowanego model liniowy opisuje zależność pomiędzy wektorem cech $x \in \mathbb{R}^p$ a zmienną docelową $y$, ucząc się parametrów na podstawie par $(x_i, y_i)$. W praktyce modele liniowe służą zarówno do regresji (gdy $y$ jest zmienną ciągłą), jak i do klasyfikacji (gdy $y$ jest zmienną dyskretną). W tym kursie modele liniowe są szczególnie ważne, ponieważ stanowią punkt odniesienia dla późniejszych metod (drzew, zespołów, SVM), a także umożliwiają wprowadzenie pojęć takich jak funkcja straty, estymacja parametrów, kompromis *bias–variance* oraz interpretacja współczynników.

W najprostszym ujęciu model liniowy zakłada, że predykcja jest liniową kombinacją cech. Dla obserwacji $i$ zapisujemy:

$$
\eta_i = \beta_0 + \beta_1 x_{i1} + \cdots + \beta_p x_{ip}
= \beta_0 + x_i^\top \beta,
$$

gdzie $\beta_0$ to wyraz wolny, a $\beta \in \mathbb{R}^p$ to wektor parametrów. Różne modele liniowe różnią się tym, jak $\eta_i$ jest powiązane z $y_i$ (funkcja łącząca) oraz jak definiowana jest funkcja straty, którą minimalizujemy w procesie uczenia.

## Regresja wieloraka

### Definicja modelu

Regresja wieloraka (regresja liniowa) jest modelem nadzorowanym, w którym zmienna docelowa $y$ jest ciągła. Zakładamy:

$$
y_i = \beta_0 + x_i^\top \beta + \varepsilon_i,
$$

gdzie $\varepsilon_i$ to składnik losowy (szum). W ML nie musimy eksponować interpretacji probabilistycznej, natomiast kluczowe jest to, że parametry $\beta_0, \beta$ uczymy tak, aby predykcje $\hat{y}_i$ były możliwie bliskie $y_i$.

### Estymacja jako problem uczenia nadzorowanego

Najczęściej stosuje się minimalizację sumy kwadratów błędów (*least squares*):

$$
\min_{\beta_0, \beta} \sum_{i=1}^n \left(y_i - (\beta_0 + x_i^\top \beta)\right)^2.
$$

Jest to klasyczny przykład uczenia nadzorowanego: posiadamy etykiety $y_i$, a algorytm uczy parametry minimalizujące zadaną funkcję straty.

::: {#exm-1}
Poniżej jest przykład regresji na rzeczywistych danych z Kaggle (dataset, nie konkurs): **Boston House Prices**. Plik danych to `boston.csv`, a kolumna zmiennej wyjściowej to `MEDV` (wartość domu).

Uwaga: Kaggle zwykle wymaga konta i tokenu API (`~/.kaggle/kaggle.json`).

Jeśli używasz Kaggle CLI, pobierz i rozpakuj dane:

```{bash}
#| eval: false
kaggle datasets download -d fedesoriano/the-boston-houseprice-data -p chapters/kaggle_boston

ls -la chapters/kaggle_boston
unzip -o chapters/kaggle_boston/*.zip -d chapters/kaggle_boston || true
```

Następnie przejdziemy do budowy modelu

```{python}
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from pathlib import Path
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression

data_path = Path("kaggle_boston/boston.csv")

df = pd.read_csv(data_path)

# Target jest znany z opisu danych
target = "MEDV"
if target not in df.columns:
    raise ValueError(
        f"Brakuje kolumny `{target}` w danych. Dostępne kolumny: " + ", ".join(df.columns)
    )

# Bierzemy wszystkie cechy liczbowe
df_num = df.select_dtypes(include=["number"]).copy()

tmp = df_num.dropna(subset=[target]).copy()
X = tmp.drop(columns=[target]).dropna()
y = tmp.loc[X.index, target]

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

reg = LinearRegression()
reg.fit(X_train, y_train)

# predykcje
y_pred_train = reg.predict(X_train)
y_pred_test = reg.predict(X_test)

# R^2
r2_train = reg.score(X_train, y_train)
r2_test = reg.score(X_test, y_test)

# „klasyczne” podsumowanie regresji (liczone na zbiorze treningowym)
n = X_train.shape[0]
p = X_train.shape[1]
dof = n - p - 1

resid = y_train.values - y_pred_train
sse = np.sum(resid**2)  # sum of squared errors
tss = np.sum((y_train.values - y_train.mean())**2)
ssr = tss - sse         # sum of squares regression

sigma = np.sqrt(sse / dof)          # residual std. error (RSE)
F = (ssr / p) / (sse / dof)         # F-statistic dla H0: beta_1=...=beta_p=0

summary = pd.DataFrame({
    "metric": ["R2_train", "R2_test", "sigma(RSE)_train", "F_train", "n_train", "p", "dof"],
    "value":  [r2_train, r2_test, sigma, F, n, p, dof],
})
print(summary.to_string(index=False))

# wykres: y_true vs y_pred (zbiór testowy)
plt.figure(figsize=(6, 6))
plt.scatter(y_test, y_pred_test, alpha=0.7)
mn = min(y_test.min(), y_pred_test.min())
mx = max(y_test.max(), y_pred_test.max())
plt.plot([mn, mx], [mn, mx], linestyle="--")
plt.xlabel(f"y_true ({target})")
plt.ylabel("y_pred")
plt.title("Boston Housing (Kaggle): y_pred vs y_true (test)")
plt.tight_layout()
plt.show()
```

-   `R2_train` = 0.751, `R2_test` = 0.669 - model liniowy wyjaśnia ok. 75% wariancji zmiennej docelowej na treningu i ok. 67% na teście. Spadek jest znaczny, co może sugerować przeuczenie modelu.
-   `sigma (RSE)_train` = 4.735 - to oszacowanie odchylenia standardowego reszt (typowy błąd predykcji) na treningu. Interpretacja jednostek zależy od tego, czym jest target: jeśli target to klasyczny `MEDV` w tysiącach USD, to przeciętny błąd rzędu \~4.7 tys. USD na treningu. To jest „typowa” skala odchyłki predykcji od wartości rzeczywistej w modelu liniowym.
-   `F_train` = 90.43 przy `p` = 13 i dof = 390 - statystyka $F$ testuje hipotezę zerową: wszystkie współczynniki nachyleń są równe 0 (model nie wnosi nic ponad średnią). Tak wysoka wartość $F$ oznacza, że model jako całość jest istotny statystycznie (praktycznie na pewno p-value $\ll 0.001$) — czyli przynajmniej część cech ma realny związek z targetem.

Ponieważ model wykazuje delikatne znamiona przeuczenia zastosujemy do niego regularyzacje.
:::

## Regularyzacja modeli liniowych

### Cel stosowania

W praktyce modele liniowe mogą cierpieć na przeuczenie, niestabilność estymacji (zwłaszcza przy silnej współliniowości cech) oraz zbyt dużą wariancję współczynników. Regularyzacja dodaje do funkcji straty karę za zbyt duże wartości parametrów, co stabilizuje estymację i poprawia uogólnianie.

Ogólny zapis problemu z regularyzacją:

$$
\min_{\beta_0, \beta} \mathcal{L}(y, X; \beta_0, \beta) + \lambda \, \mathcal{P}(\beta),
$$

gdzie $\mathcal{L}$ to funkcja straty (np. MSE w regresji lub log-loss w regresji logistycznej), $\mathcal{P}(\beta)$ to kara, a $\lambda \ge 0$ kontroluje siłę regularyzacji. Zwykle nie karzemy wyrazu wolnego $\beta_0$.

### Ridge regression (kara $L_2$)

*Ridge regression* (Tikhonov) stosuje karę $L_2$:

$$
\min_{\beta_0, \beta} \mathcal{L}(y, X; \beta_0, \beta) + \lambda \|\beta\|_2^2.
$$

Współczynniki są „kurczone” w stronę zera, ale zwykle nie stają się dokładnie równe zeru. Ridge jest szczególnie użyteczny przy współliniowości cech, bo stabilizuje estymacje i obniża wariancję.

### LASSO (kara $L_1$)

LASSO (ang. *Least Absolute Shrinkage and Selection Operator*) używa kary $L_1$:

$$
\min_{\beta_0, \beta} \mathcal{L}(y, X; \beta_0, \beta) + \lambda \|\beta\|_1.
$$

Kara L1 sprzyja rozwiązaniom rzadkim — część współczynników staje się dokładnie zerowa. Dzięki temu LASSO pełni jednocześnie rolę selekcji cech.

### Elastic Net (kara mieszana)

Elastic Net łączy zalety Ridge i LASSO:

$$
\min_{\beta_0, \beta}  \mathcal{L}(y, X; \beta_0, \beta) + \lambda \left(\alpha \|\beta\|_1 + (1-\alpha)\|\beta\|_2^2\right),
$$

gdzie $\alpha \in [0,1]$ steruje proporcją kary $L_1$ do $L_2$. Elastic Net jest szczególnie przydatny, gdy mamy wiele skorelowanych cech: LASSO wybiera pojedyncze zmienne z grupy, a Elastic Net częściej zachowuje całe grupy w postaci „współdzielonego” kurczenia.

### Interpretacja i praktyka

Regularyzacja polega na tym, że do klasycznej funkcji straty dodajemy karę za „zbyt duże” współczynniki. W modelach liniowych i logistycznych najczęściej spotkasz kary typu ($L_2$) (ridge) albo ($L_1$) (lasso). W ujęciu matematycznym dla regresji liniowej jest to odpowiednio:

$$
\min_{\beta_0,\beta} \sum_{i=1}^n (y_i - \beta_0 - x_i^\top \beta)^2 + \lambda |\beta|_2^2
\quad\text{(ridge)}
$$

albo

$$
\min_{\beta_0,\beta} \sum_{i=1}^n (y_i - \beta_0 - x_i^\top \beta)^2 + \lambda |\beta|_1
\quad\text{(lasso)}
$$

Analogicznie w regresji logistycznej zamiast sumy kwadratów masz stratę logarytmiczną, ale idea kary jest identyczna. Parametr $\lambda \ge 0$ steruje „siłą” regularyzacji: im większy, tym mocniej model preferuje małe współczynniki.

W modelu bez regularyzacji współczynniki $\hat\beta$ są dobierane tak, aby możliwie najlepiej dopasować dane uczące (minimalizować samą stratę). W przypadku danych z szumem, współliniowością cech lub dużą liczbą predyktorów, takie dopasowanie może prowadzić do tego, że współczynniki stają się „niestabilne”: niewielka zmiana danych (inna próba, inny podział train/test) daje wyraźnie inne $\hat\beta$. Regularyzacja celowo ogranicza swobodę modelu, „ściągając” współczynniki w stronę zera. To powoduje, że estymator staje się obciążony (*biased*) – współczynniki nie są już czystym odzwierciedleniem relacji w danych, bo zostały sztucznie zmniejszone przez karę. Jednocześnie znacząco spada wariancja estymatora: współczynniki są bardziej stabilne, a predykcje częściej lepiej generalizują na dane testowe. To klasyczny kompromis *bias–variance*: akceptujemy pewną stronniczość w zamian za mniejszą wrażliwość na szum.

W praktyce interpretacja jest taka: w modelu regularyzowanym nie traktujesz wartości $\beta_j$ jako „czystego” oszacowania wpływu cechy $x_j$ (w sensie klasycznej regresji), tylko jako wynik kompromisu pomiędzy dopasowaniem i prostotą modelu. Szczególnie przy silnej regularyzacji współczynniki należy interpretować ostrożnie: „to jest kierunek i względna siła sygnału po uwzględnieniu kary”, a nie „dokładna zmiana oczekiwanej wartości $y$ na jednostkę $x_j$”.

Gdy $\lambda \to 0$, kara zanika i wracamy do klasycznego uczenia bez regularyzacji: w regresji liniowej do OLS, w logistycznej do MLE bez kary. Wtedy współczynniki są „najmniej ściągnięte”, ale mogą być niestabilne (szczególnie przy współliniowości i dużym $p$). Gdy $\lambda$ rośnie, kara zaczyna dominować i wymusza zmniejszanie współczynników. Dla ridge ($L_2$) współczynniki są płynnie „kurczone” w kierunku zera, ale zwykle nie są równe zero. Dla lasso ($L_1$) część współczynników może stać się dokładnie równa zero, co prowadzi do selekcji zmiennych. Przy bardzo dużym $\lambda$ model może w praktyce „zrezygnować” z większości sygnału i zbliżyć się do modelu prawie stałego (predykcja głównie przez $\beta_0$).

Regularyzacja karze współczynniki, a nie bezpośrednio cechy. Ponieważ współczynnik $\beta_j$ jest ściśle powiązany ze skalą $x_j$, brak standaryzacji powoduje, że kara działa niesprawiedliwie względem zmiennych o różnych jednostkach.

Załóżmy dwie cechy niosące podobną informację, ale w różnych skalach:

-   $x_1$ - „dochód” rzędu dziesiątek tysięcy,
-   $x_2$ - „udział zwrotów” w zakresie 0–1.

Aby obie cechy miały podobny wpływ na $\eta$, model bez regularyzacji może dopasować:

-   mały współczynnik przy dochodzie (bo sama cecha ma duże liczby),
-   duży współczynnik przy zmiennej 0–1 (bo cecha jest mała).

Jeżeli dodamy karę typu ridge $\lambda \sum_j \beta_j^2$, to duży współczynnik przy $x_2$ zostanie ukarany dużo silniej niż mały współczynnik przy $x_1$, mimo że obie cechy mogą być równie istotne. W efekcie model może preferować cechy o dużej skali nie dlatego, że są lepsze, tylko dlatego, że wymagają mniejszych $|\beta_j|$, a więc „taniej” przechodzą przez karę. To jest artefakt skali, a nie właściwość danych.

Standardyzacja usuwa ten problem, sprowadzając każdą cechę do porównywalnej skali, najczęściej:

$$
z_{ij} = \frac{x_{ij}-\mu_j}{\sigma_j}.
$$

Po standaryzacji „jednostka” każdej cechy jest porównywalna (1 odchylenie standardowe), więc kara na współczynnikach działa symetrycznie. Dlatego w praktyce dla regresji logistycznej, ridge/lasso oraz większości modeli liniowych z regularyzacją standaryzacja jest traktowana jako element obowiązkowy.

W ujęciu uczenia maszynowego regularyzacja jest narzędziem kontrolowania złożoności modelu. $\lambda$ staje się hiperparametrem, który dobiera się na podstawie jakości generalizacji (np. przez walidację). Wraz ze wzrostem $\lambda$ model staje się prostszy i stabilniejszy, ale może gorzej dopasowywać dane treningowe. Standaryzacja jest integralną częścią tego procesu, bo zapewnia, że dobór $\lambda$ i sama kara mają sens niezależnie od jednostek i skali cech.

```{python}
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from pathlib import Path

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import ElasticNet
from sklearn.metrics import r2_score, mean_squared_error

# ----------------------------
# 1) Pipeline: standaryzacja + ElasticNet
# ----------------------------
pipe = Pipeline(steps=[
    ("scaler", StandardScaler()),
    ("model", ElasticNet(max_iter=50_000, random_state=42))
])

# ----------------------------
# 2) Kalibracja (GridSearch): lambda i alpha
#    - lambda -> model__alpha
#    - alpha  -> model__l1_ratio
# ----------------------------
# Sensowna siatka: logarytmiczna dla lambda
lambda_grid = np.logspace(-4, 2, 25)      # 1e-4 ... 1e2
alpha_grid = np.linspace(0.0, 1.0, 11)    # 0, 0.1, ..., 1.0

param_grid = {
    "model__alpha": lambda_grid,          # lambda (siła kary)
    "model__l1_ratio": alpha_grid         # alpha (mieszanka L1/L2)
}

# CV: w regresji klasycznie KFold; domyślnie GridSearchCV użyje KFold
# Skoring: R^2, bo tak raportujesz w przykładzie
search = GridSearchCV(
    estimator=pipe,
    param_grid=param_grid,
    scoring="r2",
    cv=5,
    n_jobs=-1
)

search.fit(X_train, y_train)

best_model = search.best_estimator_
best_params = search.best_params_
best_cv_r2 = search.best_score_

print("Najlepsze hiperparametry (CV):")
print(f"  lambda (model__alpha)   = {best_params['model__alpha']:.6g}")
print(f"  alpha  (model__l1_ratio)= {best_params['model__l1_ratio']:.3f}")
print(f"  CV R^2 (mean)           = {best_cv_r2:.4f}")

# ----------------------------
# 3) Ocena na train/test + metryki jak w Twoim stylu
# ----------------------------
y_pred_train = best_model.predict(X_train)
y_pred_test = best_model.predict(X_test)

r2_train = r2_score(y_train, y_pred_train)
r2_test = r2_score(y_test, y_pred_test)

# Dodatkowo RMSE (często użyteczne w regresji)
rmse_train = np.sqrt(mean_squared_error(y_train, y_pred_train))
rmse_test = np.sqrt(mean_squared_error(y_test, y_pred_test))

summary = pd.DataFrame({
    "metric": ["R2_train", "R2_test", "RMSE_train", "RMSE_test", "best_lambda", "best_alpha(l1_ratio)"],
    "value":  [r2_train, r2_test, rmse_train, rmse_test, best_params["model__alpha"], best_params["model__l1_ratio"]],
})
print("\nPodsumowanie ElasticNet:")
print(summary.to_string(index=False))

# ----------------------------
# 4) Współczynniki po regularyzacji (już po standaryzacji)
# ----------------------------
# Uwaga: współczynniki dotyczą cech po standaryzacji (porównywalne między sobą).
coefs = best_model.named_steps["model"].coef_
coef_table = pd.DataFrame({"feature": X.columns, "coef": coefs}).sort_values("coef", key=np.abs, ascending=False)

print("\nNajwiększe (bezwzględnie) współczynniki ElasticNet:")
print(coef_table.head(10).to_string(index=False))

# ----------------------------
# 5) Wykres: y_true vs y_pred (test)
# ----------------------------
plt.figure(figsize=(6, 6))
plt.scatter(y_test, y_pred_test, alpha=0.7)
mn = min(y_test.min(), y_pred_test.min())
mx = max(y_test.max(), y_pred_test.max())
plt.plot([mn, mx], [mn, mx], linestyle="--")
plt.xlabel(f"y_true ({target})")
plt.ylabel("y_pred")
plt.title("Boston Housing (Kaggle): ElasticNet y_pred vs y_true (test)")
plt.tight_layout()
plt.show()

# ----------------------------
# 6) (Opcjonalnie) mapa wyników: R^2 w funkcji (lambda, alpha)
# ----------------------------
# To jest przydatne dydaktycznie: pokazuje krajobraz hiperparametrów.
results = pd.DataFrame(search.cv_results_)
pivot = results.pivot_table(
    index="param_model__l1_ratio",
    columns="param_model__alpha",
    values="mean_test_score"
).sort_index()

plt.figure(figsize=(10, 4))
plt.imshow(pivot.values, aspect="auto")
plt.colorbar(label="mean CV R^2")
plt.yticks(range(pivot.shape[0]), [f"{v:.1f}" for v in pivot.index])
plt.xticks(range(pivot.shape[1]), [f"{v:.0e}" for v in pivot.columns], rotation=45, ha="right")
plt.ylabel("alpha (l1_ratio)")
plt.xlabel("lambda (model__alpha)")
plt.title("ElasticNet: średnie CV R^2 dla (lambda, alpha)")
plt.tight_layout()
plt.show()
```

Widać wyraźnie, że kalibaracja parametrów $\lambda$ i $\alpha$ sprowadziła praktycznie model do regresji grzbietowej (*ridge*) z parametrem $\lambda=0.01$. Wyniki porównania $R^2$ pomiędzy zbiorem treningowym i testowym po regularyzacji się nie zmieniły. Dalej występuje różnica $\approx 0.08$. To pokazuje, że sama regularyzacja nie wystarczy do usunięcia tego przeuczenia (swoją drogą nie jest ono bardzo duże). Na koniec aby przekonać się jak modele (*raw* i *ridge*) są wrażliwe na podział zbioru dokonamy ich porównania z wykorzystaniem walidacji krzyżowej.

```{python}
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.model_selection import RepeatedKFold, cross_val_score
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression, Ridge

# ----------------------------
# 1) RepeatedKFold (powtarzana walidacja krzyżowa)
# ----------------------------
cv = RepeatedKFold(n_splits=5, n_repeats=20, random_state=42)

# ----------------------------
# 2) Modele do porównania
#    - OLS: LinearRegression (bez regularyzacji)
#    - Ridge: regularyzacja L2; skalowanie w pipeline (ważne)
# ----------------------------
ols = Pipeline(steps=[
    ("scaler", StandardScaler()),
    ("model", LinearRegression())
])

ridge = Pipeline(steps=[
    ("scaler", StandardScaler()),
    ("model", Ridge(alpha=0.01, random_state=42))
])

# ----------------------------
# 3) R^2 w RepeatedKFold
# ----------------------------
scores_ols = cross_val_score(ols, X, y, cv=cv, scoring="r2", n_jobs=-1)
scores_ridge = cross_val_score(ridge, X, y, cv=cv, scoring="r2", n_jobs=-1)

# Podsumowanie liczbowe: średnia, odchylenie, kwartyle
summary = pd.DataFrame({
    "model": ["LinearRegression (OLS)", "Ridge (alpha=0.01)"],
    "mean_R2": [scores_ols.mean(), scores_ridge.mean()],
    "std_R2": [scores_ols.std(ddof=1), scores_ridge.std(ddof=1)],
    "q25": [np.quantile(scores_ols, 0.25), np.quantile(scores_ridge, 0.25)],
    "median": [np.quantile(scores_ols, 0.50), np.quantile(scores_ridge, 0.50)],
    "q75": [np.quantile(scores_ols, 0.75), np.quantile(scores_ridge, 0.75)],
})

print(summary.to_string(index=False))

# ----------------------------
# 4) Boxplot wyników
# ----------------------------
plt.figure(figsize=(7, 4))
plt.boxplot([scores_ols, scores_ridge], labels=["OLS", "Ridge"], showfliers=True)
plt.ylabel("R^2 (CV)")
plt.title("RepeatedKFold: rozkład R^2 dla OLS vs Ridge")
plt.tight_layout()
plt.show()

# ----------------------------
# 5) „Stabilizacja” wprost: porównanie wariancji wyników
# ----------------------------
print("\nStabilizacja (mniejsza zmienność wyników CV jest korzystna):")
print(f"Std(R^2) OLS  : {scores_ols.std(ddof=1):.4f}")
print(f"Std(R^2) Ridge: {scores_ridge.std(ddof=1):.4f}")
```

Wyniki są niemal identyczne, co pokazuje, że regularyzacja modelu nie pomaga wyeliminować delikatnego przeuczenia.

Pierwszy powód to **naturalna różnica między błędem treningowym a błędem generalizacji**. Model jest dopasowywany tak, aby minimalizować stratę na treningu, więc na treningu prawie zawsze będzie lepiej niż na danych niewidzianych. Nawet gdy model nie jest „przeuczony” w sensie patologicznym, pojawi się luka generalizacyjna, bo test jest inną próbą z tej samej populacji i zawiera inny układ szumu losowego.

Drugi powód to **ograniczona zgodność modelu z rzeczywistą zależnością**. Regresja liniowa zakłada liniowość i addytywność wpływów cech (bez nieliniowości i bez interakcji, jeśli ich nie dodasz). Jeśli prawdziwa relacja jest częściowo nieliniowa (co w danych nieruchomości jest częste), to model „radzi sobie” na treningu, ale na teście spada, bo dopasowanie do przypadkowego układu obserwacji w treningu nie przenosi się idealnie na inną próbę. To jest bardziej kwestia **bias/misspecification** niż „overfittingu z powodu zbyt dużej złożoności”, ale objaw w metrykach jest podobny.

Trzeci powód to **wariancja pojedynczego podziału train/test**. Ta różnica (około 0.08 w $R^2$) może w dużej mierze wynikać z tego, że akurat wylosowany test jest „trudniejszy” (np. zawiera więcej obserwacji z krańców rozkładu). Właśnie dlatego `RepeatedKFold` jest lepszym narzędziem diagnostycznym: u nas średnie CV ($R^2 \approx 0.713$) wskazuje, że wynik testowy 0.668 nie jest już tak odległy, tylko może być po prostu mniej korzystnym splitem.

Czwarty potencjalny powód to **współliniowość i niestabilność współczynników**, która nie zawsze przekłada się na wyraźną zmianę $R^2$, ale może zwiększać wrażliwość na konkretny podział danych. Ridge z bardzo małym $\lambda$ nie zmienia u nas jakości ani wariancji metryki, więc to sugeruje, że w tym konkretnym ustawieniu współliniowość nie jest dominującym źródłem luki — ale nadal może wpływać na interpretację $\beta$ i na zachowanie w innych splitach.

## Regresja logistyczna

### Definicja modelu

Regresja logistyczna jest modelem liniowym przeznaczonym do klasyfikacji binarnej. Zakładamy, że zmienna docelowa $y \in {0,1}$, a model opisuje prawdopodobieństwo klasy wyróżnionej (1). Najpierw definiujemy predyktor liniowy:

$$
\eta_i = \beta_0 + x_i^\top \beta,
$$

a następnie mapujemy go do $[0,1]$ funkcją logistyczną:

$$
p_i = \mathbb{P}(y_i = 1 \mid x_i) = \sigma(\eta_i) = \frac{1}{1 + e^{-\eta_i}}.
$$

Decyzję klasyfikacyjną podejmujemy przez ustawienie progu $t$ (zwykle 0.5):

$$
\hat{y}_i​=
\begin{cases}
1,\; p_i\geq t,\\
0,\; p_i<t.
\end{cases}
$$

Ponieważ $y_i \in {0,1}$, naturalnym modelem probabilistycznym dla $y_i$ przy danym $x_i$ jest rozkład Bernoulliego:

$$
y_i \mid x_i \sim \text{Bernoulli}(p_i),
\quad\text{czyli}\quad
\mathbb{P}(y_i\mid x_i) = p_i^{y_i}(1-p_i)^{1-y_i}.
$$

To prowadzi do funkcji wiarygodności dla całej próby (przy założeniu niezależności obserwacji):

$$
L(\beta_0,\beta) = \prod_{i=1}^n p_i^{y_i}(1-p_i)^{1-y_i}.
$$

W ujęciu statystycznym „uczenie” parametrów polega na maksymalizacji wiarygodności $L$. Ponieważ iloczyny są niewygodne obliczeniowo, przechodzi się na logartym wiarygodności:

$$
\ell(\beta_0,\beta) = \log L(\beta_0,\beta)
= \sum_{i=1}^n \left[y_i\log(p_i) + (1-y_i)\log(1-p_i)\right].
$$

Maksymalizacja $\ell$ jest równoważna minimalizacji jej negacji, co w ML nazywamy **stratą logarytmiczną** (*log loss*) albo **entropią krzyżową** (*binary cross-entropy*):

$$
\min_{\beta_0,\beta} -\sum_{i=1}^n \left[y_i \log(p_i) + (1-y_i)\log(1-p_i)\right].
$$

Intuicyjnie ta strata „nagradza” model, gdy przypisuje wysokie prawdopodobieństwo klasie, która rzeczywiście wystąpiła, i „karze” go mocno, gdy model jest bardzo pewny, ale myli się. Na przykład, jeśli $y_i=1$ i model daje $p_i=0.99$, składnik straty jest mały; jeśli natomiast $y_i=1$ i model daje $p_i=0.01$, to $-\log(0.01)$ jest duże, więc kara jest silna. Dzięki temu log loss nie jest tylko miarą „trafione/nie trafione”, lecz ocenia jakość probabilistyczną predykcji, co jest szczególnie ważne w klasyfikacji.

W praktyce nie istnieje prosty wzór zamknięty na $(\beta_0,\beta)$ analogiczny do regresji liniowej OLS. Dlatego optymalizację wykonuje się numerycznie. Najczęściej stosuje się metody oparte o gradient (i często pochodne drugiego rzędu). Kluczowym faktem jest, że funkcja straty w regresji logistycznej jest wypukła względem $(\beta_0,\beta)$, więc metody gradientowe mają dobre własności: przy poprawnej implementacji dążą do globalnego minimum. Dla kompletności warto zapisać postać gradientu względem $\beta$ (pomijając wyraz wolny dla czytelności). Jeśli $X$ to macierz cech, a $p$ wektor $p_i$, to gradient ma postać:

$$
\nabla_{\beta}  \Big(-\ell(\beta)\Big) = X^\top (p - y),
$$

czyli różnica między prognozowanymi prawdopodobieństwami a rzeczywistymi etykietami, „zebrana” przez cechy. To dobrze podkreśla charakter uczenia: gdy model przeszacowuje prawdopodobieństwo klasy 1 (duże $p_i$ przy $y_i=0$), gradient pcha parametry w kierunku zmniejszenia $\eta_i$ dla tych obserwacji, i odwrotnie.

Wreszcie, jest to model nadzorowany w sensie ML, ponieważ parametry $(\beta_0,\beta)$ uczymy na oznakowanych parach $(x_i,y_i)$ poprzez minimalizację funkcji straty. W odróżnieniu od metod nienadzorowanych, tutaj etykieta $y_i$ jest bezpośrednio składnikiem funkcji celu, a „uczenie” polega na takim doborze parametrów, aby model możliwie dobrze odtwarzał zależność między cechami a klasą docelową, nie tylko na danych uczących, ale przede wszystkim na danych niewidzianych.

::: {#exm-2}
```{python}
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from datasets import load_dataset

from sklearn.model_selection import StratifiedKFold, cross_val_predict
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression

from sklearn.metrics import (
    confusion_matrix,
    accuracy_score, precision_score, recall_score, f1_score,
    roc_auc_score, roc_curve
)

# =========================
# 1) Wczytanie danych z Hugging Face
# =========================
# Dataset ma jeden split "train" (683 obserwacje) i target "is_cancer"
ds = load_dataset("mstz/breast", "cancer")["train"]  # :contentReference[oaicite:1]{index=1}
df = ds.to_pandas()

target = "is_cancer"
y = df[target].astype(int)

X = df.drop(columns=[target])

# Minimalnie: imputacja (gdyby były braki) + standaryzacja + logreg
pipe = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="median")),
    ("scaler", StandardScaler()),
    ("model", LogisticRegression(max_iter=2000, solver="lbfgs"))
])

# =========================
# 2) Walidacja krzyżowa i out-of-fold predykcje prawdopodobieństw
# =========================
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# out-of-fold: każda obserwacja ma proba z modelu, który jej "nie widział" w treningu
proba_oof = cross_val_predict(
    pipe, X, y,
    cv=cv,
    method="predict_proba"
)[:, 1]

# Predykcja klasy przy standardowym progu 0.5
thr_default = 0.50
y_pred_default = (proba_oof >= thr_default).astype(int)

# =========================
# 3) Metryki + confusion matrix (PRZED kalibracją progu)
# =========================
def compute_metrics(y_true, y_pred, proba):
    return {
        "accuracy":  accuracy_score(y_true, y_pred),
        "precision": precision_score(y_true, y_pred, zero_division=0),
        "recall":    recall_score(y_true, y_pred, zero_division=0),
        "f1":        f1_score(y_true, y_pred, zero_division=0),
        "roc_auc":   roc_auc_score(y_true, proba),
    }

metrics_default = compute_metrics(y, y_pred_default, proba_oof)
cm_default = confusion_matrix(y, y_pred_default)

print("PRZED kalibracją progu (threshold=0.50) – metryki OOF:")
for k, v in metrics_default.items():
    print(f"  {k:9s}: {v:.4f}")
print("\nMacierz pomyłek (rows=true, cols=pred):\n", cm_default)

# =========================
# 4) ROC curve (z OOF proba)
# =========================
fpr, tpr, thresholds = roc_curve(y, proba_oof)
auc = roc_auc_score(y, proba_oof)

plt.figure(figsize=(6, 5))
plt.plot(fpr, tpr, label=f"ROC (AUC={auc:.3f})")
plt.plot([0, 1], [0, 1], linestyle="--")
plt.xlabel("False Positive Rate (1 - specificity)")
plt.ylabel("True Positive Rate (recall / sensitivity)")
plt.title("ROC curve – Logistic Regression (OOF)")
plt.legend()
plt.tight_layout()
plt.show()

# =========================
# 5) Kalibracja progu (Youden’s J)
#    J = TPR - FPR = sensitivity + specificity - 1
# =========================
J = tpr - fpr
idx = np.argmax(J)
thr_calibrated = thresholds[idx]

print(f"\nSkalibrowany próg (Youden J): {thr_calibrated:.4f}")

y_pred_cal = (proba_oof >= thr_calibrated).astype(int)

metrics_cal = compute_metrics(y, y_pred_cal, proba_oof)
cm_cal = confusion_matrix(y, y_pred_cal)

print("\nPO kalibracji progu (Youden J) – metryki OOF:")
for k, v in metrics_cal.items():
    print(f"  {k:9s}: {v:.4f}")
print("\nMacierz pomyłek (rows=true, cols=pred):\n", cm_cal)

# =========================
# 6) Porównanie confusion matrices: wizualizacja
# =========================
def plot_cm(cm, title):
    plt.figure(figsize=(4.5, 4))
    plt.imshow(cm, aspect="auto")
    plt.title(title)
    plt.colorbar()
    plt.xticks([0, 1], ["pred 0", "pred 1"])
    plt.yticks([0, 1], ["true 0", "true 1"])
    for i in range(2):
        for j in range(2):
            plt.text(j, i, cm[i, j], ha="center", va="center")
    plt.tight_layout()
    plt.show()

plot_cm(cm_default, "Confusion matrix – threshold=0.50 (OOF)")
plot_cm(cm_cal, f"Confusion matrix – threshold={thr_calibrated:.3f} (OOF)")
```
:::

## Modele dyskryminacyjne

Analiza dyskryminacyjna to klasa klasycznych metod klasyfikacji, które bardzo naturalnie wpisują się w logikę uczenia nadzorowanego: mając etykiety klas $y \in \{1,\dots,K\}$, uczymy parametry rozkładów cech w każdej klasie (część „generatywna”), a następnie stosujemy regułę Bayesa do przypisania nowej obserwacji do najbardziej prawdopodobnej klasy. W odróżnieniu od regresji logistycznej, która modeluje bezpośrednio $\mathbb{P}(y\mid x)$, LDA/QDA modelują $\mathbb{P}(x\mid y)$ oraz priory $\mathbb{P}(y)$, po czym wyznaczają $\mathbb{P}(y\mid x)$ pośrednio.

Historycznie do analizy dyskryminacyjnej dochodzono co najmniej dwiema drogami. Pierwsza (zwykle kojarzona z Fisherem) wynikała z problemu znalezienia kierunku projekcji, na którym klasy są najlepiej rozdzielone w sensie stosunku wariancji „między klasami” do wariancji „wewnątrz klas” [@fisher1936]. Druga droga [@wardjr1963]ma charakter „decyzyjno-probabilistyczny”: zaczynamy od założeń o rozkładach klas (najczęściej normalnych) i z reguły Bayesa wyprowadzamy regułę klasyfikacji w postaci porównania pewnych funkcji punktacji dla klas. Ten drugi sposób jest bardzo bliski temu, jak dziś prezentuje się LDA/QDA w uczeniu maszynowym (jako klasyfikatory generatywne) [^03-klasyfikacja-liniowa-1].

[^03-klasyfikacja-liniowa-1]: Fisher (1936) pokazał ideę rozdzielania klas przez znalezienie projekcji maksymalizującej separację (stosunek wariancji między klasami do wariancji wewnątrz klas) i zastosował ją do danych irysów. Ward (1963) jest często przywoływany w kontekście rozwoju metod grupowania i analiz wielowymiarowych; w praktyce dydaktycznej warto podkreślić, że równolegle do „drogi Fishera” (kryterium separacji/projekcji) rozwijała się droga „proceduralna” w analizie wielowymiarowej: najpierw grupowanie/definicja grup, potem konstrukcja funkcji dyskryminacyjnych do klasyfikacji i rozumienia różnic między grupami.

### Założenia modelu: normalność wielowymiarowa i rozkłady apriori klas

W klasycznym wariancie zakładamy, że dla każdej klasy $k$ wektor cech ma rozkład normalny:

$$
x \mid (y=k) \sim \mathcal{N}(\mu_k, \Sigma_k),
\qquad \pi_k = \mathbb{P}(y=k).
$$

Parametry $\mu_k$, $\Sigma_k$ oraz $\pi_k$ są nieznane i są uczone na danych treningowych z wykorzystaniem etykiet klas (czyli w pełni nadzorowanie). W praktyce: $\pi_k$ estymujemy jako częstość klasy w treningu, $\mu_k$ jako średnią wektora cech w klasie, a $\Sigma$ lub $\Sigma_k$ jako macierze kowariancji (wspólne lub klasowe, zależnie od wariantu).

### Funkcje dyskryminacyjne: po co są i co robią?

Reguła Bayesa mówi, że przy równych kosztach błędu optymalnie klasyfikujemy do klasy o największym prawdopodobieństwie a posteriori:

$$
\hat{y}(x) = \arg\max_k \mathbb{P}(y=k\mid x).
$$

Ponieważ

$$
\mathbb{P}(y=k\mid x) \propto \pi_k \, f_k(x),
$$

gdzie $f_k(x)$ to gęstość $\mathcal{N}(\mu_k,\Sigma_k)$, wygodniej porównywać logarytmy (rosną monotonicznie), definiując funkcję dyskryminacyjną:

$$
\delta_k(x) \;=\; \log \pi_k + \log f_k(x) \;+\; \text{(stała niezależna od }k\text{)}.
$$

Funkcja dyskryminacyjna jest więc „punktacją” klasy: im większa $\delta_k(x)$, tym bardziej model preferuje klasę $k$ dla obserwacji $x$. Klasyfikacja sprowadza się do:

$$
\hat{y}(x) = \arg\max_k \delta_k(x).
$$

### LDA vs QDA

QDA - zakładamy osobną macierz kowariancji dla każdej klasy: $\Sigma_k$ jest w pełni dowolna (symetryczna dodatnio określona) i estymowana osobno. Wtedy

$$
\delta_k(x)
=
-\frac{1}{2}\log|\Sigma_k|
-\frac{1}{2}(x-\mu_k)^\top \Sigma_k^{-1}(x-\mu_k)
+\log\pi_k.
$$

Ponieważ składnik $(x-\mu_k)^\top \Sigma_k^{-1}(x-\mu_k)$ jest formą kwadratową, granice decyzyjne między klasami są kwadratowe (nieliniowe w $x$). QDA jest bardziej elastyczna (potrafi modelować klasy o różnych „kształtach” i orientacjach w przestrzeni cech), ale płaci za to większą liczbą parametrów, co wymaga większej próbki treningowej dla stabilnej estymacji.

W LDA zakładamy wspólną kowariancję dla klas:

$$
\Sigma_k = \Sigma \quad \text{dla każdego }k.
$$

Wtedy człony kwadratowe w $x$ „redukują się” w porównaniu między klasami i funkcja dyskryminacyjna upraszcza się do postaci liniowej:

$$
\delta_k(x) = x^\top \Sigma^{-1}\mu_k - \frac{1}{2}\mu_k^\top \Sigma^{-1}\mu_k + \log\pi_k.
$$

Granice decyzyjne są liniowe (hiperpłaszczyzny), bo $\delta_k(x)$ jest liniowa ze względu na $x$. LDA ma zwykle mniejszą wariancję estymacji (mniej parametrów niż QDA), dlatego często jest konkurencyjna nawet wtedy, gdy prawdziwa zależność nie jest idealnie zgodna z założeniami.

### $\Sigma=I$, $\Sigma_k=\Sigma$, $\Sigma_k$ dowolne

W praktyce warto widzieć analizę dyskryminacyjną jako rodzinę modeli wynikającą z tego, jak restrykcyjnie opisujemy kowariancję.

1.  $\Sigma = I$. To najbardziej restrykcyjny wariant. Oznacza, że w każdej klasie cechy są „niezależne” i mają tę samą wariancję (w odpowiedniej skali). Wtedy odległość Mahalanobisa redukuje się do euklidesowej i reguła klasyfikacji staje się bardzo bliska *nearest centroid* (najbliższe centrum klasy). Jest to model prosty i często zaskakująco skuteczny po standaryzacji, ale może być niedopasowany, gdy cechy są skorelowane.
2.  $\Sigma_k = \Sigma$ (LDA: wspólna, ale dowolna macierz kowariancji). To kompromis: dopuszczamy korelacje i różne wariancje cech, ale zakładamy, że „kształt rozkładu” w przestrzeni cech jest taki sam dla wszystkich klas, tylko przesunięty o różne $\mu_k$. Wtedy granice są liniowe.
3.  $\Sigma_k$ dowolne (QDA). Najbardziej elastyczne: każda klasa ma własny „kształt” i orientację elipsoidy kowariancji. Granice są kwadratowe. Ten wariant jest najbardziej wrażliwy na małe próby (estymacja $\Sigma_k^{-1}$ bywa niestabilna), dlatego często wymaga albo większej liczby obserwacji, albo regularizacji (np. RDA – regularized discriminant analysis; klasycznie opisane przez Friedmana). ￼

Uwaga praktyczna: spotyka się też wariant pośredni $\Sigma$ diagonalna (brak korelacji, ale różne wariancje cech). Jest to bliskie „gaussowskiemu naiwnemu Bayesowi” w wersji z normalnymi rozkładami cech.

### Jak estymuje się parametry w uczeniu nadzorowanym?

Dla danych treningowych $\{(x_i,y_i)\}_{i=1}^n$, gdzie $n_k$ to liczba obserwacji w klasie $k$, standardowe estymatory (MLE) mają postać:

$$
\hat{\pi}_k = \frac{n_k}{n},
\qquad
\hat{\mu}_k = \frac{1}{n_k}\sum_{i:\,y_i=k} x_i.
$$

Dla LDA estymujemy wspólną kowariancję jako kowariancję „wewnątrzklasową” (*pooled covariance*):

$$
\hat{\Sigma}
=
\frac{1}{n-K}
\sum_{k=1}^K
\sum_{i:\,y_i=k}
(x_i-\hat{\mu}_k)(x_i-\hat{\mu}_k)^\top.
$$

Dla QDA estymujemy $\hat{\Sigma}_k$ osobno w każdej klasie:

$$
\hat{\Sigma}_k
=
\frac{1}{n_k-1}
\sum_{i:\,y_i=k}
(x_i-\hat{\mu}_k)(x_i-\hat{\mu}_k)^\top.
$$

Następnie do klasyfikacji używamy $\delta_k(x)$ z odpowiedniego wariantu (LDA/QDA). W praktyce w `scikit-learn` jest to realizowane wprost (fit → estymuje $\pi_k$, $\mu_k$, $\Sigma$ lub $\Sigma_k$; `predict`/`predict_proba` → liczy $\delta_k$ i normalizuje do prawdopodobieństw).

::: {#exm-3}
Poniżej przykład LDA i QDA na klasycznym zbiorze Iris dostępny na Hugging Face (`scikit-learn/iris`). Zbiór zawiera 150 obserwacji trzech gatunków irysów, opisanych czterema cechami liczbowymi. ￼

W przykładzie:

-   robimy podział train/test,
-   uczymy LDA i QDA,
-   raportujemy accuracy i macierz pomyłek,
-   pokazujemy log-loss jako miarę jakości probabilistycznej,
-   narysujemy brzegi decyzyjne obu metod.

```{python}
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from datasets import load_dataset
from sklearn.model_selection import train_test_split
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis
from sklearn.metrics import accuracy_score, confusion_matrix, log_loss
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

# 1) Wczytanie Iris z Hugging Face
ds = load_dataset("scikit-learn/iris")["train"]
df = ds.to_pandas()
df = df.iloc[:, 1:]  # Usunięcie pierwszej kolumny (Id)

target = "Species"
X = df.select_dtypes(include=["number"]).drop(columns=[target], errors="ignore")

# Kodujemy klasy do liczb (wymagane m.in. do rysowania brzegów decyzyjnych przez contourf)
y_cat = df[target].astype("category")
class_names = list(y_cat.cat.categories)
y = y_cat.cat.codes

# 2) Podział train/test (stratyfikacja dla klas)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.25, random_state=42, stratify=y
)

# 2b) Standaryzacja (fit TYLKO na train) – cały przykład działa na danych standaryzowanych
scaler = StandardScaler()
X_train_std = scaler.fit_transform(X_train)
X_test_std = scaler.transform(X_test)
X_train_std_df = pd.DataFrame(X_train_std, columns=X.columns, index=X_train.index)
X_test_std_df = pd.DataFrame(X_test_std, columns=X.columns, index=X_test.index)

# 3) LDA
lda = LinearDiscriminantAnalysis()
lda.fit(X_train_std_df, y_train)
pred_lda = lda.predict(X_test_std_df)
proba_lda = lda.predict_proba(X_test_std_df)

# 4) QDA
qda = QuadraticDiscriminantAnalysis()
qda.fit(X_train_std_df, y_train)
pred_qda = qda.predict(X_test_std_df)
proba_qda = qda.predict_proba(X_test_std_df)

# 5) Metryki i macierze pomyłek
acc_lda = accuracy_score(y_test, pred_lda)
acc_qda = accuracy_score(y_test, pred_qda)

cm_lda = confusion_matrix(y_test, pred_lda)
cm_qda = confusion_matrix(y_test, pred_qda)

print(f"LDA  accuracy: {acc_lda:.3f}, log_loss: {log_loss(y_test, proba_lda):.3f}")
print("LDA confusion matrix:\n", pd.DataFrame(cm_lda, index=class_names, columns=class_names))

print(f"\nQDA  accuracy: {acc_qda:.3f}, log_loss: {log_loss(y_test, proba_qda):.3f}")
print("QDA confusion matrix:\n", pd.DataFrame(cm_qda, index=class_names, columns=class_names))

# 6) Brzegi decyzyjne (wizualizacja 2D)
# Uwaga: Iris ma 4 cechy, a wykres 2D wymaga wyboru 2 osi.
feat_cols = list(X.columns[[0,2]])
all_feat_cols = list(X.columns)

def plot_decision_boundary_fullmodel_on_2features(
    ax,
    model,
    X_std_df,
    y_vis,
    all_feature_names,
    vary_feature_names,
    title,
    n_classes,
):
    # Decision boundary jako przekrój przestrzeni cech:
    # zmieniamy tylko 2 wybrane cechy, pozostałe ustawiamy na 0 (średnia po standaryzacji).
    x0 = X_std_df[vary_feature_names[0]].to_numpy()
    x1 = X_std_df[vary_feature_names[1]].to_numpy()

    x0_min, x0_max = x0.min() - 0.5, x0.max() + 0.5
    x1_min, x1_max = x1.min() - 0.5, x1.max() + 0.5

    xx0, xx1 = np.meshgrid(
        np.linspace(x0_min, x0_max, 300),
        np.linspace(x1_min, x1_max, 300),
    )

    base = np.zeros((xx0.size, len(all_feature_names)), dtype=float)
    idx0 = all_feature_names.index(vary_feature_names[0])
    idx1 = all_feature_names.index(vary_feature_names[1])
    base[:, idx0] = xx0.ravel()
    base[:, idx1] = xx1.ravel()

    grid_df = pd.DataFrame(base, columns=all_feature_names)
    zz = model.predict(grid_df).reshape(xx0.shape)

    levels = np.arange(n_classes + 1) - 0.5
    cmap = plt.get_cmap("tab10", n_classes)

    ax.contourf(xx0, xx1, zz, levels=levels, alpha=0.25, cmap=cmap)
    ax.scatter(x0, x1, c=y_vis, cmap=cmap, edgecolor="k", s=35)
    ax.set_xlabel(vary_feature_names[0])
    ax.set_ylabel(vary_feature_names[1])
    ax.set_title(title)

n_classes = int(np.unique(y).size)

fig, axes = plt.subplots(1, 2, figsize=(11, 4))
plot_decision_boundary_fullmodel_on_2features(
    axes[0],
    lda,
    X_test_std_df,
    y_test,
    all_feature_names=all_feat_cols,
    vary_feature_names=feat_cols,
    title="LDA – brzeg decyzyjny (przekrój po 2 cechach; model pełny)",
    n_classes=n_classes,
)
plot_decision_boundary_fullmodel_on_2features(
    axes[1],
    qda,
    X_test_std_df,
    y_test,
    all_feature_names=all_feat_cols,
    vary_feature_names=feat_cols,
    title="QDA – brzeg decyzyjny (przekrój po 2 cechach; model pełny)",
    n_classes=n_classes,
)
plt.tight_layout()
plt.show()

# 7) Brzegi decyzyjne w przestrzeni PCA(2)
# PCA robimy po standaryzacji.
# Brzeg decyzyjny liczymy w przestrzeni PC1/PC2, ale predykcje robi model uczony
# na pełnych cechach: PC-grid -> inverse_transform -> predykcja.
pca = PCA(n_components=2, random_state=42)
X_train_pca = pca.fit_transform(X_train_std)
X_test_pca = pca.transform(X_test_std)

pc_cols = ["PC1", "PC2"]
X_train_pca_df = pd.DataFrame(X_train_pca, columns=pc_cols, index=X_train.index)
X_test_pca_df = pd.DataFrame(X_test_pca, columns=pc_cols, index=X_test.index)

def plot_decision_boundary_fullmodel_in_pca2(
    ax,
    model,
    pca,
    X_pca_df,
    y_vis,
    orig_feature_names,
    title,
    n_classes,
):
    x0 = X_pca_df["PC1"].to_numpy()
    x1 = X_pca_df["PC2"].to_numpy()

    x0_min, x0_max = x0.min() - 0.5, x0.max() + 0.5
    x1_min, x1_max = x1.min() - 0.5, x1.max() + 0.5

    xx0, xx1 = np.meshgrid(
        np.linspace(x0_min, x0_max, 300),
        np.linspace(x1_min, x1_max, 300),
    )

    pc_grid = np.c_[xx0.ravel(), xx1.ravel()]
    grid_std = pca.inverse_transform(pc_grid)
    grid_std_df = pd.DataFrame(grid_std, columns=orig_feature_names)
    zz = model.predict(grid_std_df).reshape(xx0.shape)

    levels = np.arange(n_classes + 1) - 0.5
    cmap = plt.get_cmap("tab10", n_classes)

    ax.contourf(xx0, xx1, zz, levels=levels, alpha=0.25, cmap=cmap)
    ax.scatter(x0, x1, c=y_vis, cmap=cmap, edgecolor="k", s=35)
    ax.set_xlabel("PC1")
    ax.set_ylabel("PC2")
    ax.set_title(title)

fig, axes = plt.subplots(1, 2, figsize=(11, 4))
plot_decision_boundary_fullmodel_in_pca2(
    axes[0],
    lda,
    pca,
    X_test_pca_df,
    y_test,
    orig_feature_names=all_feat_cols,
    title="LDA – brzeg decyzyjny (PCA2; model pełny)",
    n_classes=n_classes,
)
plot_decision_boundary_fullmodel_in_pca2(
    axes[1],
    qda,
    pca,
    X_test_pca_df,
    y_test,
    orig_feature_names=all_feat_cols,
    title="QDA – brzeg decyzyjny (PCA2; model pełny)",
    n_classes=n_classes,
)
plt.tight_layout()
plt.show()
```
:::