---
title: "Modele addytywne i splajny"
---

## Wprowadzenie

W modelowaniu danych tablicowych bardzo często obserwujemy relacje, które nie są liniowe, ale jednocześnie nie chcemy od razu sięgać po „czarne skrzynki” (typu sieci neurononwe). Splajny i modele addytywne stanowią naturalny kompromis - pozwalają modelować nieliniowości w sposób kontrolowany i interpretowalny. W praktyce splajny pojawiają się zarówno jako samodzielne narzędzie regresji, jak i jako komponenty innych metod (np. w FDA – *Flexible Discriminant Analysis* – oraz w GAM – *Generalized Additive Models*).

## Regresje sklejane (splajny)

### Splajn jako wielomianowa regresja kawałkowa

Załóżmy, że chcemy modelować zależność $y \approx f(x)$ dla jednej zmiennej $x$, ale relacja nie jest liniowa. Najprostszą ideą jest podzielenie osi $x$ na przedziały i dopasowanie na każdym z nich osobnego wielomianu. Niech $\tau_1 < \tau_2 < \cdots < \tau_K$ oznacza węzły (*knots*), które dzielą dziedzinę na $K+1$ przedziałów. Splajn regresyjny rzędu $d$ to funkcja:

- która na każdym przedziale jest wielomianem stopnia $d$,
- a na węzłach jest „sklejona” w sposób zapewniający gładkość.

Najczęściej spotykamy splajny sześcienne ($d=3$), ponieważ dają dobrą elastyczność i gładkość, a jednocześnie nie są tak niestabilne jak wielomiany wysokiego stopnia dopasowywane globalnie.

### Warunki gładkości na węzłach

Samo „posklejanie” wielomianów bez warunków prowadzi do nieciągłości i załamań. Dlatego narzuca się warunki zgodności w węzłach. Dla splajnu sześciennego standardem jest wymóg ciągłości funkcji i jej pochodnych do rzędu drugiego:

$$
f(\tau_k^-)=f(\tau_k^+),\quad f'(\tau_k^-)=f'(\tau_k^+),\quad f''(\tau_k^-)=f''(\tau_k^+),\qquad k=1,\ldots,K.
$$

Wtedy splajn jest funkcją gładką (ciągłą z ciągłą krzywizną) na całej dziedzinie.

```{python}
#| code-fold: true

import numpy as np
import matplotlib.pyplot as plt

# (opcjonalnie, ale zalecane) SciPy daje wygodną implementację "prawdziwego" splajnu sześciennego LSQ
from scipy.interpolate import LSQUnivariateSpline

# ============================================================
# 1) Dane i "prawdziwa" funkcja f(x)
# ============================================================
rng = np.random.default_rng(42)

n = 220
x = np.sort(rng.uniform(0, 10, size=n))

def f_true(x):
    # nieliniowa funkcja referencyjna
    return 0.8*np.sin(1.2*x) + 0.15*x + 0.3*np.cos(0.5*x)

y = f_true(x) + rng.normal(0, 0.25, size=n)  # obserwacje z szumem

# Siatka do rysowania funkcji
xg = np.linspace(x.min(), x.max(), 1200)
yg_true = f_true(xg)

# ============================================================
# 2) Węzły (knots): tau_1 < ... < tau_K
#    Bierzemy np. kwantyle (wewnętrzne węzły muszą leżeć w (min,max))
# ============================================================
K = 4
tau = np.quantile(x, [0.2, 0.4, 0.6, 0.8])  # węzły wewnętrzne
tau = np.unique(tau)  # na wypadek powtórzeń
# Przedziały: (-inf, tau1], (tau1, tau2], ..., (tauK, +inf)
bounds = np.r_[x.min(), tau, x.max()]

# Pomocniczo: funkcja zwracająca indeks przedziału
def interval_index(x_val, bounds):
    # zwraca i takie, że x_val należy do [bounds[i], bounds[i+1]]
    return np.clip(np.searchsorted(bounds[1:-1], x_val, side="right"), 0, len(bounds)-2)

# ============================================================
# 3) Aproksymacja 1: funkcje stałe na przedziałach (piecewise constant)
#    - w każdym przedziale dopasowujemy stałą = średnia y w tym przedziale
# ============================================================
const_vals = np.zeros(len(bounds)-1)
for i in range(len(bounds)-1):
    left, right = bounds[i], bounds[i+1]
    if i == 0:
        mask = (x >= left) & (x <= right)
    else:
        mask = (x > left) & (x <= right)
    const_vals[i] = np.mean(y[mask])

def piecewise_constant_predict(x_query):
    idx = np.array([interval_index(v, bounds) for v in x_query])
    return const_vals[idx]

yg_const = piecewise_constant_predict(xg)

# ============================================================
# 4) Aproksymacja 2: wielomiany 3 stopnia na przedziałach BEZ sklejania (bez gładkości)
#    - w każdym przedziale niezależnie dopasowujemy polyfit stopnia 3
#    - brak warunków f(tau^-)=f(tau^+), f'(tau^-)=f'(tau^+), f''(...)  => mogą być skoki
# ============================================================
poly_coefs = []
for i in range(len(bounds)-1):
    left, right = bounds[i], bounds[i+1]
    if i == 0:
        mask = (x >= left) & (x <= right)
    else:
        mask = (x > left) & (x <= right)
    xi = x[mask]
    yi = y[mask]
    # zabezpieczenie: jeśli w przedziale jest zbyt mało punktów, obniżamy stopień
    deg = 3 if xi.size >= 6 else min(1, xi.size-1)
    coefs = np.polyfit(xi, yi, deg=deg)
    poly_coefs.append(coefs)

def piecewise_poly_predict(x_query):
    yhat = np.zeros_like(x_query, dtype=float)
    for j, v in enumerate(x_query):
        i = interval_index(v, bounds)
        yhat[j] = np.polyval(poly_coefs[i], v)
    return yhat

yg_poly_unsmoothed = piecewise_poly_predict(xg)

# ============================================================
# 5) Aproksymacja 3: splajn sześcienny z gładkością C^2 na węzłach
#    - LSQUnivariateSpline: dopasowanie najmniejszych kwadratów + splajn sześcienny
#    - splajn jest C^2: f, f', f'' są ciągłe na węzłach (spełnia warunki z opisu)
# ============================================================
spline = LSQUnivariateSpline(x, y, t=tau, k=3)  # k=3 => sześcienny, t => węzły wewnętrzne
yg_spline = spline(xg)

# ============================================================
# 6) Rysunek: 4 okna
# ============================================================
fig, axes = plt.subplots(2, 2, figsize=(14, 9), constrained_layout=True)
axes = axes.ravel()

def add_knots(ax):
    for t in tau:
        ax.axvline(t, linestyle="--", linewidth=1)

# (1) Prawdziwa funkcja + dane
axes[0].plot(xg, yg_true, linewidth=2, label="prawdziwa f(x)")
axes[0].scatter(x, y, s=18, alpha=0.65, label="dane (szum)")
add_knots(axes[0])
axes[0].set_title("1) Prawdziwa funkcja f(x) oraz obserwacje")
axes[0].set_xlabel("x")
axes[0].set_ylabel("y")
axes[0].legend()

# (2) Aproksymacja stałymi (funkcje kawałkami stałe)
axes[1].plot(xg, yg_true, linewidth=2, alpha=0.6, label="prawdziwa f(x)")
axes[1].plot(xg, yg_const, linewidth=2, label="aproks. stałymi (piecewise constant)")
axes[1].scatter(x, y, s=14, alpha=0.35)
add_knots(axes[1])
axes[1].set_title("2) Aproksymacja stałymi na przedziałach")
axes[1].set_xlabel("x")
axes[1].set_ylabel("y")
axes[1].legend()

# (3) Wielomiany 3-stopnia BEZ sklejania
axes[2].plot(xg, yg_true, linewidth=2, alpha=0.6, label="prawdziwa f(x)")
axes[2].plot(xg, yg_poly_unsmoothed, linewidth=2, label="piecewise cubic (bez gładkości)")
axes[2].scatter(x, y, s=14, alpha=0.35)
add_knots(axes[2])
axes[2].set_title("3) Wielomiany 3-stopnia bez sklejania na węzłach")
axes[2].set_xlabel("x")
axes[2].set_ylabel("y")
axes[2].legend()

# (4) Splajn sześcienny z gładkością C^2 na węzłach
axes[3].plot(xg, yg_true, linewidth=2, alpha=0.6, label="prawdziwa f(x)")
axes[3].plot(xg, yg_spline, linewidth=2, label="splajn sześcienny (C² na węzłach)")
axes[3].scatter(x, y, s=14, alpha=0.35)
add_knots(axes[3])
axes[3].set_title("4) Splajn sześcienny z warunkami gładkości na węzłach")
axes[3].set_xlabel("x")
axes[3].set_ylabel("y")
axes[3].legend()

plt.show()
```

#### Reprezentacja bazowa

1. Ogólna postać bazowa (wspólna dla wszystkich)

Każdą z poniższych konstrukcji możemy zapisać jako model liniowy w bazie:

$$
f(x)=\sum_{j=1}^{J}\beta_j\, b_j(x)
\quad\text{(czasem z wyrazem wolnym wliczonym w bazę)}.
$$

Różnice między „rodzajami splajnów” polegają na tym, jakie funkcje bazowe $b_j$ wybieramy oraz czy dodajemy karę wygładzającą.

2. *Truncated power basis* (baza potęgowa ucięta)

To najbardziej „analityczna” i łatwa do zapisania baza dla splajnów regresyjnych stopnia $d$ z węzłami $\tau_1<\cdots<\tau_K$:

$$
f(x)=\beta_0+\beta_1 x+\cdots+\beta_d x^d\;+\;\sum_{k=1}^{K}\theta_k\, (x-\tau_k)_+^{d},
\qquad (u)_+=\max(u,0).
$$

W praktyce intuicyjna, ale bywa gorzej uwarunkowana numerycznie niż B-splajny (szczególnie dla dużego $K$ i wyższych stopni).

3. B-splajny (*B-spline basis*)

To standard najczęściej stosowany i dobry numeryczny. Splajn stopnia $p$ (często $p=3$) zapisujemy jako:

$$
f(x)=\sum_{j=1}^{J}\beta_j\, B_{j,p}(x),
$$

gdzie $B_{j,p}(x)$ to funkcje bazowe B-splajnów o stopniu $p$, zdefiniowane rekurencyjnie (Cox–de Boor). Dla pełności:

- dla stopnia 0:

$$
B_{j,0}(x)=\mathbf{1}\{\kappa_j \le x < \kappa_{j+1}\},
$$

- rekurencja dla $p\ge 1$:

$$
B_{j,p}(x)=\frac{x-\kappa_j}{\kappa_{j+p}-\kappa_j}B_{j,p-1}(x)\;+\;
\frac{\kappa_{j+p+1}-x}{\kappa_{j+p+1}-\kappa_{j+1}}B_{j+1,p-1}(x),
$$

gdzie $\kappa$ to tzw. *knot vector* (węzły z powtórzeniami na brzegach).

4. Splajn naturalny (*Natural Cubic Spline*)

Splajn sześcienny z dodatkowymi ograniczeniami brzegowymi, które „uspokajają” zachowanie na krańcach. Klasycznie wymaga się, aby druga pochodna była równa zero na brzegach:
$$
f''(a)=0,\qquad f''(b)=0
$$

dla zakresu $[a,b]$ danych. Naturalny splajn nadal można zapisać w bazie:

$$
f(x)=\beta_0+\beta_1 x+\sum_{k=1}^{K}\theta_k\, N_k(x),
$$

gdzie $N_k(x)$ są odpowiednimi funkcjami bazowymi wynikającymi z ograniczeń naturalności (w praktyce zwykle budowane w oparciu o B-splajny i narzucone restrykcje liniowe na współczynniki).

Efekt praktyczny, to stabilniejsze ekstrapolowanie na krańcach i mniejsze ryzyko „dziwnych” zachowań na brzegach.

5. Splajny wygładzające (*Smoothing splines*)

Tu zamiast wybierać małą liczbę węzłów, dopuszczamy bardzo elastyczną klasę funkcji (węzły często na wszystkich unikalnych $x_i$), a złożoność kontrolujemy przez karę na krzywiznę:

$$
\min_{f}\ \sum_{i=1}^{n}\big(y_i-f(x_i)\big)^2\;+\;\lambda\int_a^b\big(f''(t)\big)^2\,dt.
$$

Kluczowy fakt teoretyczny - rozwiązanie jest splajnem sześciennym z węzłami w punktach $x_i$. Praktyczna reprezentacja ma postać:

$$
f(x)=\sum_{i=1}^{n}\alpha_i\, K(x,x_i)\;+\;\beta_0+\beta_1 x,
$$

gdzie $K$ jest jądrem odpowiadającym karze $\int (f'')^2$. W praktyce implementacyjnej często przechodzimy do bazy (np. B-splajnów) i rozwiązujemy problem liniowy z karą.

6. P-splajny (*Penalized B-splines*)

To bardzo popularne w GAM, bo łączą stabilność B-splajnów z łatwą regularizacją. Model:

$$
f(x)=\sum_{j=1}^{J}\beta_j\, B_{j,p}(x),
$$

a kara jest nakładana nie na pochodne funkcji, tylko na różnice współczynników (np. druga różnica):

$$
\min_{\beta}\ \|y - X\beta\|^2 + \lambda \|D^{(2)}\beta\|^2,
$$

gdzie $D^{(2)}$ to macierz operatora drugich różnic (np. $\beta_{j}-2\beta_{j-1}+\beta_{j-2}$).

7. *Thin Plate Splines* (TPS) i *thin plate regression splines*

To szczególnie ważne funkcje bazowe, gdy chcemy uogólnienia na wiele wymiarów (np. $x\in\mathbb{R}^d$) i nie chcemy ręcznie wybierać węzłów. W wersji 2D klasyczna postać TPS to:

$$
f(x)=\beta_0+\beta_1 x_1+\beta_2 x_2+\sum_{i=1}^{n}\alpha_i\,\phi(\|x-x_i\|),
$$

gdzie funkcja radialna (dla 2D) ma postać:

$$
\phi(r)=r^2\log r.
$$

TPS minimalizuje analog kary „energii gięcia” (*bending energy*), będącej odpowiednikiem $\int (f'')^2$ w wielu wymiarach. W praktyce GAM często używa się *thin plate regression splines* – wersji z redukcją wymiaru bazy, aby obliczenia były wykonalne.

8. Splajny tensorowe (*Tensor product splines*) – dla interakcji 2D/3D w GAM

Gdy chcemy modelować gładką zależność od dwóch zmiennych $x_1,x_2$, często stosujemy bazę tensorową:

$$
f(x_1,x_2)=\sum_{j=1}^{J_1}\sum_{\ell=1}^{J_2}\beta_{j\ell}\, b_j(x_1)\, c_\ell(x_2),
$$

gdzie $b_j$ i $c_\ell$ są bazami jednowymiarowymi (np. B-splajny). To daje kontrolowaną, interpretowalną gładką „powierzchnię” zamiast pojedynczych interakcji parametrycznych.

9. *Cardinal splines* / splajny Hermite’a (czasem spotykane)

W zastosowaniach sygnałowych lub interpolacyjnych spotyka się splajny, w których parametryzujemy wartości i pochodne w węzłach (Hermite):

Na przedziale $[x_k,x_{k+1}]$:

$$
f(x)=h_{00}(t)\,y_k+h_{10}(t)\,m_k+h_{01}(t)\,y_{k+1}+h_{11}(t)\,m_{k+1},
\quad t=\frac{x-x_k}{x_{k+1}-x_k},
$$

gdzie $y_k=f(x_k), m_k=f'(x_k), a h_{00},h_{10},h_{01},h_{11}$ to wielomiany Hermite’a. To podejście jest bardziej „interpolacyjne” niż regresyjne, ale warto znać.

#### Liczba węzłów a elastyczność i przeuczenie

Liczba węzłów $K$ steruje złożonością funkcji $f$. Im więcej węzłów, tym więcej stopni swobody i większa elastyczność dopasowania. To jest korzystne, gdy relacja jest rzeczywiście złożona, ale zwiększa ryzyko przeuczenia – model może zacząć dopasowywać przypadkowe fluktuacje.

Możemy to ująć w języku bias–variance: małe $K$ daje większy *bias* (zbyt sztywna krzywa), duże $K$ daje większą wariancję (za duża elastyczność). Dlatego w praktyce:

- albo dobieramy $K$ (np. walidacją krzyżową),
- albo stosujemy splajny wygładzające (*smoothing splines*), gdzie liczba węzłów może być duża, ale złożoność kontrolujemy przez karę na „szorstkość” funkcji.

#### Splajny jako element FDA i GAM

W FDA (*Flexible Discriminant Analysis*) splajny służą do budowy nieliniowych transformacji predyktorów, a następnie stosuje się ideę dyskryminacji na przekształconej przestrzeni cech. W GAM splajny są naturalnym „budulcem” nieliniowych składowych addytywnych. W obu przypadkach kluczowe jest to, że splajn zachowuje interpretowalność: nadal widzimy, jak zmienna wpływa na wynik, ale w sposób nieliniowy.

## Generalized Additive Models (GAM)

Model addytywny zakłada, że wpływ predyktorów sumuje się, ale każda składowa może być funkcją nieliniową:

$$
y \approx \beta_0 + f_1(x_1) + f_2(x_2) + \cdots + f_p(x_p).
$$

Gdy wszystkie $f_j$ są liniowe, otrzymujemy regresję liniową. GAM pozwala, aby każda $f_j$ była np. splajnem, co umożliwia modelowanie nieliniowości bez utraty interpretowalności: możemy analizować „efekt cząstkowy” każdej zmiennej. GAM jest uogólnieniem GLM. Zamiast zakładać liniowy predyktor w skali $y$, modelujemy liniowy predyktor w skali funkcji łączącej $g$:

$$
g\big(\mathbb{E}[Y\mid X]\big) = \beta_0 + \sum_{j=1}^{p} f_j(x_j).
$$

Przykłady:

- regresja: $g(\mu)=\mu$ (tożsamość),
- klasyfikacja binarna - $g(\mu)=\log\frac{\mu}{1-\mu}$ (logit),
- zliczenia - $g(\mu)=\log(\mu)$.

W praktyce GAM pozwala budować modele analogiczne do regresji logistycznej, ale z nieliniowymi efektami predyktorów. Każdą funkcję $f_j$ zapisujemy w bazie (np. splajnów):

$$
f_j(x) = \sum_{\ell=1}^{K_j} \beta_{j\ell}\, b_{j\ell}(x).
$$

Wtedy model staje się liniowy w parametrach $\beta$, ale aby uniknąć przeuczenia, stosuje się regularizację (wygładzanie). Typowa estymacja polega na minimalizacji dla regresji (MSE):

$$
\min_{\beta}\ \sum_{i=1}^{n}\left(y_i - \beta_0 - \sum_{j=1}^{p} f_j(x_{ij})\right)^2
\;+\; \sum_{j=1}^{p} \lambda_j\, \beta_j^\top S_j \beta_j,
$$

gdzie $S_j$ to macierz kary (związana z „szorstkością” funkcji), a $\lambda_j$ kontroluje wygładzenie.

- dla klasyfikacji/log-loss - analogicznie minimalizujemy ujemną log-wiarygodność + kara.

Zuważmy, że w GAM „regularizacja” jest interpretowana jako dobór gładkości funkcji $f_j$, a nie tylko jako „kurczenie współczynników”.

#### Dobór parametrów wygładzania

Dobór $\lambda_j$ jest kluczowy - zbyt małe wartości prowadzą do przeuczenia (zbyt faliste funkcje), a zbyt duże do niedouczenia (prawie linia prosta). W praktyce stosuje się:

- walidację krzyżową,
- kryteria typu GCV (*generalized cross-validation*),
- metody oparte na estymacji wiarygodności (np. REML) – zależnie od implementacji.

### Interpretacja GAM

Największa zaleta GAM jest interpretowalność - każdą funkcję $f_j$ możemy wykreślić jako krzywą efektu cząstkowego, trzymając pozostałe zmienne „w tle”. Dzięki temu otrzymujemy model elastyczny, ale nadal opisujący jak każda zmienna wpływa na wynik.

### Zalety i wady GAM

- GAM łączy elastyczność i interpretowalność. Umożliwia uchwycenie nieliniowości bez konieczności wchodzenia w modele „czarnej skrzynki”. Jest też często świetnym kompromisem w zastosowaniach naukowych i aplikacyjnych, gdzie potrzebujemy zrozumienia mechanizmu, a nie tylko predykcji.
- GAM jest bardziej złożony obliczeniowo niż regresja liniowa, a wyniki są wrażliwe na dobór parametrów wygładzania i typu bazy. Ponadto standardowy GAM jest addytywny, więc złożone interakcje między zmiennymi nie pojawiają się automatycznie (chyba że jawnie dodamy składniki interakcyjne, np. splajn dwuwymiarowy).

### Przykłady

Poniżej pokazujemy dwa przykłady: splajny w regresji liniowej przez budowę bazy oraz GAM.

1. Splajn regresyjny jako cecha w regresji (baza splajnów)

W `scikit-learn` najprościej użyć `SplineTransformer`, który tworzy bazę splajnów, a następnie dopasować model liniowy.

```{python}
import numpy as np
import pandas as pd

from sklearn.pipeline import Pipeline
from sklearn.preprocessing import SplineTransformer
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score

# Przykład: dane syntetyczne 1D
rng = np.random.default_rng(42)
n = 400
x = rng.uniform(0, 10, size=n)
y = np.sin(x) + 0.2*rng.normal(size=n)

X = x.reshape(-1, 1)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)

model = Pipeline(steps=[
    ("spl", SplineTransformer(degree=3, n_knots=8, include_bias=False)),
    ("lin", LinearRegression())
])

model.fit(X_train, y_train)
pred = model.predict(X_test)

print("R2:", r2_score(y_test, pred))

# Rysunek: rzeczywista funkcja, dane, i dopasowany splajn
import matplotlib.pyplot as plt

# Siatka do gładkiego rysowania
x_plot = np.linspace(0, 10, 500).reshape(-1, 1)
y_true_plot = np.sin(x_plot.ravel())
y_spline_plot = model.predict(x_plot)

fig, ax = plt.subplots(figsize=(12, 6), constrained_layout=True)

# Rzeczywista funkcja
ax.plot(x_plot, y_true_plot, "b--", linewidth=2.5, alpha=0.8, label="Rzeczywista f(x) = sin(x)")

# Aproksymacja splajnami
ax.plot(x_plot, y_spline_plot, "r-", linewidth=2.5, label="Aproksymacja splajnami")

# Dane treningowe i testowe
ax.scatter(X_train, y_train, s=40, alpha=0.6, color="green", label="Dane treningowe", edgecolors="darkgreen")
ax.scatter(X_test, y_test, s=40, alpha=0.6, color="orange", label="Dane testowe", edgecolors="darkorange")

ax.set_xlabel("x", fontsize=12)
ax.set_ylabel("y", fontsize=12)
ax.set_title("Splajn sześcienny: rzeczywista funkcja vs aproksymacja", fontsize=13, fontweight="bold")
ax.legend(fontsize=11, loc="upper right")
ax.grid(alpha=0.3)

plt.show()

```

W tym przykładzie splajn działa jak „nieliniowa transformacja cechy”, a regresja liniowa dopasowuje współczynniki do bazy.

2. GAM dla klasyfikacji lub regresji (`pyGAM`)

GAM oferują automatyczne dobieranie parametrów wygładzania i możliwość rysowania efektów cząstkowych. W tym przykładzie budujemy GAM z dwoma zmiennymi objaśniającymi, każda reprezentowana przez splajn. Model automatycznie dobiera stopień wygładzenia dla każdej zmiennej, a my możemy interpretować jej wpływ na wynik niezależnie od pozostałych zmiennych.

**Funkcja rzeczywista** (którą będziemy estymować):

$$
y = \sin(6x_1) + 0.5 x_2^2 + \epsilon,
$$

gdzie $x_1, x_2 \in [0,1]$ są predyktorami, a $\epsilon$ to szum gaussowski. Funkcja pokazuje dwie bardzo różne zależności: pierwsza jest sinusoidalna, druga jest paraboliczna.

```{python}
import numpy as np
import matplotlib.pyplot as plt
from pygam import LinearGAM, s
from sklearn.metrics import r2_score

# Przykład: regresja z dwoma zmiennymi
rng = np.random.default_rng(42)
X = rng.random((300, 2))

# Rzeczywista funkcja generująca dane
def true_function(X):
    return np.sin(6*X[:, 0]) + 0.5*(X[:, 1]**2)

y_true_values = true_function(X)
y = y_true_values + 0.1*rng.normal(size=300)  # dodanie szumu

# Dopasowanie GAM: s(0) + s(1) oznacza splajn dla każdej zmiennej
gam = LinearGAM(s(0) + s(1)).fit(X, y)

# Predykcja
y_pred = gam.predict(X)

# Ocena modelu
r2 = r2_score(y, y_pred)
print(f"R² dla GAM: {r2:.4f}")

# Rysunek: rzeczywista funkcja vs dopasowanie GAM
fig, axes = plt.subplots(1, 2, figsize=(15, 5), constrained_layout=True)

# ========== Lewy panel: Pierwsza zmienna ==========
# Siatka dla pierwszej zmiennej (druga zmienna na medianie)
x1_grid = np.linspace(0, 1, 200)
x2_median = np.median(X[:, 1])
X_grid1 = np.column_stack([x1_grid, np.repeat(x2_median, len(x1_grid))])

y_true_grid1 = true_function(X_grid1)
y_pred_grid1 = gam.predict(X_grid1)

# Obserwacje dla pierwszej zmiennej (grupujemy po drugim wymiarze)
x1_obs = X[:, 0]
axes[0].scatter(x1_obs, y, s=30, alpha=0.4, color="gray", label="Obserwacje")
axes[0].plot(x1_grid, y_true_grid1, "b--", linewidth=2.5, alpha=0.8, label="Rzeczywista f(x₁)")
axes[0].plot(x1_grid, y_pred_grid1, "r-", linewidth=2.5, label="GAM dopasowanie")

axes[0].set_xlabel("x₁", fontsize=12)
axes[0].set_ylabel("y", fontsize=12)
axes[0].set_title(f"Efekt pierwszej zmiennej (x₂ = {x2_median:.2f})", fontsize=12, fontweight="bold")
axes[0].legend(fontsize=10)
axes[0].grid(alpha=0.3)

# ========== Prawy panel: Druga zmienna ==========
# Siatka dla drugiej zmiennej (pierwsza zmienna na medianie)
x2_grid = np.linspace(0, 1, 200)
x1_median = np.median(X[:, 0])
X_grid2 = np.column_stack([np.repeat(x1_median, len(x2_grid)), x2_grid])

y_true_grid2 = true_function(X_grid2)
y_pred_grid2 = gam.predict(X_grid2)

# Obserwacje dla drugiej zmiennej
x2_obs = X[:, 1]
axes[1].scatter(x2_obs, y, s=30, alpha=0.4, color="gray", label="Obserwacje")
axes[1].plot(x2_grid, y_true_grid2, "b--", linewidth=2.5, alpha=0.8, label="Rzeczywista f(x₂)")
axes[1].plot(x2_grid, y_pred_grid2, "r-", linewidth=2.5, label="GAM dopasowanie")

axes[1].set_xlabel("x₂", fontsize=12)
axes[1].set_ylabel("y", fontsize=12)
axes[1].set_title(f"Efekt drugiej zmiennej (x₁ = {x1_median:.2f})", fontsize=12, fontweight="bold")
axes[1].legend(fontsize=10)
axes[1].grid(alpha=0.3)

plt.show()

# Wypisanie efektów cząstkowych
print("\nPodsumowanie GAM:")
print(f"Parametry wygładzania (lambda): {gam.lam}")
print(f"Stopnie swobody: {gam.statistics_['edof']}")

# ========== Wizualizacja 3D: rzeczywista funkcja vs GAM ==========
import plotly.graph_objects as go
from plotly.subplots import make_subplots

# Siatka dla wizualizacji 3D
x1_3d = np.linspace(0, 1, 50)
x2_3d = np.linspace(0, 1, 50)
X1_grid, X2_grid = np.meshgrid(x1_3d, x2_3d)

# Rzeczywista funkcja
Z_true = np.sin(6*X1_grid) + 0.5*(X2_grid**2)

# Predykcja GAM
X_grid_flat = np.column_stack([X1_grid.ravel(), X2_grid.ravel()])
Z_pred = gam.predict(X_grid_flat).reshape(X1_grid.shape)

# Podział danych na train/test dla wizualizacji
from sklearn.model_selection import train_test_split
X_train_vis, X_test_vis, y_train_vis, y_test_vis = train_test_split(X, y, test_size=0.2, random_state=42)

# Tworzenie subplotów 3D
fig = make_subplots(
    rows=2, cols=1,
    specs=[[{'type': 'surface'}], [{'type': 'surface'}]],
    subplot_titles=("Rzeczywista funkcja", "Estymacja GAM"),
    vertical_spacing=0.05
)

# Górny panel: rzeczywista funkcja
fig.add_trace(
    go.Surface(
        x=X1_grid,
        y=X2_grid,
        z=Z_true,
        colorscale='Viridis',
        name='Rzeczywista',
        showscale=False
    ),
    row=1, col=1
)

# Dodanie punktów treningowych (górny panel)
fig.add_trace(
    go.Scatter3d(
        x=X_train_vis[:, 0],
        y=X_train_vis[:, 1],
        z=y_train_vis,
        mode='markers',
        marker=dict(size=4, color='green', opacity=0.6),
        name='Train',
        showlegend=True
    ),
    row=1, col=1
)

# Dodanie punktów testowych (górny panel)
fig.add_trace(
    go.Scatter3d(
        x=X_test_vis[:, 0],
        y=X_test_vis[:, 1],
        z=y_test_vis,
        mode='markers',
        marker=dict(size=4, color='orange', opacity=0.6),
        name='Test',
        showlegend=True
    ),
    row=1, col=1
)

# Dolny panel: estymacja GAM
fig.add_trace(
    go.Surface(
        x=X1_grid,
        y=X2_grid,
        z=Z_pred,
        colorscale='Viridis',
        name='GAM',
        showscale=True
    ),
    row=2, col=1
)

# Dodanie punktów treningowych (dolny panel)
fig.add_trace(
    go.Scatter3d(
        x=X_train_vis[:, 0],
        y=X_train_vis[:, 1],
        z=y_train_vis,
        mode='markers',
        marker=dict(size=4, color='green', opacity=0.6),
        name='Train',
        showlegend=False
    ),
    row=2, col=1
)

# Dodanie punktów testowych (dolny panel)
fig.add_trace(
    go.Scatter3d(
        x=X_test_vis[:, 0],
        y=X_test_vis[:, 1],
        z=y_test_vis,
        mode='markers',
        marker=dict(size=4, color='orange', opacity=0.6),
        name='Test',
        showlegend=False
    ),
    row=2, col=1
)

# Aktualizacja osi
fig.update_xaxes(title_text="x₁", row=1, col=1)
fig.update_yaxes(title_text="x₂", row=1, col=1)
fig.update_xaxes(title_text="x₁", row=2, col=1)
fig.update_yaxes(title_text="x₂", row=2, col=1)

fig.update_layout(
    title_text="Porównanie rzeczywistej funkcji i estymacji GAM (widok 3D)",
    width=1300,
    height=700
)

fig.show()
```

`pyGAM` automatycznie dobiera parametry wygładzania (zwykle przez CV/GCV). Największą zaletą GAM jest możliwość wizualizacji efektów cząstkowych każdej zmiennej - widzimy dokładnie, w jaki sposób każdy predyktor wpływa na przewidywaną wartość, trzymając pozostałe zmienne „w tle". Wizualizacja 3D (powyżej) pokazuje jak dobrze model GAM aproksymuje rzeczywistą powierzchnię, uchwycając zarówno oscylacyjny jak i paraboliczny charakter funkcji. W praktyce GAM stanowi świetny kompromis między prostotą interpretacji a elastycznością modelowania nieliniowości.

## Podsumowanie

Splajny pozwalają modelować nieliniowości w sposób kontrolowany: są wielomianami kawałkowymi, sklejanymi gładko w węzłach, a ich elastyczność zależy od liczby węzłów lub parametrów wygładzania. GAM wykorzystuje splajny jako składowe addytywne, tworząc model interpretowalny, który nie zakłada ścisłej liniowości zależności. W praktyce splajny i GAM stanowią bardzo ważny etap pośredni między prostymi modelami liniowymi a metodami zespołowymi czy głębokimi sieciami: zachowują zrozumiałość, ale potrafią uchwycić znaczną część złożoności danych.
